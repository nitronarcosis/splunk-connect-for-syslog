{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Splunk Connect for Syslog!","text":"<p>Splunk Connect for Syslog is an open source packaged solution for  getting data into Splunk.  It is based on the syslog-ng Open Source Edition (Syslog-NG OSE) and transports data to Splunk via the Splunk  HTTP event Collector (HEC) rather than writing events to disk for collection by a Universal Forwarder.</p>"},{"location":"#product-goals","title":"Product Goals","text":"<ul> <li>Bring a tested configuration and build of syslog-ng OSE to the market that will function consistently regardless of the underlying host\u2019s linux distribution</li> <li>Provide a container with the tested configuration for Docker/K8s that can be more easily deployed than upstream packages directly on a customer OS</li> <li>Provide validated (testable and tested) implementations of filter and parse functions for common vendor products</li> <li>Reduce latency and improve scale by balancing event distribution across Splunk Indexers</li> </ul>"},{"location":"#support","title":"Support","text":"<p>Splunk Support: If you are an existing Splunk customer with access to the Support Portal, create a support ticket for the quickest resolution to any issues you experience. Here are some examples of when it may be appropriate to create a support ticket: - If you experience an issue with the current version of SC4S, such as a feature gap or a documented feature that is not working as expected. - If you have difficulty with the configuration of SC4S, either at the back end or with the out-of-box parsers or index configurations. - If you experience performance issues and need help understanding the bottlenecks. - If you have any questions or issues with the SC4S documentation.</p> <p>GitHub Issues: For all enhancement requests, please feel free to create GitHub issues. We prioritize and work on issues based on their priority and resource availability. You can help us by tagging the requests with the appropriate labels. </p> <p>Splunk Developers are active in the external usergroup on best effort basis, please use support case/github issues to resolve your issues quickly</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome feedback and contributions from the community! Please see our contribution guidelines for more information on how to get involved.</p>"},{"location":"#license","title":"License","text":"<ul> <li> <p>Configuration and documentation licensed subject to CC0</p> </li> <li> <p>Code and scripts licensed subject to BSD-2-Clause </p> </li> <li> <p>Third Party Axoflow image of syslog-ng License</p> </li> <li> <p>Third Party Syslog-NG (OSE) License</p> </li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>Syslog-ng Documentation provided by Axoflow Docs</li> </ul>"},{"location":"CONTRIBUTING/","title":"CONTRIBUTING","text":"<p>Splunk welcomes contributions from the SC4S community, and your feedback and enhancements are appreciated. There\u2019s always code that can be clarified, functionality that can be extended, and new data filters to develop, and documentation to refine. If you see something you think should be fixed or added, go for it!</p>"},{"location":"CONTRIBUTING/#data-safety","title":"Data Safety","text":"<p>Splunk Connect for Syslog is a community built and maintained product. Anyone with internet access can get a Splunk GitHub account and participate. As with any publicly available repository, care must be taken to never share private data via Issues, Pull Requests or any other mechanisms. Any data that is shared in the Splunk Connect for Syslog GitHub repository is made available to the entire Community without limits. Members of the Community and/or their employers (including Splunk) assume no responsibility or liability for any damages resulting from the sharing of private data via the Splunk GitHub.</p> <p>Any data samples shared in the Splunk GitHub repository must be free of private data. * Working locally, identify potentially sensitive field values in data samples (Public IP address, URL, Hostname, Etc.) * Replace all potentially sensitive field values with synthetic values * Manually review data samples to re-confirm they are free of private data before sharing in the Splunk GitHub</p>"},{"location":"CONTRIBUTING/#prerequisites","title":"Prerequisites","text":"<p>When contributing to this repository, please first discuss the change you wish to make via a GitHub issue or Slack message with the owners of this repository.</p>"},{"location":"CONTRIBUTING/#setup-development-environment","title":"Setup Development Environment","text":"<p>For a basic development environment docker and a bash shell is all that is required. For a more complete IDE experience see our wiki (Setup PyCharm)[https://github.com/splunk/splunk-connect-for-syslog/wiki/SC4S-Development-Setup-Using-PyCharm] </p>"},{"location":"CONTRIBUTING/#feature-requests-and-bug-reports","title":"Feature Requests and Bug Reports","text":"<p>Have ideas on improvements or found a problem? While the community encourages everyone to contribute code, it is also appreciated when someone reports an issue. Please report any issues or bugs you find through GitHub\u2019s issue tracker.</p> <p>If you are reporting a bug, please include the following details:</p> <ul> <li>Your operating system name and version</li> <li>Any details about your local setup that might be helpful in troubleshooting (ex. container runtime you use, etc.)</li> <li>Data sample (in raw, \u201con the wire\u201d format)</li> <li>Detailed steps to reproduce the bug</li> </ul> <p>We want to hear about your enhancements as well. Feel free to submit them as issues:</p> <ul> <li>Explain in detail how they should work</li> <li>Keep the scope as narrow as possible. This will make it easier to implement</li> </ul>"},{"location":"CONTRIBUTING/#fixing-issues","title":"Fixing Issues","text":"<p>Look through our issue tracker to find problems to fix! Feel free to comment and tag community members of this project with any questions or concerns.</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>What is a \u201cpull request\u201d? It informs the project\u2019s core developers about the changes you want to review and merge. Once you submit a pull request, it enters a stage of code review where you and others can discuss its potential modifications and even add more commits to it later on.</p> <p>If you want to learn more, please consult this tutorial on how pull requests work in the GitHub Help Center.</p> <p>Here\u2019s an overview of how you can make a pull request against this project:</p> <ul> <li>Fork the Splunk-connect-for-syslog GitHub repository</li> <li>Clone your fork using git and create a branch off develop <pre><code>git clone git@github.com:YOUR_GITHUB_USERNAME/splunk-connect-for-syslog.git\ncd splunk-connect-for-syslog\n</code></pre></li> <li>This project uses \u2018develop\u2019 for all development activity, so create your branch off that <pre><code>git checkout -b your-bugfix-branch-name develop\n</code></pre></li> <li>Run all the tests to verify your environment <pre><code>cd splunk-connect-for-syslog\n./test-with-compose.sh\n</code></pre></li> <li>Make your changes, commit and push once your tests have passed <pre><code>git commit -m \"\"\ngit push\n</code></pre></li> <li>Submit a pull request through the GitHub website using the changes from your forked codebase</li> </ul>"},{"location":"CONTRIBUTING/#code-review","title":"Code Review","text":"<p>There are two aspects of code review: giving and receiving. To make it easier for your PR to receive reviews, consider the reviewers will need you to:</p> <ul> <li>Follow the project coding conventions</li> <li>Write good commit messages</li> <li>Break large changes into a logical series of smaller patches which individually make easily understandable changes, and in aggregate solve a broader issue</li> <li>Reviewers are highly encouraged to revisit the Code of Conduct and must go above and beyond to promote a collaborative, respectful community.</li> <li>When reviewing PRs from others, \u201cThe Gentle Art of Patch Review\u201d suggests an iterative series of focuses which is designed to lead new contributors to positive collaboration without inundating them initially with nuances:<ul> <li>Is the idea behind the contribution sound?</li> <li>Is the contribution architected correctly?</li> <li>Is the contribution polished?</li> </ul> </li> <li>For this project, we require that at least 2 approvals are given and a build from our continuous integration system is successful off of your branch. Please note that any new changes made with your existing pull request during review will automatically unapprove and retrigger another build/round of tests.</li> </ul>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>Testing is the responsibility of all contributors. In general, we try to adhere to TDD, writing the test first. There are multiple types of tests. The location of the test code varies with type, as do the specifics of the environment needed to successfully run the test.</p> <ul> <li>Review existing tests in the tests folder of the repo</li> </ul> <p>We could always use improvements to our documentation! Anyone can contribute to these docs - whether you\u2019re new to the project, you\u2019ve been around a long time, and whether you self-identify as a developer, an end user, or someone who just can\u2019t stand seeing typos. What exactly is needed?</p> <ul> <li>More complementary documentation. Have you perhaps found something unclear?</li> <li>More examples or generic templates that others can use.</li> <li>Blog posts, articles and such \u2013 they\u2019re all very much appreciated.</li> <li>You can also edit documentation files directly in the GitHub web interface, without creating a local copy. This can be convenient for small typos or grammar fixes.</li> </ul>"},{"location":"CONTRIBUTING/#release-notes","title":"Release Notes","text":"<p>To add commit messages to release notes, tag the message in following format <pre><code>[TYPE] &lt;commit message&gt;\n</code></pre> [TYPE] can be among the following * FEATURE * FIX * DOC * TEST * CI * REVERT * FILTERADD * FILTERMOD</p> <pre><code>Sample commit:\ngit commit -m \"[TEST] test-message\"\n</code></pre>"},{"location":"architecture/","title":"SC4S Architectural Considerations","text":"<p>SC4S provides performant and reliable syslog data collection. When you are planning your configuration, review the following architectural considerations. These recommendations pertain to the Syslog protocol and age, and are not specific to Splunk Connect for Syslog.</p>"},{"location":"architecture/#the-syslog-protocol","title":"The syslog Protocol","text":"<p>The syslog protocol design prioritizes speed and efficiency, which can occur at the expense of resiliency and reliability.  User Data Protocol (UDP) provides the ability to \u201csend and forget\u201d events over the network without regard to or acknowledgment of receipt. Transport Layer Secuirty (TLS) and Secure Sockets Layer (SSL) protocols are also supported, though UDP prevails as the preferred syslog transport for most data centers.</p> <p>Because of these tradeoffs, traditional methods to provide scale and resiliency do not necessarily transfer to syslog.  </p>"},{"location":"architecture/#ip-protocol","title":"IP protocol","text":"<p>By default, SC4S listens on ports using IPv4. IPv6 is also supported, see <code>SC4S_IPV6_ENABLE</code> in source configuration options.</p>"},{"location":"architecture/#collector-location","title":"Collector Location","text":"<p>Since syslog is a \u201csend and forget\u201d protocol, it does not perform well when routed through substantial network infrastructure. This  includes front-side load balancers and WAN.  The most reliable way to collect syslog traffic is to provide for edge collection rather than centralized collection. If you centrally locate your syslog server, the UDP and (stateless) TCP traffic cannot adjust and data loss will occur.</p>"},{"location":"architecture/#syslog-data-collection-at-scale","title":"syslog Data Collection at Scale","text":"<p>As a best practice, do not co-locate syslog-ng servers for horizontal scale and load balance to them with a front-side load balancer:</p> <ul> <li> <p>Attempting to load balance for scale can cause more data loss due to normal device operations and attendant buffer loss. A simple, robust single server or shared-IP cluster provides the best performance.</p> </li> <li> <p>Front-side load balancing causes inadequate data distribution on the upstream side, leading to uneven data load on the indexers.</p> </li> </ul>"},{"location":"architecture/#high-availability-considerations-and-challenges","title":"High availability considerations and challenges","text":"<p>Load balancing for high availability does not work well for stateless, unacknowledged syslog traffic. More data is preserved when you use a more simple design such as vMotioned VMs.  With syslog, the protocol itself is prone to loss, and syslog data collection can be made \u201cmostly available\u201d at best.</p>"},{"location":"architecture/#udp-vs-tcp","title":"UDP vs. TCP","text":"<p>Run your syslog configuration on UDP rather than TCP.</p> <p>The syslogd daemon optimally uses UDP for log forwarding to reduce overhead. This is because UDP\u2019s streaming method does not require the overhead of establishing a network session.  UDP reduces network load on the network stream with no required receipt verification or window adjustment.</p> <p>TCP uses Acknowledgement Signals (ACKS) to avoid data loss, however, loss can still occur when:</p> <ul> <li>The TCP session is closed: Events published while the system is creating a new session are lost. </li> <li>The remote side is busy and cannot send an acknowledgement signal fast enough: Events are lost due to a full local buffer.</li> <li>A single acknowledgement signal is lost by the network and the client closes the connection: Local and remote buffer are lost.</li> <li>The remote server restarts for any reason: Local buffer is lost.</li> <li>The remote server restarts without closing the connection: Local buffer plus timeout time are lost.</li> <li>The client side restarts without closing the connection.</li> <li>Increased overhead on the network can lead to loss.</li> </ul> <p>Use TCP if the syslog event is larger than the maximum size of the UDP packet on your network typically limited to Web Proxy, DLP, and IDs type sources. To mitigate the drawbacks of TCP you can use TLS over TCP:</p> <ul> <li>The TLS can continue a session over a broken TCP to reduce buffer loss conditions.</li> <li>The TLS fills packets for more efficient use of memory.</li> <li>The TLS compresses data in most cases.</li> </ul>"},{"location":"configuration/","title":"SC4S Configuration Variables","text":"<p>Other than device filter creation, SC4S is almost entirely controlled by environment variables.  Here are the categories and variables needed to properly configure SC4S for your environment.</p>"},{"location":"configuration/#global-configuration","title":"Global Configuration","text":"Variable Values Description SC4S_USE_REVERSE_DNS yes or no(default) Use reverse DNS to identify hosts when HOST is not valid in the syslog header. SC4S_REVERSE_DNS_KEEP_FQDN yes or no(default) When enable, SC4S will not extract hostname from FQDN, and instead will pass the full domain name to HOST. SC4S_CONTAINER_HOST string Variable that is passed to the container to identify the actual log host for container implementations. <ul> <li> <p>NOTE: Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature.  Moreover, HEC Ack would significantly degrade performance for streaming data such as syslog.</p> </li> <li> <p>NOTE:  Use of the <code>SC4S_USE_REVERSE_DNS</code> variable can have a significant impact on performance if the reverse DNS facility (typically a caching nameserver) is not performant.  If you notice events being indexed far later than their actual timestamp in the event (latency between <code>_indextime</code> and <code>_time</code>), this is the first place to check.</p> </li> </ul>"},{"location":"configuration/#configure-use-of-external-http-proxy","title":"Configure use of external http proxy","text":"<p>Warning: Many http proxies are not provisioned with application traffic in mind. Ensure adequate capacity is available to avoid data loss and or proxy outages. Note: the follow variables are lower case</p> Variable Values Description http_proxy undefined Use libcurl format proxy string \u201chttp://username:password@proxy.server:port\u201d https_proxy undefined Use libcurl format proxy string \u201chttp://username:password@proxy.server:port\u201d"},{"location":"configuration/#splunk-hec-destination-configuration","title":"Splunk HEC Destination Configuration","text":"Variable Values Description SC4S_DEST_SPLUNK_HEC_CIPHER_SUITE comma separated list Open SSL cipher suite list SC4S_DEST_SPLUNK_HEC_SSL_VERSION comma separated list Open SSL version list SC4S_DEST_SPLUNK_HEC_WORKERS numeric Number of destination workers (default: 10 threads).  This should rarely need to be changed; consult sc4s community for advice on appropriate setting in extreme high- or low-volume environments. SC4S_DEST_SPLUNK_INDEXED_FIELDS r_unixtime,facility,severity,container,loghost,destport,fromhostip,protonone List of sc4s indexed fields that will be included with each event in Splunk (default is the entire list except \u201cnone\u201d).  Two other indexed fields, <code>sc4s_vendor_product</code> and <code>sc4s_syslog_format</code>, will also appear along with the fields selected via the list and cannot be turned on or off individually.  If no indexed fields are desired (including the two internal ones), set the value to the single value of \u201cnone\u201d.  When setting this variable, separate multiple entries with commas and do not include extra spaces.This list maps to the following indexed fields that will appear in all Splunk events:facility: sc4s_syslog_facilityseverity: sc4s_syslog_severitycontainer: sc4s_containerloghost: sc4s_loghostdport: sc4s_destportfromhostip: sc4s_fromhostipproto: sc4s_proto <ul> <li> <p>NOTE:  When using alternate HEC destinations, the destination operating parameters outlined above (<code>CIPHER_SUITE</code>, <code>SSL_VERSION</code>, etc.) can be individually controlled per <code>DESTID</code> (see \u201cConfiguration of Additional Splunk HEC Destinations\u201d immediately below).  For example, to set the number of workers for the alternate HEC destination <code>d_hec_FOO</code> to 24, set <code>SC4S_DEST_SPLUNK_HEC_FOO_WORKERS=24</code>.</p> </li> <li> <p>NOTE2:  Configuration files for destinations must have a <code>.conf</code> extension</p> </li> </ul>"},{"location":"configuration/#configure-additional-pki-trust-anchors","title":"Configure additional PKI Trust Anchors","text":"<p>Additional trusted (private) Certificate authorities may be trusted by appending each PEM formatted certificate to <code>/opt/sc4s/tls/trusted.pem</code></p>"},{"location":"configuration/#configuration-of-timezone-for-legacy-sources","title":"Configuration of timezone for legacy sources","text":"<p>Legacy sources (those that remain non compliant with RFC5424) often leave the recipient to guess at the actual time zone offset. SC4S uses an advanced feature of syslog-ng to \u201cguess\u201d the correct time zone for real time sources. However, this feature requires the source (device) clock to be synchronized to within +/- 30s of the SC4S system clock. Industry accepted best practice is to set such legacy systems to GMT (sometimes inaccurately called UTC). However, this is not always possible and in such cases two additional methods are available. For a list of time zones see. Only the \u201cTZ Database name\u201d OR \u201coffset\u201d format may be used.</p>"},{"location":"configuration/#change-global-default-time-zone","title":"Change Global default time zone","text":"<p>This setting is used when the container cost is not set for UTC (best practice). Using this setting is often confusing and should be avoided.</p> <p>Set the <code>SC4S_DEFAULT_TIMEZONE</code> variable to a recognized \u201czone info\u201d (Region/City) time zone format such as <code>America/New_York</code>. Setting this value will force SC4S to use the specified timezone (and honor its associated Daylight Savings/Summer Time rules) for all events without a timezone offset in the header or message payload.</p>"},{"location":"configuration/#sc4s-disk-buffer-configuration","title":"SC4S Disk Buffer Configuration","text":"<p>Disk buffers in SC4S are allocated per destination.  Keep this in mind when using additional destinations that have disk buffering configured.  By default, when alternate HEC destinations are configured as outlined above disk buffering will be configured identically to that of the main HEC destination (unless overridden individually).</p>"},{"location":"configuration/#important-notes-regarding-disk-buffering","title":"Important Notes Regarding Disk Buffering:","text":"<ul> <li> <p>\u201cReliable\u201d disk buffering offers little advantage over \u201cnormal\u201d disk buffering, at a significant performance penalty. For this reason, normal disk buffering is recommended.</p> </li> <li> <p>If you add destinations locally in your configuration, pay attention to the cumulative buffer requirements when allocating local disk.</p> </li> <li> <p>Disk buffer storage is configured via container volumes and is persistent between restarts of the container. Be sure to account for disk space requirements on the local sc4s host when creating the container volumes in your respective runtime environment (outlined in the \u201cgetting started\u201d runtime docs). These volumes can grow significantly if there is an extended outage to the SC4S destinations (HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info.</p> </li> <li> <p>The values for the variables below represent the total sizes of the buffers for the destination.  These sizes are divided by the number of workers (threads) when setting the actual syslog-ng buffer options, because the buffer options apply to each worker rather than the entire destination.  Pay careful attention to this when using the \u201cBYOE\u201d version of SC4S, where direct access to the syslog-ng config files may hide this nuance.  Lastly, be sure to factor in the syslog-ng data structure overhead (approx. 2x raw message size) when calculating the total buffer size needed. To determine the proper size of the disk buffer, consult the \u201cData Resilience\u201d section below.</p> </li> <li> <p>When changing the disk buffering directory, the new directory must exist.  If it doesn\u2019t, then syslog-ng will fail to start.</p> </li> <li> <p>When changing the disk buffering directory, if buffering has previously occurred on that instance, a persist file may exist which will prevent syslog-ng from changing the directory.</p> </li> </ul>"},{"location":"configuration/#disk-buffer-variables","title":"Disk Buffer Variables","text":"Variable Values/Default Description SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_ENABLE yes(default) or no Enable local disk buffering SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_RELIABLE yes or no(default) Enable reliable/normal disk buffering (normal recommended) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_MEMBUFSIZE bytes (10241024) Memory buffer size in bytes (used with reliable disk buffering) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_MEMBUFLENGTH messages (15000) Memory buffer size in message count (used with normal disk buffering) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_DISKBUFSIZE bytes (53687091200) Size of local disk buffer in bytes (default 50 GB) SC4S_DEST_SPLUNK_HEC_DEFAULT_DISKBUFF_DIR path Location to store the disk buffer files.  This variable should only be set when using BYOE; this location is fixed when using the Container."},{"location":"configuration/#archive-file-configuration","title":"Archive File Configuration","text":"<p>This feature is designed to support compliance or \u201cdiode mode\u201d archival of all messages. Instructions for mounting the appropriate local directory to use this feature are included in each \u201cgetting started\u201d runtime document. The files will be stored in a folder structure at the mount point using the pattern shown in the table below depending on the value of the <code>SC4S_GLOBAL_ARCHIVE_MODE</code> variable. All events for both modes are formatted using syslog-ng\u2019s EWMM template.</p> Variable Value/Default Location/Pattern SC4S_GLOBAL_ARCHIVE_MODE compliance(default) <code>&lt;archive mount&gt;/${.splunk.sourcetype}/${HOST}/$YEAR-$MONTH-$DAY-archive.log</code> SC4S_GLOBAL_ARCHIVE_MODE diode <code>&lt;archive mount&gt;/${YEAR}/${MONTH}/${DAY}/${fields.sc4s_vendor_product}_${YEAR}${MONTH}${DAY}${HOUR}${MIN}.log\"</code> <p>WARNING POTENTIAL OUTAGE CAUSING CONSEQUENCE</p> <p>Use the following variables to select global archiving or per-source archiving.  C4S does not prune the files that are created; therefore the administrator must provide a means of log rotation to prune files and/or move them to an archival system to avoid exhaustion of disk space.</p> Variable Values Description SC4S_ARCHIVE_GLOBAL yes or undefined Enable archive of all vendor_products SC4S_DEST_&lt;VENDOR_PRODUCT&gt;_ARCHIVE yes(default) or undefined See sources section of documentation enables selective archival"},{"location":"configuration/#syslog-source-configuration","title":"Syslog Source Configuration","text":"Variable Values/Default Description SC4S_SOURCE_TLS_ENABLE yes or no(default) Enable TLS globally.  Be sure to configure the cert as shown immediately below. SC4S_LISTEN_DEFAULT_TLS_PORT undefined or 6514 Enable a TLS listener on port 6514 SC4S_LISTEN_DEFAULT_RFC6425_PORT undefined or 5425 Enable a TLS listener on port 5425 SC4S_SOURCE_TLS_OPTIONS <code>no-sslv2</code> Comma-separated list of the following options: <code>no-sslv2, no-sslv3, no-tlsv1, no-tlsv11, no-tlsv12, none</code>.  See syslog-ng docs for the latest list and defaults SC4S_SOURCE_TLS_CIPHER_SUITE See openssl Colon-delimited list of ciphers to support, e.g. <code>ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384</code>.  See openssl docs for the latest list and defaults SC4S_SOURCE_TCP_MAX_CONNECTIONS 2000 Max number of TCP Connections SC4S_SOURCE_UDP_IW_USE yes or no(default) If we want to change the Initial Window Size for UDP SC4S_SOURCE_UDP_FETCH_LIMIT 1000 Number of events to fetch from server buffer at once SC4S_SOURCE_UDP_IW_SIZE 250000 Initial Window size SC4S_SOURCE_TCP_IW_SIZE 20000000 Initial Window size SC4S_SOURCE_TCP_FETCH_LIMIT 2000 Number of events to fetch from server buffer at once SC4S_SOURCE_UDP_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly. SC4S_SOURCE_TCP_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly. SC4S_SOURCE_TLS_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly. SC4S_SOURCE_RFC5426_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly. SC4S_SOURCE_RFC6587_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly. SC4S_SOURCE_RFC5425_SO_RCVBUFF 17039360 UDP server buffer size in bytes. Make sure that the host OS kernel is configured similarly. SC4S_SOURCE_LISTEN_UDP_SOCKETS 4 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss.  Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC5426_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss.  Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC6587_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss.  Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_LISTEN_RFC5425_SOCKETS 1 Number of kernel sockets per active UDP port, which configures multi-threading of the UDP input buffer in the kernel to prevent packet loss.  Total UDP input buffer is the multiple of SOCKETS * SO_RCVBUFF SC4S_SOURCE_STORE_RAWMSG undefined or \u201cno\u201d Store unprocessed \u201con the wire\u201d raw message in the RAWMSG macro for use with the \u201cfallback\u201d sourcetype.  Do not set this in production; substantial memory and disk overhead will result. Use for log path/filter development only. SC4S_IPV6_ENABLE yes or no(default) enable (dual-stack)IPv6 listeners and health checks"},{"location":"configuration/#syslog-source-tls-certificate-configuration","title":"Syslog Source TLS Certificate Configuration","text":"<ul> <li>Create a folder <code>/opt/sc4s/tls</code> if not already done as part of the \u201cgetting started\u201d process.</li> <li>Uncomment the appropriate mount line in the unit or yaml file (again, documented in the \u201cgetting started\u201d runtime documents).</li> <li>Save the server private key in PEM format with NO PASSWORD to <code>/opt/sc4s/tls/server.key</code></li> <li>Save the server certificate in PEM format to <code>/opt/sc4s/tls/server.pem</code></li> <li>Ensure the entry <code>SC4S_SOURCE_TLS_ENABLE=yes</code> exists in <code>/opt/sc4s/env_file</code></li> </ul>"},{"location":"configuration/#sc4s-metadata-configuration","title":"SC4S metadata configuration","text":""},{"location":"configuration/#log-path-overrides-of-index-or-metadata","title":"Log Path overrides of index or metadata","text":"<p>A key aspect of SC4S is to properly set Splunk metadata prior to the data arriving in Splunk (and before any TA processing  takes place).  The filters will apply the proper index, source, sourcetype, host, and timestamp metadata automatically by individual data source.  Proper values for this metadata, including a recommended index and output format (template), are included with all \u201cout-of-the-box\u201d log paths included with SC4S and are chosen to properly interface with the corresponding TA in Splunk.  The administrator will need to ensure all recommended indexes be created to accept this data if the defaults are not changed.</p> <p>It will be common to override default values in many installations. To accommodate this, each log path consults an internal lookup file that maps Splunk metadata to the specific data source being processed.  This file contains the defaults that are used by SC4S to set the appropriate Splunk metadata (<code>index</code>, <code>host</code>, <code>source</code>, and <code>sourcetype</code>) for each data source.  This file is not directly available to the administrator, but a copy of the file is deposited in the local mounted directory (by default <code>/opt/sc4s/local/context/splunk_metadata.csv.example</code>) for reference.  It is important to note that this copy is not used directly, but is provided solely for reference.  To add to the list, or to override default entries, simply create an override file without the <code>example</code> extension (e.g. <code>/opt/sc4s/local/context/splunk_metadata.csv</code>) and modify it according to the instructions below.</p> <p><code>splunk_metadata.csv</code> is a CSV file containing a \u201ckey\u201d that is referenced in the log path for each data source.  These keys are documented in the individual source files in this section, and allow one to override Splunk metadata either in whole or part. The use of this file is best shown by example.  Here is the Netscreen \u201cSourcetype and Index Configuration\u201d table from the Juniper source documentation:</p> key sourcetype index notes juniper_netscreen netscreen:firewall netfw none <p>Here is a line from a typical <code>splunk_metadata.csv</code> override file:</p> <pre><code>juniper_netscreen,index,ns_index\n</code></pre> <p>The columns in this file are <code>key</code>, <code>metadata</code>, and <code>value</code>.  To make a change via the override file, consult the <code>example</code> file (or the source documentation) for the proper key when overriding an existing source and modify and/or add rows in the table, specifying one or more of the following <code>metadata/value</code> pairs for a given <code>key</code>:</p> <ul> <li><code>key</code> which refers to the vendor and product name of the data source, using the <code>vendor_product</code> convention.  For overrides, these keys    will be listed in the <code>example</code> file.  For new (custom) sources, be sure to choose a key that accurately reflects the vendor and product    being configured, and that matches what is specified in the log path.</li> <li><code>index</code> to specify an alternate <code>value</code> for index</li> <li><code>source</code> to specify an alternate <code>value</code> for source</li> <li><code>host</code> to specify an alternate <code>value</code> for host</li> <li><code>sourcetype</code> to specify an alternate <code>value</code> for sourcetype (be very careful when changing this; only do so if an upstream     TA is not being used, or a custom TA (built by you) is being used.)</li> <li><code>sc4s_template</code> to specify an alternate <code>value</code> for the syslog-ng template that will be used to format the event that will be    indexed by Splunk.  Changing this carries the same warning as the sourcetype above; this will affect the upstream TA.  The template    choices are documented elsewhere in this Configuration section.</li> </ul> <p>In our example above, the <code>juniper_netscreen</code> key references a new index used for that data source called <code>ns_index</code>.</p> <p>In general, for most deployments the index should be the only change needed; other default metadata should almost never be overridden (particularly for the \u201cOut of the Box\u201d data sources).  Even then, care should be taken when considering any alternates, as the defaults for SC4S were chosen with best practices in mind.</p> <ul> <li> <p>NOTE:  The <code>splunk_metadata.csv</code> file is a true override file and the entire <code>example</code> file should not be copied over to the override.  In most cases, the override file is just one or two lines, unless an entire index category (e.g. <code>netfw</code>) needs to be overridden. This is similar in concept to the \u201cdefault\u201d and \u201clocal\u201d conf file precedence in Splunk Enterprise.</p> </li> <li> <p>NOTE The <code>splunk_metadata.csv</code> file should always be appended with an appropriate new key and default for the index when building a custom SC4S log path, as the new key will not exist in the internal lookup (nor the <code>example</code> file).  Care should be taken during log path design to choose appropriate index, sourcetype and template defaults so that admins are not compelled to override them.  If the custom log path is later added to the list of SC4S-supported sources, this addendum can be removed.</p> </li> <li> <p>NOTE:  As noted above, the <code>splunk_metadata.csv.example</code> file is provided for reference only and is not used directly by SC4S.  However, it is an exact copy of the internal file, and can therefore change from release to release.  Be sure to check the example file first to make sure the keys for any overrides map correctly to the ones in the example file.</p> </li> </ul>"},{"location":"configuration/#override-index-or-metadata-based-on-host-ip-or-subnet-compliance-overrides","title":"Override index or metadata based on host, ip, or subnet (compliance overrides)","text":"<p>In other cases it is appropriate to provide the same overrides but based on PCI scope, geography, or other criterion rather than globally. This is accomplished by the use of a file that uniquely identifies these source exceptions via syslog-ng filters, which maps to an associated lookup of alternate indexes, sources, or other metadata.  In addition, (indexed) fields can also be added to further classify the data.</p> <ul> <li> <p>The <code>conf</code> and <code>csv</code> files referenced below will be populated into the <code>/opt/sc4s/local/context</code> directory when SC4S is run for the first time after being set up according to the \u201cgetting started\u201d runtime documents, in a similar fashion to <code>splunk_metadata.csv</code>. After this first-time population of the files takes place, they can be edited (and SC4S restarted) for the changes to take effect.  To get started:</p> </li> <li> <p>Edit the file <code>compliance_meta_by_source.conf</code> to supply uniquely named filters to identify events subject to override.</p> </li> <li>Edit the file <code>compliance_meta_by_source.csv</code> to supply appropriate field(s) and values.</li> </ul> <p>The three columns in the <code>csv</code> file are <code>filter name</code>, <code>field name</code>, and <code>value</code>.  Filter names in the <code>conf</code> file must match one or more corresponding <code>filter name</code> rows in the <code>csv</code> file.  The <code>field name</code> column obeys the following convention:</p> <ul> <li><code>.splunk.index</code> to specify an alternate <code>value</code> for index</li> <li><code>.splunk.source</code> to specify an alternate <code>value</code> for source</li> <li><code>.splunk.sourcetype</code> to specify an alternate <code>value</code> for sourcetype (be very careful when changing this; only do so if a downstream     TA is not being used, or a custom TA (built by you) is being used.)</li> <li><code>fields.fieldname</code> where <code>fieldname</code> will become the name of an indexed field sent to Splunk with the supplied <code>value</code> </li> </ul> <p>This file construct is best shown by an example.  Here is a sample <code>compliance_meta_by_source.conf</code> file:</p> <p><pre><code>filter f_test_test {\n   host(\"something-*\" type(glob)) or\n   netmask(192.168.100.1/24)\n};\n</code></pre> and the corresponding <code>compliance_meta_by_source.csv</code> file:</p> <pre><code>f_test_test,.splunk.index,\"pciindex\"\nf_test_test,fields.compliance,\"pci\"\n</code></pre> <p>First off, ensure that the filter name(s) in the <code>conf</code> file match one or more rows in the <code>csv</code> file. In this case, any incoming message with a hostname starting with <code>something-</code> or arriving from a netmask of <code>192.168.100.1/24</code> will match the <code>f_test_test</code> filter, and the corresponding entries in the <code>csv</code> file will be checked for overrides. In this case, the new index is <code>pciindex</code>, and an indexed field named <code>compliance</code> will be sent to Splunk, with it\u2019s value set to <code>pci</code>. To add additional overrides, simply add another <code>filter foo_bar {};</code> stanza to the <code>conf</code> file, and add appropriate entries to the <code>csv</code> file that match the filter name(s) to the overrides you desire.</p> <ul> <li>IMPORTANT:  The files above are actual syslog-ng config file snippets that get parsed directly by the underlying syslog-ng process.  Take care that your syntax is correct; for more information on proper syslog-ng syntax, see the syslog-ng documentation. A syntax error will cause the runtime process to abort in the \u201cpreflight\u201d phase at startup.</li> </ul> <p>Finally, to update your changes for the systemd-based runtimes, restart SC4S using the commands: <pre><code>sudo systemctl daemon-reload\nsudo systemctl restart sc4s\n</code></pre></p> <p>For the Docker Swarm runtime, redeploy the updated service using the command: <pre><code>docker stack deploy --compose-file docker-compose.yml sc4s\n</code></pre></p>"},{"location":"configuration/#dropping-all-data-by-ip-or-subnet-deprecated","title":"Dropping all data by ip or subnet (deprecated)","text":"<p>The usage of <code>vendor_product_by_source</code> to null queue is now deprecated. Please refer to the recommended method for dropping data in Filtering events from output.</p> <p>In some cases rogue or port-probing data can be sent to SC4S from misconfigured devices or vulnerability scanners. Update the <code>vendor_product_by_source.conf</code> filter <code>f_null_queue</code> with one or more ip/subnet masks to drop events without logging. Note that drop metrics will be recorded.</p>"},{"location":"configuration/#fixing-overriding-the-host-field","title":"Fixing (overriding) the host field","text":"<p>In some cases the host value is not present in an event (or an IP address is in its place). For administrators who require that a true hostname will be attached to each event, SC4S provides an optional facility to perform a reverse IP to name lookup. If the variable <code>SC4S_USE_REVERSE_DNS</code> is set to \u201cyes\u201d, then SC4S first checks <code>host.csv</code> and replaces the value of <code>host</code> with the specified value that matches the incoming IP address. If a value is not found in <code>host.csv</code>, SC4S attempts a reverse DNS lookup against the configured nameserver. In this case, SC4S by default extracts only the hostname from FQDN (<code>example.domain.com</code> -&gt; <code>example</code>). If <code>SC4S_REVERSE_DNS_KEEP_FQDN</code> variable is set to \u201cyes\u201d, full domain name is assigned to the host field.</p> <ul> <li>NOTE:  <code>SC4S_USE_REVERSE_DNS</code> can have a significant impact on performance if the reverse DNS facility (typically a caching nameserver) is not performant. If you notice events being indexed far later than their actual timestamp in the event (latency between <code>_indextime</code> and <code>_time</code>), you should check this variable first.</li> </ul>"},{"location":"configuration/#splunk-connect-for-syslog-output-templates-syslog-ng-templates","title":"Splunk Connect for Syslog output templates (syslog-ng templates)","text":"<p>Splunk Connect for Syslog utilizes the syslog-ng template mechanism to format the output payload (event) that will be sent to Splunk. These templates can format the messages in a number of ways (straight text, JSON, etc.) as well as utilize the many syslog-ng \u201cmacros\u201d (fields) to specify what gets placed in the payload that is delivered to the destination.  Here is a list of the templates used in SC4S, which can be used in the metadata override section immediately above.  New templates can also be added by the administrator in the \u201clocal\u201d section for local destinations; pay careful attention to the syntax as the templates are \u201clive\u201d syslog-ng config code.</p> Template name Template contents Notes t_standard ${DATE} ${HOST} ${MSGHDR}${MESSAGE} Standard template for most RFC3164 (standard syslog) traffic t_msg_only ${MSGONLY} syslog-ng $MSG is sent, no headers (host, timestamp, etc.) t_msg_trim $(strip $MSGONLY) As above with whitespace stripped t_everything ${ISODATE} ${HOST} ${MSGHDR}${MESSAGE} Standard template with ISO date format t_hdr_msg ${MSGHDR}${MESSAGE} Useful for non-compliant syslog messages t_legacy_hdr_msg ${LEGACY_MSGHDR}${MESSAGE} Useful for non-compliant syslog messages t_hdr_sdata_msg ${MSGHDR}${MSGID} ${SDATA} ${MESSAGE} Useful for non-compliant syslog messages t_program_msg ${PROGRAM}[${PID}]: ${MESSAGE} Useful for non-compliant syslog messages t_program_nopid_msg ${PROGRAM}: ${MESSAGE} Useful for non-compliant syslog messages t_JSON_3164 $(format-json \u2013scope rfc3164\u2013pair PRI=\u201d&lt;$PRI&gt;\u201d\u2013key LEGACY_MSGHDR\u2013exclude FACILITY\u2013exclude PRIORITY) JSON output of all RFC3164-based syslog-ng macros.  Useful with the \u201cfallback\u201d sourcetype to aid in new filter development. t_JSON_5424 $(format-json \u2013scope rfc5424\u2013pair PRI=\u201d&lt;$PRI&gt;\u201d\u2013key ISODATE\u2013exclude DATE\u2013exclude FACILITY\u2013exclude PRIORITY) JSON output of all RFC5424-based syslog-ng macros; for use with RFC5424-compliant traffic. t_JSON_5424_SDATA $(format-json \u2013scope rfc5424\u2013pair PRI=\u201d&lt;$PRI&gt;\u201d\u2013key ISODATE\u2013exclude DATE\u2013exclude FACILITY\u2013exclude PRIORITY)\u2013exclude MESSAGE JSON output of all RFC5424-based syslog-ng macros except for MESSAGE; for use with RFC5424-compliant traffic."},{"location":"configuration/#data-resilience-local-disk-buffer-configuration","title":"Data Resilience - Local Disk Buffer Configuration","text":"<p>SC4S provides capability to minimize the number of lost events if the connection to all the Splunk Indexers goes down.  This capability utilizes the disk buffering feature of Syslog-ng. SC4S receives a response from the Splunk HTTP Event Collector (HEC) when a message is received successfully. If a confirmation message from the HEC endpoint is not received (or a \u201cserver busy\u201d reply, such as a \u201c503\u201d is sent), the load balancer will try the next HEC endpoint in the pool. If all pool members are exhausted (such as would occur if there were a full network outage to the HEC endpoints), events will queue to the local disk buffer on the SC4S Linux host. SC4S will continue attempting to send the failed events while it buffers all new incoming events to disk. If the disk space allocated to disk buffering fills up then SC4S will stop accepting new events and subsequent events will be lost. Once SC4S gets confirmation that events are again being received by one or more indexers, events will then stream from the buffer using FIFO queueing. The number of events in the disk buffer will reduce as long as the incoming event volume is less than the maximum SC4S (with the disk buffer in the path) can handle. When all events have been emptied from the disk buffer, SC4S will resume streaming events directly to Splunk.</p> <p>For more detail on the Syslog-ng behavior the documentation can be found here: https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.22/administration-guide/55#TOPIC-1209280</p> <p>SC4S has disk buffering enabled by default and it is strongly recommended that you keep it on, however this feature does have a performance cost. Without disk buffering enabled SC4S can handle up to 345K EPS (800 bytes/event avg) With \u201cNormal\u201d disk buffering enabled SC4S can handle up to 60K EPS (800 bytes/event avg) \u2013 This is still a lot of data!</p> <p>To guard against data loss it is important to configure the appropriate type and amount of storage for SC4S disk buffering. To estimate the storage allocation, follow these steps:</p> <ul> <li>Start with your estimated maximum events per second that each SC4S server will experience. Based on the maximum throughput of SC4S with disk buffering enabled, the conservative estimate for maximum events per second would be 60K (however, you should use the maximum rate in your environment for this calculation, not the max rate SC4S can handle). </li> <li>Next is your average estimated event size based on your data sources. It is common industry practice to estimate log events as 800 bytes on average. </li> <li>Then, factor in the maximum length of connectivity downtime you want disk buffering to be able to handle. This measure is very much dependent on your risk tolerance.</li> <li>Lastly, syslog-ng imposes significant overhead to maintain its internal data structures (primarily macros) so that the data can be properly \u201cplayed back\u201d upon network restoration.  This overhead currently runs at about 1.7x above the total storage size for the raw messages themselves, and can be higher for \u201cfallback\u201d data sources due to the overlap of syslog-ng macros (data fields) containing some or all of the original message.</li> </ul> <p>For example, to protect against a full day of lost connectivity from SC4S to all your indexers at maximum throughput the calculation would look like the following:</p> <p>60,000 EPS * 86400 seconds * 800 bytes * 1.7 = 6.4 TB of storage</p> <p>To configure storage allocation for the SC4S disk buffering, do the following:</p> <ul> <li>Edit the file /opt/sc4s/default/env_file</li> <li>Add the SC4S_DEST_SPLUNK_HEC_DISKBUFF_DISKBUFSIZE variable to the file and set the value to the number of bytes based on your estimation (e.g. 7050240000000 in the example above)</li> <li>Splunk does not recommend reducing the disk allocation below 500 GB</li> <li>Restart SC4S</li> </ul> <p>Given that in a connectivity outage to the Indexers events will be saved and read from disk until the buffer is emptied, it is ideal to use the fastest type of storage available. For this reason, NVMe storage is recommended for SC4S disk buffering.</p> <p>It is best to design your deployment so that the disk buffer will drain after connectivity is restored to the Splunk Indexers (while incoming data continues at the same general rate).  Since \u201cyour mileage may vary\u201d with different combinations of data load, instance type, and disk subsystem performance, it is good practice to provision a box that performs twice as well as is required for your max EPS. This headroom will allow for rapid recovery after a connectivity outage.</p>"},{"location":"configuration/#ebpf","title":"eBPF","text":"<p>eBPF is a feature that helps with congestion of single heavy stream of data by utilizing multithreading. Used with SC4S_SOURCE_LISTEN_UDP_SOCKETS. To leverage this feature you need host os to be able to use eBPF. Additional pre-requisite is running docker/podman in privileged mode.</p> Variable Values Description SC4S_ENABLE_EBPF=yes yes or no(default) use ebpf to leverage multithreading when consuming from a single connection SC4S_EBPF_NO_SOCKETS=4 integer sets number of threads to use, for optimal preformance it should not be less than value set for  SC4S_SOURCE_LISTEN_UDP_SOCKETS <p>To run docker/podman in privileged mode edit service file (/lib/systemd/system/sc4s.service). Add <code>--privileged</code> flag to docker/podman run command: <pre><code>ExecStart=/usr/bin/podman run \\\n        -e \"SC4S_CONTAINER_HOST=${SC4SHOST}\" \\\n        -v \"$SC4S_PERSIST_MOUNT\" \\\n        -v \"$SC4S_LOCAL_MOUNT\" \\\n        -v \"$SC4S_ARCHIVE_MOUNT\" \\\n        -v \"$SC4S_TLS_MOUNT\" \\\n        --privileged \\\n        --env-file=/opt/sc4s/env_file \\\n        --health-cmd=\"/healthcheck.sh\" \\\n        --health-interval=10s --health-retries=6 --health-timeout=6s \\\n        --network host \\\n        --name SC4S \\\n        --rm $SC4S_IMAGE\n</code></pre></p>"},{"location":"configuration/#misc-options","title":"Misc options","text":"<ul> <li><code>SC4S_LISTEN_STATUS_PORT</code> Change the \u201cstatus\u201d port used by the internal health check process default value is <code>8080</code></li> </ul>"},{"location":"dashboard/","title":"SC4S Metrics and Events Dashboard","text":"<p>The SC4S Metrics and Events Dashboard lets you monitor crucial metrics and event flows for all the SC4S instances sending data to a chosen Splunk platform.</p>"},{"location":"dashboard/#functionalities","title":"Functionalities","text":""},{"location":"dashboard/#overview-metrics","title":"Overview metrics","text":"<p> The dashboard displays the cumulative sum of received and dropped messages for all SC4S instances in a chosen interval and for the specified time range. By default the interval is set to 30 seconds and the time range is 15 minutes.</p> <p>The Received Messages panel can be used as a heartbeat metric. A healthy SC4S instance should send at least one message per 30 seconds. This metrics message is included in the count.</p> <p>The Dropped Messages panel should remain at a constant level of 0. If SC4S drops messages due to filters, slow performance, or for any other reason, the number of dropped messages will persist until the instance restarts. This panel does not include potential UDP messages dropped from the port buffer, which SC4S is not able to track.</p>"},{"location":"dashboard/#single-instance-metrics","title":"Single instance metrics","text":"<p> You can display the instance name and SC4S version for a chosen SC4S instance. SC4S is available in versions greater than or equal to 3.16.0.</p> <p>The dashboard also displays a timechart of deltas for received, queued, and dropped messages for a chosen SC4S instance.</p>"},{"location":"dashboard/#single-instance-events","title":"Single instance events","text":"<p> The dashboard helps to analyze traffic processed by an SC4S instance by visualizing the following events data:</p> <ul> <li>total number of events</li> <li>distributions of events by index</li> <li>trends of events by index</li> <li>data parsers in use</li> <li>applied tags</li> </ul>"},{"location":"dashboard/#installation","title":"Installation","text":"<ol> <li>In Splunk platform open <code>Search</code> -&gt; <code>Dashboards</code>.  </li> <li>Click on <code>Create New Dashboard</code> and make an empty dashboard. Be sure to choose <code>Classic Dashboards</code>.</li> <li>In the <code>Edit Dashboard</code> view go to <code>Source</code> and replace the initial xml with the contents of dashboard/dashboard.xml published in the SC4S repository.</li> <li>After saving the changes your dashboard will be ready to use.</li> </ol>"},{"location":"destinations/","title":"SC4S Destination Configuration","text":"<p>Splunk Connect for Syslog can be configured to utilize any destination available in syslog-ng OSE. The configuration system provides ease of use helpers to manage configuration for the three most common destination needs, Splunk HEC, RFC5424 Syslog, and Legacy BSD Syslog.</p> <p>In the getting started guide you configured the Splunk HEC \u201cDEFAULT\u201d destination to receive all traffic by default. The \u201cDEFAULT\u201d destination should be configured to accept all events to ensure that at least one destination has the event to avoid data loss due to misconfiguration. The following example demonstrates configuration of a second HEC destination where only \u201cselected\u201d data will be sent.</p>"},{"location":"destinations/#example-1-send-all-events","title":"Example 1 Send all events","text":"<pre><code>#Note \"OTHER\" should be a meaningful name\nSC4S_DEST_SPLUNK_HEC_OTHER_URL=https://splunk:8088\nSC4S_DEST_SPLUNK_HEC_OTHER_TOKEN=${SPLUNK_HEC_TOKEN}\nSC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY=no\nSC4S_DEST_SPLUNK_HEC_OTHER_MODE=GLOBAL\n</code></pre>"},{"location":"destinations/#example-2-send-only-cisco-ios-events","title":"Example 2 Send only cisco IOS Events","text":"<pre><code>#Note \"OTHER\" should be a meaningful name\nSC4S_DEST_SPLUNK_HEC_OTHER_URL=https://splunk:8088\nSC4S_DEST_SPLUNK_HEC_OTHER_TOKEN=${SPLUNK_HEC_TOKEN}\nSC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY=no\nSC4S_DEST_SPLUNK_HEC_OTHER_MODE=SELECT\nSC4S_DEST_CISCO_IOS_ALTERNATES=d_fmt_hec_OTHER\n</code></pre>"},{"location":"destinations/#example-3-send-only-cisco-ios-events-that-are-not-debug","title":"Example 3 Send only cisco IOS events that are not debug","text":"<pre><code>#Note \"OTHER\" should be a meaningful name\nSC4S_DEST_SPLUNK_HEC_OTHER_URL=https://splunk:8088\nSC4S_DEST_SPLUNK_HEC_OTHER_TOKEN=${SPLUNK_HEC_TOKEN}\nSC4S_DEST_SPLUNK_HEC_OTHER_TLS_VERIFY=no\nSC4S_DEST_SPLUNK_HEC_OTHER_MODE=SELECT\n</code></pre> <pre><code>#filename:\napplication sc4s-lp-cisco_ios_dest_fmt_other{{ source }}[sc4s-lp-dest-select-d_fmt_hec_other] {\n    filter {\n        'CISCO_IOS' eq \"${fields.sc4s_vendor}_${fields.sc4s_product}\"\n        #Match any cisco event that is not like \"%ACL-7-1234\"\n        and not message('^%[^\\-]+-7-');\n    };    \n};\n</code></pre>"},{"location":"destinations/#example-4-mcafee-epo-send-rfc5424-events-without-frames-to-third-party-system","title":"Example 4 Mcafee EPO send RFC5424 events without frames to third party system","text":"<p>Note in most cases when a destination requires syslog the requirement is referring to legacy BSD syslog (RFC3194) not standard syslog RFC5424</p> <p>The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful. </p> <pre><code>#env_file\nSC4S_DEST_SYSLOG_MYSYS_HOST=172.17.0.1\nSC4S_DEST_SYSLOG_MYSYS_PORT=514\nSC4S_DEST_SYSLOG_MYSYS_MODE=SELECT\n# set to #yes for ietf frames\nSC4S_DEST_SYSLOG_MYSYS_IETF=no \n</code></pre> <pre><code>#filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_syslog_msys.conf\napplication sc4s-lp-mcafee_epo_d_syslog_msys[sc4s-lp-dest-select-d_syslog_msys] {\n    filter {\n        'mcafee' eq \"${fields.sc4s_vendor}\"\n        and 'epo' eq \"${fields.sc4s_product}\"\n    };    \n};\n</code></pre>"},{"location":"destinations/#example-5-cisco-asa-send-to-a-third-party-siem","title":"Example 5 Cisco ASA send to a third party SIEM","text":"<p>The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful</p> <p>In most cases when a third party system needs \u201csyslog\u201d the requirement is to send \u201clegacy BSD\u201d as follows This is often refereed to as RFC3194 </p> <pre><code>#env_file\nSC4S_DEST_BSD_OLDSIEM_HOST=172.17.0.1\nSC4S_DEST_BSD_OLDSIEM_PORT=514\nSC4S_DEST_BSD_OLDSIEM_MODE=SELECT\n# set to #yes for ietf frames\n</code></pre> <pre><code>#filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_bsd_oldsiem.conf\napplication sc4s-lp-mcafee_epo_d_bsd_oldsiem[sc4s-lp-dest-select-d_bsd_oldsiem] {\n    filter {\n        'mcafee' eq \"${fields.sc4s_vendor}\"\n        and 'epo' eq \"${fields.sc4s_product}\"\n    };    \n};\n</code></pre>"},{"location":"destinations/#example-6-mcafee-epo-send-rfc5424-events-without-frames-to-third-party-system","title":"Example 6 Mcafee EPO send RFC5424 events without frames to third party system","text":"<p>The destination name is taken from the env var each destination must have a unique name regardless of type. This value should be short and meaningful</p> <pre><code>#env_file\nSC4S_DEST_SYSLOG_MYSYS_HOST=172.17.0.1\nSC4S_DEST_SYSLOG_MYSYS_PORT=514\nSC4S_DEST_SYSLOG_MYSYS_MODE=SELECT\n# set to #yes for ietf frames\nSC4S_DEST_SYSLOG_MYSYS_IETF=no \n</code></pre> <pre><code>#filename: /opt/sc4s/local/config/app_parsers/selectors/sc4s-lp-mcafee_epo_d_syslog_msys.conf\napplication sc4s-lp-mcafee_epo_d_syslog_msys[sc4s-lp-dest-select-d_syslog_msys] {\n    filter {\n        'cisco' eq \"${fields.sc4s_vendor}\"\n        and 'asa' eq \"${fields.sc4s_product}\"\n    };    \n};\n</code></pre>"},{"location":"destinations/#supported-simple-destination-configurations","title":"Supported Simple Destination configurations","text":"<p>SC4S Supports the following destination configurations via configuration. Any custom destination can be supported (defined by syslog-ng OSE)</p> <ul> <li>Splunk HTTP Event Collector (HEC)</li> <li>RFC5424 format without frames i.e. <code>&lt;166&gt;1 2022-02-02T14:59:55.000+00:00 kinetic-charlie - - - - %FTD-6-430003: DeviceUUID:</code></li> <li>RFC5424 format with frames also known as RFC6587 <code>123 &lt;166&gt;1 2022-02-02T14:59:55.000+00:00 kinetic-charlie - - - - %FTD-6-430003: DeviceUUID:</code></li> <li>RFC3164 (BSD format) <code>&lt;134&gt;Feb  2 13:43:05.000 horse-ammonia CheckPoint[26203]:</code></li> </ul>"},{"location":"destinations/#hec-destination-configuration","title":"HEC Destination Configuration","text":"Variable Values Description SC4S_DEST_SPLUNK_HEC_&lt;ID&gt;_URL url URL(s) of the Splunk endpoint, can be a single URL space separated list SC4S_DEST_SPLUNK_HEC_&lt;ID&gt;_TOKEN string Splunk HTTP Event Collector Token SC4S_DEST_SPLUNK_HEC_&lt;ID&gt;_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY yes(default) or no verify HTTP(s) certificate"},{"location":"destinations/#http-compression","title":"HTTP Compression","text":"<p>HTTP traffic compression helps to reduce network bandwidth usage. SC4S currently supports gzip for compressing transmitted traffic.\\ Using the \u2018gzip\u2019 compression algorithm can result in lower CPU load and increased utilization of RAM. The algorithm may also cause a decrease in performance. Tests observed a decrease in message processing speed by 6% to 7%.\\ Compression affects the content but does not affect the HTTP headers. Enable batch packet processing to make the solution particularly efficient, as this allows compression of a large number of logs at once.</p> Variable Values Description SC4S_DEST_SPLUNK_HEC_&lt;ID&gt;_HTTP_COMPRESSION; yes or no(default) compress outgoing HTTP traffic using gzip method"},{"location":"destinations/#syslog-standard-destination","title":"Syslog Standard destination.","text":"<p>Note: in many cases destinations incorrectly assert \u201csyslog\u201d support. IETF standards RFC5424, RFC5425, RFC6587 define the use of \u201csyslog\u201d as a network protocol. Often the actual configuration required is Legacy BSD syslog which is NOT a standard and was documented \u201chistorically\u201d in RFC3164 see BSD Destination section.</p> Variable Values Description SC4S_DEST_SYSLOG_&lt;ID&gt;_HOST fqdn or ip the FQDN or IP of the target SC4S_DEST_SYSLOG_&lt;ID&gt;_PORT number 601 (default when framed) 514 (default when not framed) SC4S_DEST_SYSLOG_&lt;ID&gt;_IETF yes,no default \u201cyes\u201d use IETF Standard frames SC4S_DEST_SYSLOG_&lt;ID&gt;_TRANSPORT tcp,udp,tls default tcp SC4S_DEST_SYSLOG_&lt;ID&gt;_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d"},{"location":"destinations/#bsd-legacy-destination-non-standard","title":"BSD legacy destination (Non standard)","text":"<p>Note: in many cases, destinations incorrectly assert \u201csyslog\u201d support. Internet Engineering Task Force standards RFC5424, RFC5425, and RFC6587 define the use of \u201csyslog\u201d as a network protocol. Often the actual configuration required is Legacy BSD syslog which is not a standard and was documented in RFC3164.</p> Variable Values Description SC4S_DEST_BSD_&lt;ID&gt;_HOST fqdn or ip the FQDN or IP of the target SC4S_DEST_BSD_&lt;ID&gt;_PORT number default 514 SC4S_DEST_BSD_&lt;ID&gt;_TRANSPORT tcp,udp,tls default tcp SC4S_DEST_BSD_&lt;ID&gt;_MODE string \u201cGLOBAL\u201d or \u201cSELECT\u201d"},{"location":"destinations/#configuration-of-filtered-alternate-destinations-advanced","title":"Configuration of Filtered Alternate Destinations (Advanced)","text":"<p>Though source-specific forms of the variables configured above will limit configured alternate destinations to a specific data source, there are cases where even more granularity is desired within a specific data source (e.g. to send all Cisco ASA \u201cdebug\u201d traffic to Cisco Prime for analysis).  This extra traffic may or may not be needed in Splunk.  To accommodate this use case, Filtered Alternate Destinations allow a filter to be supplied to redirect a portion of a given source\u2019s traffic to a list of alternate destinations (and, optionally, to prevent matching events from being sent to Splunk).  Again, these are configured through environment variables similar to the ones above:</p> Variable Values Description SC4S_DEST_&lt;VENDOR_PRODUCT&gt;_ALT_FILTER syslog-ng filter Filter to determine which events are sent to alternate destination(s) SC4S_DEST_&lt;VENDOR_PRODUCT&gt;_FILTERED_ALTERNATES Comma or space-separated list of syslog-ng destinations Send filtered events to alternate syslog-ng destinations using the VENDOR_PRODUCT syntax, e.g. <code>SC4S_DEST_CISCO_ASA_FILTERED_ALTERNATES</code> <ul> <li> <p>NOTE:  This is an advanced capability, and filters and destinations using proper syslog-ng syntax must be constructed prior to utilizing this feature.</p> </li> <li> <p>NOTE:  Unlike the standard alternate destinations configured above, the regular \u201cmainline\u201d destinations (including the primary HEC destination or configured archive destination (<code>d_hec</code> or <code>d_archive</code>)) are not included for events matching the configured alternate destination filter.  If an event matches the filter, the list of filtered alternate destinations completely replaces any mainline destinations including defaults and global or source-based standard alternate destinations.  Be sure to include them in the filtered destination list if desired.</p> </li> <li> <p>HINT:  Since the filtered alternate destinations completely replace the mainline destinations (including HEC to Splunk), a filter that matches all traffic can be used with a destination list that does not include the standard HEC destination to effectively turn off HEC for a given data source.</p> </li> </ul>"},{"location":"edge_processor/","title":"Edge Processor integration guide (Experimental)","text":""},{"location":"edge_processor/#intro","title":"Intro","text":"<p>You can use the <code>Edge Processor</code> to:</p> <ul> <li>Enrich log messages with extra data, such as adding a new field or overriding an index using <code>SPL2</code>.</li> <li>Filter log messages using <code>SPL2</code>.</li> <li>Send log messages to alternative destinations, for example, <code>AWS S3</code> or <code>Apache Kafka</code>.</li> </ul>"},{"location":"edge_processor/#how-it-works","title":"How it works","text":"<pre><code>stateDiagram\n    direction LR\n\n    SC4S: SC4S\n    EP: Edge Processor\n    Dest: Another destination\n    Device: Your device\n    S3: AWS S3\n    Instance: Instance\n    Pipeline: Pipeline with SPL2\n\n    Device --&gt; SC4S: Syslog protocol\n    SC4S --&gt; EP: HEC\n    state EP {\n      direction LR\n      Instance --&gt; Pipeline\n    }\n    EP --&gt; Splunk\n    EP --&gt; S3\n    EP --&gt; Dest</code></pre>"},{"location":"edge_processor/#set-up-the-edge-processor-for-sc4s","title":"Set up the Edge Processor for SC4S","text":"<p>SC4S using same protocol for communication with Splunk and Edge Processor. For that reason setup process will be very similar, but it have some differences.</p> Set up on Docker / PodmanSet up on Kubernetes <ol> <li>On the <code>env_file</code>, configure the HEC URL as IP of managed instance, that you registered on Edge Processor.</li> <li>Add your HEC token. You can find your token in the Edge Processor \u201cglobal settings\u201d page. </li> </ol> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://x.x.x.x:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> <ol> <li>Set up the Edge Processor on your <code>values.yaml</code> HEC URL using the IP of managed instance, that you registered on Edge Processor.</li> <li>Provide the hec_token. You can find this token on the Edge Processor\u2019s \u201cglobal settings\u201d page.</li> </ol> <pre><code>splunk:\n  hec_url: \"http://x.x.x.x:8088\"\n  hec_token: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  hec_verify_tls: \"no\"\n</code></pre>"},{"location":"edge_processor/#mtls-encryption","title":"mTLS encryption","text":"<p>Before setup, generate mTLS certificates. Server mTLS certificates should be uploaded to <code>Edge Processor</code> and client certifcates should be used with <code>SC4S</code>.</p> <p>Rename the certificate files. SC4S requires the following names:</p> <ul> <li><code>key.pem</code> - client certificate key</li> <li><code>cert.pem</code> - client certificate</li> <li><code>ca_cert.pem</code> - certificate authority</li> </ul> Set up on Docker / PodmanSet up on Kubernetes <ol> <li>Use HTTPS in HEC url: <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://x.x.x.x:8088</code>.</li> <li>Move your clients mTLS certificates (<code>key.pem</code>, <code>cert.pem</code>, <code>ca_cert.pem</code>) to <code>/opt/sc4s/tls/hec</code>.</li> <li>Mount <code>/opt/sc4s/tls/hec</code> to <code>/etc/syslog-ng/tls/hec</code> using docker/podman volumes.</li> <li>Define mounting mTLS point for HEC: <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_MOUNT=/etc/syslog-ng/tls/hec</code>.</li> <li>Start or restart SC4S.</li> </ol> <ol> <li>Add the secret name of the mTLS certificates to the <code>values.yaml</code> file:</li> </ol> <pre><code>splunk:\n  hec_url: \"https://x.x.x.x:8088\"\n  hec_token: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n  hec_tls: \"hec-tls-secret\"\n</code></pre> <ol> <li>Add your mTLS certificates to the <code>charts/splunk-connect-for-syslog/secrets.yaml</code> file:</li> </ol> <pre><code>hec_tls:\n  secret: \"hec-tls-secret\"\n  value:\n    key: |\n      -----BEGIN PRIVATE KEY-----\n      Exmaple key\n      -----END PRIVATE KEY-----\n    cert: |\n      -----BEGIN CERTIFICATE-----\n      Exmaple cert\n      -----END CERTIFICATE-----\n    ca: |\n      -----BEGIN CERTIFICATE-----\n      Example ca\n      -----END CERTIFICATE-----\n</code></pre> <ol> <li>Encrypt your <code>secrets.yaml</code>:</li> </ol> <pre><code>ansible-vault encrypt charts/splunk-connect-for-syslog/secrets.yaml\n</code></pre> <ol> <li> <p>Add the IP address for your cluster nodes to the inventory file <code>ansible/inventory/inventory_microk8s_ha.yaml</code>.</p> </li> <li> <p>Deploy the Ansible playbook:</p> </li> </ol> <pre><code>ansible-playbook -i ansible/inventory/inventory_microk8s_ha.yaml ansible/playbooks/microk8s_ha.yml --ask-vault-pass\n</code></pre>"},{"location":"edge_processor/#scaling-edge-processor","title":"Scaling Edge Processor","text":"<p>To scale you can distribute traffic between Edge Processor managed instances. To set this up, update the HEC URL with a comma-separated list of URLs for your managed instances.</p> Set up on Docker/PodmanSet up on Kubernetes <p>Update HEC URL in <code>env_file</code>:</p> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://x.x.x.x:8088,http://x.x.x.x:8088,http://x.x.x.x:8088\n</code></pre> <p>Update HEC URL in <code>values.yaml</code>:</p> <pre><code>splunk:\n  hec_url: \"http://x.x.x.x:8088,http://x.x.x.x:8088,http://x.x.x.x:8088\"\n</code></pre>"},{"location":"experiments/","title":"Current Experimental Features","text":""},{"location":"experiments/#3120","title":"&gt; 3.12.0","text":"<p><code>SC4S_USE_NAME_CACHE=yes</code> supports IPv6.</p>"},{"location":"experiments/#300","title":"&gt; 3.0.0","text":""},{"location":"experiments/#ebpf","title":"eBPF","text":"<p>eBPF is a feature that leverages Linux kernel infrastructure to evenly distribute the load especially in cases when there is a huge stream of messages incoming from a single appliance. Prerequisite for using eBPF feature is a host machine with os that supports eBPF. It should be used only in cases when other ways of sc4s tuning are failing. Please refer to the instruction for configuration details.  To learn more visit this blog post.</p>"},{"location":"experiments/#sc4s-lite","title":"SC4S Lite","text":"<p>In the new 3.0.0 update, we\u2019ve introduced SC4S Lite. It\u2019s designed for those who prefer speed and custom filters over the pre-set ones that come with the standard SC4S. It\u2019s basically the same as our default version, minus the pre-defined filters and complex app_parser topics.More information can be found under dedicated page.</p>"},{"location":"experiments/#2130","title":"&gt; 2.13.0","text":"<ul> <li>In env_file set <code>SC4S_USE_NAME_CACHE=yes</code> to enable caching last valid host string and replacing nill, null, or ipv4 with last good value and stores this information in the hostip.sqlite file. <ul> <li>Benefit: More correct host name values in Splunk when source vendor fails to provide valid syslog message</li> <li>Risk: Potential disk I/O usage (space, iops) Potential reduction in throughput when a high proportion of events are incomplete.</li> </ul> </li> <li>To clear hostip.sqlite file, set <code>SC4S_CLEAR_NAME_CACHE=yes</code> flag in env_file. This action will automatically delete  the hostip.sqlite file when sc4s restarts.</li> <li>In env_file set <code>SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG=yes</code> To enable additional post processing to merge multiline vmware events. Recommend also enabling <code>SC4S_USE_NAME_CACHE=yes</code> as many events can be malformed or missing host name</li> <li>In env_file set <code>SC4S_USE_VPS_CACHE=yes</code> To enable automatic configuration of vendor_product by source where possible. This feature caches \u201cvendor\u201d and \u201cproduct\u201d fields from to use in determination of the best values for  generic linux events for example without this feature the \u201cvendor product by host\u201d app parser must be configured to identify esx hosts so that esx SSHD events can be routed using the meta key <code>vmware_vsphere_nix_syslog</code> with this feature enabled a common event such containing \u201cprogram=vpxa\u201d will cache this value. <ul> <li>Benefit: Less config interaction</li> <li>Risk: Potential disk I/O usage (space, iops) Potential reduction in throughput when a high proportion of events are incomplete.</li> <li>Risk: misidentification due to load balancers and relay sources. </li> </ul> </li> <li><code>SC4S_SOURCE_PROXYCONNECT=yes</code> for TCP and TLS connection expect \u201cPROXY CONNECT\u201d to provide the original client IP in SNAT load balancing</li> </ul>"},{"location":"faq/","title":"Splunk Connect for Syslog (SC4S) Frequently Asked Questions","text":"<p>Q: The Universal Forwarder/files based architecture has been the documented Splunk best practice for a long time.  Why switch to a HTTP Event Collector (HEC) based architecture?</p> <p>A: Using HEC to stream events directly to the Indexers provides superior load balancing which has shown to produce dramatically more even data distribution across the Indexers. This even distribution results in significantly enhanced search performance. This benefit is especially valuable in large Splunk deployments.</p> <p>The HEC architecture designed into SC4S is also far easier to administer with newer versions of syslog-ng, which SC4S takes advantage of.  There are far fewer opportunities for mis-configuration, resulting in higher overall performance and customer adoption.</p> <p>Lastly, HEC (and in particular, the \u201c/event\u201d endpoint) offers the opportunity for a far richer data stream to Splunk, with lower resource utilization at ingest.  This rich data stream can be taken advantage of in next-generation TAs. </p> <p>Q: Is the Splunk HTTP Event Collector (HEC) as reliable as the Splunk Universal Forwarder?</p> <p>A: HEC utilizes standard HTTP mechanisms to confirm that the endpoint is responsive before sending data. The HEC architecture allows for the use of an industry standard load balancer between SC4S and the Indexer, or the included load balancing capability built into SC4S itself.</p> <p>Q: What if my team doesn\u2019t know how to manage containers?</p> <p>A: SC4S supports both container-based and \u201cbring-your-own-environment\u201d (BYOE) deployment methods. That said, using a runtime like Podman to deploy and manage SC4S containers is exceptionally easy even for those with no prior \u201ccontainer experience\u201d. Our application of container technology behaves much like a packaging system. The interaction is mostly via \u201csystemctl\u201d commands a Linux admin would use for other common administration activities. The best approach is to try it out in a lab to see what the experience is like for yourself!</p> <p>BYOE is intended for advanced deployments that can not use the Splunk container for some reason. One possible reason is a need to \u201cfork\u201d SC4S in order to implement heavy bespoke customization. Though many will initially gravitate toward BYOE because managing config files and syslog-ng directly is \u201cwhat they know\u201d, most enterprises will have the best experience using the container approach.</p> <p>Q: Can my team use SC4S if we are Windows only shop?</p> <p>A: You can now run Docker on Windows! Microsoft has introduced public preview technology for Linux containers on Windows. Alternatively, a minimal Centos/Ubuntu Linux VM running on Windows hyper-v is a reliable production-grade choice. </p> <p>Q: My company has the traditional UF/files based syslog architecture deployed and running, should I rip/replace a working installation with SC4S?</p> <p>A: Generally speaking, if a deployment is working and you are happy with it, it\u2019s best to leave it as is until there is need for major deployment changes such as higher scale. That said, the search performance gains realized from better data distribution is a benefit not to be overlooked. If Splunk users have complained about search performance or you are curious about the possible performance gains, we recommend doing an analysis of the data distribution across the indexers.</p> <p>It may make sense to upgrade to SC4S if there is a change in administration as well.  Properly architecting a performant UF/files syslog-ng deployment is difficult, and an administrative personnel change offers the opportunity to \u201cmake a break\u201d  to SC4S, where a new set of administrators would otherwise be tasked with understanding the existing (likely complicated) architecture.</p> <p>Q: What is the best way to migrate to SC4S from an existing syslog architecture?</p> <p>A: When exploring migration to SC4S we strongly recommend experimentation in a lab prior to deployment to production. There are a couple of approaches to consider: </p> <p>One option is to stand up and configure the new SC4S infrastructure for all your sources, then confirm all the sourcetypes are being indexed as expected, and finally stop the existing syslog servers. This big bang approach may result in the fewest duplicate events in Splunk vs other options. In some large or complex environments this may not be feasible however. </p> <p>A second option is to start with the sources currently sending events on port 514 (the default). In this case you would stand up the new SC4S infrastructure in its default configuration, confirm all the sourcetypes are being indexed as expected, then retire the old syslog servers listening on port 514. Once the 514 sources are complete you can move on to migrating any other sources one by one. To migrate these other sources you would configure SC4S filters to explicitly identify them either via unique port, hostID or CIDR block. Again, once you confirm that each sourcetype is successfully being indexed then you may disable the old syslog configurations for that source. </p> <p>Q: How can SC4S be deployed to provide high availability?</p> <p>A: It is challenging to provide HA for syslog because the syslog protocol itself was not designed with HA as a goal. See Performant AND Reliable Syslog UDP is best for an excellent overview of this topic.</p> <p>The gist is that the protocol itself limits the extent to which you can make any syslog collection architecture HA; at best it can be made \u201cmostly available\u201d.  Think of syslog as MP3 \u2013 it is a \u201clossy\u201d protocol and there is nothing you can do to restore it to CD quality (lossless). Some have attempted to implement HA via front-side load balancers; please don\u2019t!  This is the most common architectural mistake folks make when architecting large-scale syslog data collection. So \u2013 how to make it \u201cmostly available\u201d?  Keep it simple, and use OS clustering (shared IP) or even just VMs with vMotion.  This simple architecture will encounter far less data loss over time than more complicated schemes. Another possible option being evaluated is containerization HA schemes for SC4S (centered around microk8s) that will take some of the admin burden of clustering away \u2013 but it is still OS clustering under the hood.</p> <p>Q: I\u2019m worried about data loss if SC4S goes down. Could I feed syslog to redundant SC4S servers to provide HA, without creating duplicate events in Splunk?</p> <p>A: In many/most system design decisions there is some level of compromise. Any network protocol that doesn\u2019t have an application level ack will lose data, as speed was selected over reliability in the design, this is the case with syslog. Use of a clustered IP with an active/passive node will however offer a level of resilience while keeping complexity to a minimum.  It could be possible to implement a far more complex solution utilizing an additional intermediary technology like Kafka, however the costs may outweigh the real world benefits.</p> <p>Q: Can the SC4S container be deployed using OpenShift or K8s?</p> <p>A: There are a number of reasons that OpenShift/K8s are not a good fit for syslog, SNMP or SIP. They can\u2019t use UDP and TCP on the same port which breaks multiple Bluecoat and Cisco feeds among others. Layered networking shrinks the maximum UDP message which causes data loss due to truncation and drops Long lived TCP connections cause well known problems OpenShift doesn\u2019t actually use Podman, it uses a library to wrap OCI that Podman also uses. this wrapper around the wrapper has some shortcomings that prevent the service definitions SC4S requires. Basically, K8s was built for a very different set of problems than syslog</p> <p>Q: If the XL reference HW can handle just under 1 TB/day how can SC4S be scaled to handle large deployments of many TB/day?</p> <p>A: SC4S is a distributed architecture. SC4S instances should be deployed in the same VLAN as the source devices. This means that each SC4S instance will only see a subset of the total syslog traffic in a large deployment. Even in a 100+ TB deployment the individual SC4S instances will see loads in GB/day not TB/day.</p> <p>Q: How are security vulnerabilities handled with SC4S?</p> <p>A: SC4S is comprised of several components including RHL, Syslog-ng and temporized configurations. If a vulnerability is found in the SC4S configurations, they will be given a critical priority in the Development queue. If vulnerabilities are identified in the third party components (RHL, Syslog-ng, etc.) the fixed versions will be pulled in upon the next SC4S release. Fixed security issues are identified by \u201c[security]\u201d in SC4S release notes.</p> <p>Q: SC4S is being blocked by <code>fapolicyd</code>, how do I fix that? Create a rule that allows running sc4s in fapolicyd configuration: * Create the file <code>/etc/fapolicyd/rules.d/15-sc4s.rules</code> . * Put this into the file: <code>allow perm=open exe=/ : dir=/usr/lib64/ all trust=1</code> . * Run <code>fagenrules --load</code> to load the new rule. * Run <code>systemctl restart fapolicyd</code> to restart the process. * Start <code>sc4s systemctl start sc4s</code> and verify there are no errors systemctl status sc4s.</p> <p>Q: I am facing a unique issue that my postfilter configuration is not working although i don\u2019t have any postfilter for the mentioned source?</p> <p>A: There is a possibility that there is OOB postfilter for the source which will be applied , the same can be validated by checking the value of sc4s_tags in splunk UI, to fix this Please use a new topic called <code>[sc4s-finalfilter]</code> please don\u2019t use it in any other case as it can add the cost of the processing of data</p> <p>Q: Where the config for the vendors should be placed? There are folders of app-parsers and its directories. Which one to use? Does this also mean that csv files for metadata are no longer required?</p> <p>A: It should be placed inside <code>/opt/sc4s/local/config/*/.conf</code>. Most of the folders are placeholder and it will work in any of these folders if it has .conf extension. It is required but it should be placed in <code>local/context/*.csv</code>. Using splunk_metadata.csv is good for metadata override but it is recommended to use .conf file for everything else in place of other csv files.</p> <p>Q: Can we have a file using which we can create all default indexes in one go?</p> <p>A: Refer this file which contains all indexes being created in one go. Also, above file has lastChanceIndex configured, please use it only if it fits your requirement. If not, then please discard the use of lastChanceIndex. For more information on this file, please refer Splunk docs.</p>"},{"location":"lb/","title":"About using load balancers","text":"<p>Load balancers are not a best practice for SC4S. The exception to this is a narrow use case where the syslog server is exposed to untrusted clients on the internet, for example, with Palo Alto Cortex.</p>"},{"location":"lb/#considerations","title":"Considerations","text":"<ul> <li>UDP can only pass a load balancer using DNAT and source IP must be preserved. If you use this configuration, the load balancer becomes a new single point of failure.</li> <li>TCP/TLS can use either a DNAT configuration or SNAT with \u201cPROXY\u201d Protocol enabled <code>SC4S_SOURCE_PROXYCONNECT=yes</code>. </li> <li>TCP/TLS load balancers do not consider the weight of individual connection load and are frequently biased to one instance. Vertically scale all members in a single resource pool to accommodate the full workload.</li> </ul>"},{"location":"lb/#alternatives","title":"Alternatives","text":"<p>The best deployment model for high availability is a Microk8s based deployment with MetalLB in BGP mode. This model uses a special class of load balancer that is implemented as destination network translation.</p>"},{"location":"lite/","title":"SC4S Lite","text":""},{"location":"lite/#about-sc4s-lite","title":"About SC4S Lite","text":"<p>SC4S Lite provides a scalable, performance-oriented solution for ingesting syslog data into Splunk. Pluggable modular parsers offer you the flexibility to incorporate custom data processing logic to suit specific use cases.</p>"},{"location":"lite/#architecture","title":"Architecture","text":""},{"location":"lite/#sc4s-lite_1","title":"SC4S Lite","text":"<p>SC4S Lite provides a lightweight, high-performance SC4S solution.</p>"},{"location":"lite/#pluggable-modules","title":"Pluggable Modules","text":"<p>Pluggable modules are predefined modules that you can enable and disable through configuration files. Each pluggable module represents a set of parsers for each vendor that supports SC4S. You can only enable or disable modules, you cannot create new modules or update existing ones. For more information see the pluggable modules documentation .</p>"},{"location":"lite/#splunk-enterprise-or-splunk-cloud","title":"Splunk Enterprise or Splunk Cloud","text":"<p>You configure SC4S Lite to send syslog data to Splunk Enterprise or Splunk Cloud. The Splunk Platform provides comprehensive analysis, searching, and visualization of your processed data.</p>"},{"location":"lite/#how-sc4s-lite-processes-your-data","title":"How SC4S Lite processes your data","text":"<ol> <li>Source systems send syslog data to SC4S Lite. The data may be transmitted using UDP, TCP, or RELP, depending on your system\u2019s capabilities and configurations.</li> <li>SC4S Lite receives the syslog data and routes it through the appropriate parsers, as defined by you during configuration.</li> <li>The parsers in the pluggable module process the data, such as parsing, filtering, and enriching the data with metadata.</li> <li>SC4S Lite forwards the processed syslog data to the Splunk platform over the HTTP Event Collector (HEC).</li> </ol>"},{"location":"lite/#security-considerations","title":"Security considerations","text":"<p>SC4S Lite is built on an Alpine lightweight container which has very little vulnerability. SC4S Lite supports secure syslog data transmission protocols such as RELP and TLS over TCP to protect your data in transit. Additionally, the environment in which SC4S Lite is deployed enhances data security.</p>"},{"location":"lite/#scalability-and-performance","title":"Scalability and performance","text":"<p>SC4S Lite provides superior performance and scalability thanks to the lightweight architecture and pluggable parsers, which distribute the processing load. It is also packaged with eBPF functionality to further enhance performance. Note that actual performance may depend on factors such as your server capacity and network bandwidth.</p>"},{"location":"lite/#implement-sc4s-lite","title":"Implement SC4S Lite","text":"<p>To implementat of SC4S Lite:</p> <ol> <li>Set up the SC4S Lite environment.</li> <li>Install SC4S Lite following the instructions for your chosen environment with the following changes:</li> </ol> <ul> <li>In the service file for Podman or Docker replace references of standard container image (<code>container2</code> or <code>container3</code>) with <code>container3lite</code>.</li> <li>For MicroK8s replace reference to standard image in <code>values.yaml</code> file.</li> </ul> <ol> <li>Configure source systems to send syslog data to SC4S Lite.</li> <li>Enable or disable your pluggable modules. All pluggable modules are enabled by default.</li> <li>Test the setup to ensure that your syslog data is correctly received, processed, and forwarded to Splunk.</li> </ol>"},{"location":"performance/","title":"Performance and Sizing","text":"<p>Performance testing against our lab configuration produces the following results and limitations. </p>"},{"location":"performance/#tested-configurations","title":"Tested Configurations","text":""},{"location":"performance/#splunk-cloud-noah","title":"Splunk Cloud Noah","text":""},{"location":"performance/#environment","title":"Environment","text":"<ul> <li>Loggen (syslog-ng 3.25.1) - m5zn.3xlarge</li> <li>SC4S(2.30.0) + podman (4.0.2) - m5zn family</li> <li>SC4S_DEST_SPLUNK_HEC_WORKERS=10 (default)</li> <li>Splunk Cloud Noah 8.2.2203.2 - 3SH + 3IDX</li> </ul> <pre><code>/opt/syslog-ng/bin/loggen -i --rate=100000 --interval=1800 -P -F --sdata=\"[test name=\\\"stress17\\\"]\" -s 800 --active-connections=10 &lt;local_hostmane&gt; &lt;sc4s_external_tcp514_port&gt;\n</code></pre>"},{"location":"performance/#result","title":"Result","text":"SC4S instance root networking slirp4netns networking m5zn.large average rate = 21109.66 msg/sec, count=38023708, time=1801.25, (average) msg size=800, bandwidth=16491.92 kB/sec average rate = 20738.39 msg/sec, count=37344765, time=1800.75, (average) msg size=800, bandwidth=16201.87 kB/sec m5zn.xlarge average rate = 34820.94 msg/sec, count=62687563, time=1800.28, (average) msg size=800, bandwidth=27203.86 kB/sec average rate = 35329.28 msg/sec, count=63619825, time=1800.77, (average) msg size=800, bandwidth=27601.00 kB/sec m5zn.2xlarge average rate = 71929.91 msg/sec, count=129492418, time=1800.26, (average) msg size=800, bandwidth=56195.24 kB/sec average rate = 70894.84 msg/sec, count=127630166, time=1800.27, (average) msg size=800, bandwidth=55386.60 kB/sec m5zn.2xlarge average rate = 85419.09 msg/sec, count=153778825, time=1800.29, (average) msg size=800, bandwidth=66733.66 kB/sec average rate = 84733.71 msg/sec, count=152542466, time=1800.26, (average) msg size=800, bandwidth=66198.21 kB/sec"},{"location":"performance/#splunk-enterprise","title":"Splunk Enterprise","text":""},{"location":"performance/#environment_1","title":"Environment","text":"<ul> <li>Loggen (syslog-ng 3.25.1) - m5zn.large</li> <li>SC4S(2.30.0) + podman (4.0.2) - m5zn family</li> <li>SC4S_DEST_SPLUNK_HEC_WORKERS=10 (default)</li> <li>Splunk Enterprise 9.0.0 Standalone</li> </ul> <pre><code>/opt/syslog-ng/bin/loggen -i --rate=100000 --interval=600 -P -F --sdata=\"[test name=\\\"stress17\\\"]\" -s 800 --active-connections=10 &lt;local_hostmane&gt; &lt;sc4s_external_tcp514_port&gt;\n</code></pre>"},{"location":"performance/#result_1","title":"Result","text":"SC4S instance root networking slirp4netns networking m5zn.large average rate = 21511.69 msg/sec, count=12930565, time=601.095, (average) msg size=800, bandwidth=16806.01 kB/sec  average rate = 21583.13 msg/sec, count=12973491, time=601.094, (average) msg size=800, bandwidth=16861.82 kB/sec average rate = 20738.39 msg/sec, count=37344765, time=1800.75, (average) msg size=800, bandwidth=16201.87 kB/sec m5zn.xlarge average rate = 37514.29 msg/sec, count=22530855, time=600.594, (average) msg size=800, bandwidth=29308.04 kB/sec  average rate = 37549.86 msg/sec, count=22552210, time=600.594, (average) msg size=800, bandwidth=29335.83 kB/sec average rate = 35329.28 msg/sec, count=63619825, time=1800.77, (average) msg size=800, bandwidth=27601.00 kB/sec m5zn.2xlarge average rate = 98580.10 msg/sec, count=59157495, time=600.096, (average) msg size=800, bandwidth=77015.70 kB/sec  average rate = 99463.10 msg/sec, count=59687310, time=600.095, (average) msg size=800, bandwidth=77705.55 kB/sec average rate = 84733.71 msg/sec, count=152542466, time=1800.26, (average) msg size=800, bandwidth=66198.21 kB/sec"},{"location":"performance/#guidance-on-sizing-hardware","title":"Guidance on sizing hardware","text":"<ul> <li>Though vCPU (hyper threading) was used in these examples, syslog processing is a CPU-intensive task and resource oversubscription through sharing is not advised.</li> <li>The size of the instance must be larger than the absolute peak to prevent data loss; most sources cannot buffer during traffic congestion.</li> <li>CPU Speed is critical; slower or faster CPUs will impact throughput.</li> <li>Not all sources are equal in resource utilization. Well-formed Legacy BSD syslog messages were used in this test, but many sources are not syslog compliant and will require additional resources to process.</li> </ul>"},{"location":"pluggable_modules/","title":"Working with pluggable modules","text":"<p>SC4S Lite pluggable modules are predefined modules that you can enable or disable by modifying your <code>config.yaml</code> file. This file contains a list of add-ons. See the example and list of available pluggable modules in (config.yaml reference file) for more information. Once you update  <code>config.yaml</code>, you mount it to the Docker container and override <code>/etc/syslog-ng/config.yaml</code>.</p>"},{"location":"pluggable_modules/#install-sc4s-lite-using-docker-compose","title":"Install SC4S Lite using Docker Compose","text":"<p>The installation process is identical to the installation process for Docker Compose for SC4S with the following modifications.</p> <ul> <li> <p>Use the SC4S Lite image instead of the SC4S image: <pre><code>image: ghcr.io/splunk/splunk-connect-for-syslog/container3lite\n</code></pre></p> </li> <li> <p>Mount your <code>config.yaml</code> file with your add-ons to <code>/etc/syslog-ng/config.yaml</code>:</p> </li> </ul> <pre><code>volumes:\n    - /path/to/your/config.yaml:/etc/syslog-ng/config.yaml\n</code></pre>"},{"location":"pluggable_modules/#kubernetes","title":"Kubernetes:","text":"<p>The installation process is identical to the installation process for Kubernetes for SC4S with the following modifications:</p> <ul> <li> <p>Use the SC4S Lite image instead of SC4S in <code>values.yaml</code>: <pre><code>image:\n  repository: ghcr.io/splunk/splunk-connect-for-syslog/container3lite\n</code></pre></p> </li> <li> <p>Mount <code>config.yaml</code>. Add an <code>addons</code> section inside <code>sc4s</code> in <code>values.yaml</code>:</p> </li> </ul> <pre><code>sc4s:\n    addons:\n        config.yaml: |-\n            ---\n            addons:\n                - cisco\n                - paloalto\n                - dell\n</code></pre>"},{"location":"upgrade/","title":"Upgrading Splunk Connect for Syslog","text":"<p>Splunk Connect for Syslog is updated regularly using a CI/CD development process.  The notes below outline significant changes that must be taken into account prior and after an upgrade.  Ensure to follow specific instructions below to ensure a smooth transition to a new version of SC4S in production.</p>"},{"location":"upgrade/#upgrade-process","title":"Upgrade process","text":"<p>Check the current version of SC4S by running <code>sudo &lt;docker or podman&gt; logs SC4S</code>. For the latest version, use the <code>latest</code> tag for the SC4S image in the sc4s.service unit file:</p> <pre><code>[Service]\nEnvironment=\"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\"\n</code></pre> <p>Restart the service <code>sudo systemctl restart sc4s</code></p> <p>Using the \u201c3\u201d version is recommended, but a specific version can be set in the unit file if desired:</p> <pre><code>[Service]\nEnvironment=\"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container3:3.0.0\"\n</code></pre> <p>See the release information for more detail.</p>"},{"location":"upgrade/#upgrade-notes","title":"Upgrade Notes","text":"<p>Version 3 does not introduce any breaking change. To upgrade to version 3 review service file and change container reference from <code>container2</code> to <code>container3</code>. For a step by step guide see here. Need up migrating legacy \u201clog paths\u201d or v1 app-parsers for v2. Open an issue with the original config attached and a compressed pcap of sample data for testing and we will evaluate inclusion of the source in an upcoming release.</p>"},{"location":"upgrade/#upgrade-from-2230","title":"Upgrade from &lt;2.23.0","text":"<ul> <li>Vmware vsphere fix esx and vcenter sourcetype for TA compatibility</li> </ul>"},{"location":"upgrade/#upgrade-from-2","title":"Upgrade from &lt;2","text":"<ul> <li>Before upgrading to 2.x review sc4s.service and manually update differences compared to current doc</li> <li>EXPERIMENTAL SNMP Trap feature has been removed migrate to Splunk Connect for SNMP</li> <li>Legacy \u201cgomplate\u201d log path template support was deprecated in 1.x and has been removed in 2.x log paths must be migrated to app-parser style config prior to upgrade</li> <li>Check env_file for \u201cMICROFOCUS_ARCSIGHT\u201d variables and replace with CEF variables see source doc</li> <li>Remove old style \u201cCISCO_*_LEGACY\u201d from env_file and replace per docs</li> <li>New images will no longer be published to docker.io please review current getting started docs and update the sc4s.service file accordingly</li> <li>Internal metrics will now use \u201cmulti\u201d format by default if using unsupported versions of Splunk &lt;8.1 see configuration doc to revert to \u201cevent\u201d or \u201csingle\u201d format.</li> <li>Internal metrics will now use the _metrics index by default update vendor_product key \u2018sc4s_metrics\u2019 to change the index</li> <li>Deprecated use of vendor_product_by_source for null queue or dropping events see See Filtering events from output this use will be removed in v3</li> <li>Deprecated use of vendor_product_by_source for identification of source by host/ip see new app-parser syntax documented per applicable product</li> <li>Deprecated use of <code>SPLUNK_HEC_ALT_DESTS</code> this variable is no longer used and will be ignored</li> <li>Deprecated use of <code>SC4S_DEST_GLOBAL_ALTERNATES</code> this variable will be removed in future major versions see Destinations section in configuration</li> <li>Corrected Vendor/Product keys BREAKING Please see source doc pages and revise configuration as part of upgrade</li> <li>Zscaler (multiple changes)</li> <li>dell_emc_powerswitch_n</li> <li>F5_BIGIP</li> <li>INFOBLOX</li> <li>Dell RSA SecureID</li> <li>ubiquiti</li> <li>SC4S will now use \u201csplunk as the vendor value, \u201csc4s\u201d as the product</li> <li>Fireye HX</li> <li>Juniper</li> <li>ossec</li> <li>Palo Alto Networks</li> <li>Pulse Connect</li> <li>ricoh</li> <li>tanium</li> <li>tintri</li> <li>Vmware esx,vcenter,nsx,horizon</li> <li>Wallix Bastion</li> <li>Internal Changes</li> <li><code>.dest_key</code> field is no longer used</li> <li><code>sc4s_vendor_product</code> is read only and will be removed</li> <li><code>sc4s_vendor</code> new contains \u201cvendor\u201d portion of vendor_product</li> <li><code>sc4s_vendor_product</code> new contains \u201cproduct\u201d portion of vendor product</li> <li><code>sc4s_class</code> new contains additional data previously concatenated to vendor_product</li> <li>removed <code>meta_key</code></li> <li>Custom \u201capp-parsers\u201d Critical Change</li> </ul> <pre><code>#Current app parsers contain one or more lines\nvendor_product('value_here')\n#This must change to failure to make this change will prevent sc4s from starting\nvendor('value')\nproduct('here')\n</code></pre>"},{"location":"v3_upgrade/","title":"Upgrading Splunk Connect for Syslog v2 -&gt; v3","text":""},{"location":"v3_upgrade/#upgrade-process-for-version-newer-than-230","title":"Upgrade process (for version newer than 2.3.0)","text":"<p>In general the upgrade process consists of three steps: - change of container version - restart of service - validation NOTE: Version 3 of SC4S is using alpine linux distribution as base image in opposition to previous versions which used UBI (Red Hat) image.</p>"},{"location":"v3_upgrade/#dockerpodman","title":"Docker/Podman","text":""},{"location":"v3_upgrade/#update-container-image-version","title":"Update container image version","text":"<p>In the service file: <code>/lib/systemd/system/sc4s.service</code> container image reference should be updated to version 3 with <code>latest</code> tag: <pre><code>[Service]\nEnvironment=\"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\"\n</code></pre></p>"},{"location":"v3_upgrade/#restart-sc4s-service","title":"Restart sc4s service","text":"<p>Restart the service: <code>sudo systemctl restart sc4s</code></p>"},{"location":"v3_upgrade/#validate","title":"Validate","text":"<p>After the above command is executed successfully, the following information with the version becomes visible in the container logs: <code>sudo podman logs SC4S</code> for podman or <code>sudo docker logs SC4S</code> for docker. Expected output: <pre><code>SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:fallback...\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:events...\nsyslog-ng checking config\nsc4s version=3.0.0\nstarting goss\nstarting syslog-ng \n</code></pre></p> <p>If you are upgrading from version lower than 2.3.0 please refer to this guide.</p>"},{"location":"gettingstarted/","title":"Before you start","text":""},{"location":"gettingstarted/#getting-started","title":"Getting Started","text":"<p>Splunk Connect for Syslog (SC4S) is a distribution of syslog-ng that simplifies getting your syslog data into Splunk Enterprise and Splunk Cloud. SC4S provides a runtime-agnostic solution that lets you deploy using the container runtime environment of choice and a configuration framework. This lets you process logs out-of-the-box from many popular devices and systems.</p>"},{"location":"gettingstarted/#planning-deployment","title":"Planning Deployment","text":"<p>Syslog can refer to multiple message formats as well as, optionally, a wire protocol for event transmission between computer systems over UDP, TCP, or TLS. This protocol minimizes overhead on the sender, favoring performance over reliability. This means any instability or resource constraint can cause data to be lost in transmission.</p> <ul> <li>When practical and cost-effective, place the sc4s instance in the same VLAN as the source device.</li> <li>Avoid crossing a Wireless network, WAN, Firewall, Load Balancer, or inline IDS.</li> <li>If you reguire high availability for SC4S, implement multi-node clustering.</li> <li>Avoid TCP except where the source is unable to contain the event to a single UDP packet.</li> <li>Avoid TLS except where the event may cross an untrusted network.</li> <li>Plan for appropriately sized hardware</li> </ul>"},{"location":"gettingstarted/#implementation","title":"Implementation","text":""},{"location":"gettingstarted/#splunk-setup","title":"Splunk Setup","text":""},{"location":"gettingstarted/#runtime-configuration","title":"Runtime configuration","text":""},{"location":"gettingstarted/ansible-docker-podman/","title":"Podman/Docker","text":"<p>SC4S installation can be automated with Ansible. To do this, you provide a list of hosts on which you want to run SC4S and basic configuration information, such as the Splunk endpoint, HEC token, and TLS configuration.</p>"},{"location":"gettingstarted/ansible-docker-podman/#step-1-prepare-your-initial-configuration","title":"Step 1: Prepare your initial configuration","text":"<ol> <li>Before running SC4S with Ansible, provide <code>env_file</code> with your Splunk endpoint and HEC token:</li> </ol> <p><pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://xxx.xxx.xxx.xxx:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxxxxxxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> 2. Provide a list of hosts on which you want to run your cluster and the host application in the inventory file: <pre><code>all:\n  hosts:\n  children:\n    node:\n      hosts:\n        node_1:\n          ansible_host:\n</code></pre></p>"},{"location":"gettingstarted/ansible-docker-podman/#step-2-deploy-sc4s-on-your-configuration","title":"Step 2: Deploy SC4S on your configuration","text":"<ol> <li>If you have Ansible installed on your host, run the Ansible playbook to deploy SC4S. Otherwise, use the Docker Ansible image provided in the package: <pre><code># From repository root\ndocker-compose -f ansible/docker-compose.yml build\ndocker-compose -f ansible/docker-compose.yml up -d\ndocker exec -it ansible_sc4s /bin/bash\n</code></pre></li> <li>If you used the Docker Ansible image in the previous step, then from your container remote shell, authenticate to and run the playbook.</li> </ol> <ul> <li>To authenticate with username and password: <pre><code>ansible-playbook -i path/to/inventory.yaml -u &lt;username&gt; --ask-pass path/to/playbooks/docker.yml\nor\nansible-playbook -i path/to/inventory.yaml -u &lt;username&gt; --ask-pass path/to/playbooks/podman.yml\n</code></pre></li> <li>To authenticate using a key pair: <pre><code>ansible-playbook -i path/to/inventory.yaml -u &lt;username&gt; --key-file &lt;key_file&gt; path/to/playbooks/docker.yml\nor\nansible-playbook -i path/to/inventory.yaml -u &lt;username&gt; --key-file &lt;key_file&gt; path/to/playbooks/podman.yml\n</code></pre></li> </ul>"},{"location":"gettingstarted/ansible-docker-podman/#step-3-validate-your-configuration","title":"Step 3: Validate your configuration","text":"<p>SC4S performs checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. Once the checks are complete, validate that SC4S properly communicate with Splunk. To do this, execute the following search in Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>This should yield an event similar to the following:</p> <p><pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> You can verify if all SC4S instances work by checking the <code>sc4s_container</code> in Splunk. Each instance should have a different container ID. All other fields should be the same.</p> <p>The startup process should proceed normally without syntax errors. If it does not, follow the steps below before proceeding to deeper-level troubleshooting:</p> <ol> <li>Verify that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Verify that your indexes are created in Splunk, and that your token has access to them.</li> <li>If you are using a load balancer, verify that it is operating properly.</li> <li>Execute the following command to check the SC4S startup process running in the container. <pre><code>sudo docker ps\n</code></pre></li> </ol> <ul> <li>You will get an ID and image name, next: </li> </ul> <p><pre><code>docker logs &lt;ID | image name&gt; \n</code></pre> or: <pre><code>sudo systemctl status sc4s\n</code></pre></p> <ul> <li>In the output, you should see events similar to this example:</li> </ul> <pre><code>SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:fallback...\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:events...\nsyslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre> <p>If you do not see this output, see \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d for more information.</p>"},{"location":"gettingstarted/ansible-docker-swarm/","title":"Docker Swarm","text":"<p>SC4S installation can be automated with Ansible. To do this, you provide a list of hosts on which you want to run SC4S and the basic configuration, such as Splunk endpoint, HEC token, and TLS configuration. To perform this task, you must have existing understanding of Docker Swarm and be able to set up your Swarm architecture and configuration.</p>"},{"location":"gettingstarted/ansible-docker-swarm/#step-1-prepare-your-initial-configuration","title":"Step 1: Prepare your initial configuration","text":"<ol> <li>Before running SC4S with Ansible, provide <code>env_file</code> with your Splunk endpoint and HEC token:</li> </ol> <p><pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://xxx.xxx.xxx.xxx:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxxxxxxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> 2. Provide a list of hosts on which you want to run your Docker Swarm cluster and the host application in the inventory file: <pre><code>all:\n  hosts:\n  children:\n    manager:\n      hosts:\n        manager_node_1:\n          ansible_host:\n\n    worker:\n      hosts:\n        worker_node_1:\n          ansible_host:\n        worker_node_2:\n          ansible_host:\n</code></pre> 3. You can run your cluster with one or more manager nodes. One advantage of hosting SC4S with Docker Swarm is that you can leverage the Swarm internal load balancer. See your Swarm Mode documentation at Docker. </p> <ol> <li>You can also provide extra service configurations, for example, the number of replicas, in the <code>/ansible/app/docker-compose.yml</code> file: <pre><code>version: \"3.7\"\nservices:\n  sc4s:\n    deploy:\n      replicas: 2\n      ...\n</code></pre></li> </ol>"},{"location":"gettingstarted/ansible-docker-swarm/#step-2-deploy-sc4s-on-your-configuration","title":"Step 2: Deploy SC4S on your configuration","text":"<ol> <li>If you have Ansible installed on your host, run the Ansible playbook to deploy SC4S. Otherwise, use the Docker Ansible image provided in the package: <pre><code># From repository root\ndocker-compose -f ansible/docker-compose.yml build\ndocker-compose -f ansible/docker-compose.yml up -d\ndocker exec -it ansible_sc4s /bin/bash\n</code></pre></li> <li>If you used the Docker Ansible image in Step 1, then from your container remote shell, run the Docker Swam Ansible playbook.</li> </ol> <ul> <li>You can authenticate with username and password: <pre><code>ansible-playbook -i path/to/inventory_swarm.yaml -u &lt;username&gt; --ask-pass path/to/playbooks/docker_swarm.yml\n</code></pre></li> <li>Or authenticate using key pair: <pre><code>ansible-playbook -i path/to/inventory_swarm.yaml -u &lt;username&gt; --key-file &lt;key_file&gt; path/to/playbooks/docker_swarm.yml\n</code></pre></li> </ul> <ol> <li>If your deployment is successfull, you can check the state of the Swarm cluster and deployed stack from the manager node remote shell:</li> </ol> <ul> <li>To verify that the stack is created: <code>sudo docker stack ls</code></li> </ul> NAME SERVICES ORCHESTRATOR sc4s 1 Swarm <ul> <li> <p>To scale your number of services: <code>sudo docker service update --replicas 2 sc4s_sc4s</code></p> </li> <li> <p>To see services running in a given stack:  <code>sudo docker stack services sc4s</code></p> </li> </ul> ID NAME MODE REPLICAS IMAGE PORTS 1xv9vvbizf3m sc4s_sc4s replicated 2/2 ghcr.io/splunk/splunk-connect-for-syslog/container3:latest :514-&gt;514/tcp, :601-&gt;601/tcp, :6514-&gt;6514/tcp, :514-&gt;514/udp"},{"location":"gettingstarted/ansible-docker-swarm/#step-3-validate-your-configuration","title":"Step 3: validate your configuration","text":"<p>SC4S performs checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. Once the checks are complete, validate that SC4S properly communicate with Splunk. To do this, execute the following search in Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>You should see an event similar to the following:</p> <p><pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> You can verify if all services in the Swarm cluster work by checking the <code>sc4s_container</code> in Splunk. Each service should have a different container ID. All other fields should be the same.</p> <p>The startup process should proceed normally without syntax errors. If it does not, follow the steps below before proceeding to deeper-level troubleshooting:</p> <ol> <li>Verify that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Verify that your indexes are created in Splunk, and that your token has access to them.</li> <li>If you are using a load balancer, verify that it is operating properly.</li> <li>Execute the following command to check the SC4S startup process running in the container.</li> </ol> <pre><code>sudo docker|podman ps\n</code></pre> <ul> <li>You will get an ID and image name: </li> </ul> <pre><code>docker|podman logs &lt;ID | image name&gt; \n</code></pre> <ul> <li>In the output, you should see events similar to this example:</li> </ul> <pre><code>SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:fallback...\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:events...\nsyslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre> <ol> <li>If you do not see this output, see \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d for more information.</li> </ol>"},{"location":"gettingstarted/ansible-mk8s/","title":"mk8s","text":"<p>To automate SC4S installation with Ansible, you provide a list of hosts on which you want to run SC4S as well as basic configuration information, such as the Splunk endpoint, HEC token, and TLS configuration. To perform this task, you must have existing understanding of MicroK8s and be able to set up your Kubernetes cluster architecture and configuration.</p>"},{"location":"gettingstarted/ansible-mk8s/#step-1-prepare-your-initial-configuration","title":"Step 1: Prepare your initial configuration","text":"<ol> <li> <p>Before you run SC4S with Ansible, update <code>values.yaml</code> with your Splunk endpoint and HEC token.  You can find the example file here.</p> </li> <li> <p>In the inventory file, provide a list of hosts on which you want to run your cluster and the host application: <pre><code>all:\n  hosts:\n  children:\n    node:\n      hosts:\n        node_1:\n          ansible_host:\n</code></pre></p> </li> <li>Alternatively, you can spin up a high-availability cluster: <pre><code>all:\n  hosts:\n  children:\n    manager:\n      hosts:\n        manager:\n          ansible_host:\n\n    workers:\n      hosts:\n        worker1:\n          ansible_host:\n        worker2:\n          ansible_host:\n</code></pre></li> </ol>"},{"location":"gettingstarted/ansible-mk8s/#step-2-deploy-sc4s-on-your-configuration","title":"Step 2: Deploy SC4S on your configuration","text":"<ol> <li>If you have Ansible installed on your host, run the Ansible playbook to deploy SC4S. Otherwise, use the Docker Ansible image provided in the package: <pre><code># From repository root\ndocker-compose -f ansible/docker-compose.yml build\ndocker-compose -f ansible/docker-compose.yml up -d\ndocker exec -it ansible_sc4s /bin/bash\n</code></pre></li> <li>If you used the Docker Ansible image, then from your container remote shell, authenticate to and run the MicroK8s playbook.</li> </ol> <ul> <li> <p>To authenticate with username and password: <pre><code>ansible-playbook -i path/to/inventory_mk8s.yaml -u &lt;username&gt; --ask-pass path/to/playbooks/microk8s.yml\n</code></pre></p> </li> <li> <p>To authenitcate if you are running a high-availability cluster: <pre><code>ansible-playbook -i path/to/inventory_mk8s_ha.yaml -u &lt;username&gt; --ask-pass path/to/playbooks/microk8s_ha.yml\n</code></pre></p> </li> <li> <p>To authenticate using a key pair: <pre><code>ansible-playbook -i path/to/inventory_mk8s.yaml -u &lt;username&gt; --key-file &lt;key_file&gt; path/to/playbooks/microk8s.yml\n</code></pre></p> </li> </ul>"},{"location":"gettingstarted/ansible-mk8s/#step-3-validate-your-configuration","title":"Step 3: Validate your configuration","text":"<p>SC4S performs checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. Once the checks are complete, validate that SC4S properly communicates with Splunk. To do this, execute the following search in Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>This should yield an event similar to the following:</p> <pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> <p>You can verify whether all services in the cluster work by checking the <code>sc4s_container</code> in Splunk. Each service should have a different container ID. All other fields should be the same.</p> <p>The startup process should proceed normally without syntax errors. If it does not, follow the steps below before proceeding to deeper-level troubleshooting:</p> <ol> <li>Verify that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Verify that your indexes are created in Splunk, and that your token has access to them.</li> <li>If you are using a load balancer, verify that it is operating properly.</li> <li>Execute the following command to check the SC4S startup process running in the container.</li> </ol> <pre><code>sudo microk8s kubectl get pods\nsudo microk8s kubectl logs &lt;podname&gt;\n</code></pre> <p>You should see events similar to those below in the output:</p> <pre><code>SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:fallback...\nSC4S_ENV_CHECK_HEC: Splunk HEC connection test successful to index=main for sourcetype=sc4s:events...\nsyslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre>"},{"location":"gettingstarted/byoe-rhel8/","title":"SC4S \u201cBring Your Own Environment\u201d","text":"<ul> <li>FOREWORD:  The BYOE SC4S deliverable should be considered as a self/community supported option for SC4S deployment, and should be considered only by those with specific needs based on advanced understanding of syslog-ng architectures and linux/syslog-ng system administration and the ability to develop and automate testing in non-production environments. The container deliverable is the most often correct deliverable of SC4S for almost all enterprises. If you are simply trying to \u201cget syslog working\u201d, the turnkey, container approach described in the other runtime documents will be the fastest route to success.</li> </ul> <p>The \u201cBring Your Own Environment\u201d instructions that follow allow expert administrators to utilize the SC4S syslog-ng config files directly on the host OS running on a hardware server or virtual machine.  Administrators must provide an appropriate host OS (RHEL 8 used in this document) as well as an up-to-date syslog-ng installation either built from source (not documented here) or installed from community-built RPMs.  Modification of the base configuration will be required for most customer environments due to enterprise infrastructure variations. Once installed preparing an upgrade requires evaluation of the current environment compared to this reference then developing and testing a installation specific install plan. This activity is the responsibility of the administrator.</p> <ul> <li> <p>NOTE: Installing or modifying system configurations can have unexpected consequences, and advanced linux system administration and syslog-ng configuration experience is assumed when using the BYOE version of SC4S.</p> </li> <li> <p>NOTE:  Do not depend on the distribution-supplied version of syslog-ng, as it will likely be far too old. Read this explanation for the reason why syslog-ng builds are so dated in almost all RHEL/Debian distributions.</p> </li> </ul>"},{"location":"gettingstarted/byoe-rhel8/#byoe-installation-instructions","title":"BYOE Installation Instructions","text":"<p>These installation instructions assume a recent RHEL or CentOS-based release.  Minor adjustments may have to be made for Debian/Ubuntu.  In addition, almost all pre-compiled binaries for syslog-ng assume installation in <code>/etc/syslog-ng</code>; these instructions will reflect that.</p> <p>The following installation instructions are summarized from a  blog maintained by a developer at One Identity (formerly Balabit), who is the owner of the syslog-ng Open Source project. It is always advisable to review the blog for the latest changes to the repo(s), as changes here are quite dynamic.</p> <ul> <li> <p>Install CentOS or RHEL 8.0</p> </li> <li> <p>Enable EPEL (Centos 8)</p> </li> </ul> <pre><code>dnf install 'dnf-command(copr)' -y\ndnf install epel-release -y\ndnf copr enable czanik/syslog-ng336  -y\ndnf install syslog-ng syslog-ng-python syslog-ng-http python3-pip gcc python3-devel -y\n</code></pre> <ul> <li>Disable the distro-supplied syslog-ng unit file, as the syslog-ng process configured here will run as the <code>sc4s</code> service.  rsyslog will continue to be the system logger, but should be left enabled only if it is configured to not listen on the same ports as sc4s.  sc4s BYOE can be configured to provide local logging as well if desired.</li> </ul> <pre><code>sudo systemctl stop syslog-ng\nsudo systemctl disable syslog-ng\n</code></pre> <ul> <li> <p>Download the latest bare_metal.tar from releases on github and untar the package in <code>/etc/syslog-ng</code> using the command example below.</p> </li> <li> <p>NOTE:  The <code>wget</code> process below will unpack a tarball with the sc4s version of the syslog-ng config files in the standard <code>/etc/syslog-ng</code> location, and will overwrite existing content.  Ensure that any previous configurations of syslog-ng are saved if needed prior to executing the download step.</p> </li> <li> <p>NOTE:  At the time of writing, the latest major release is <code>v1.33</code>.  The latest release is typically listed first on the page above, unless there is an <code>-alpha</code>,<code>-beta</code>, or <code>-rc</code> release that is newer (which will be clearly indicated).  For production use, select the latest that does not have an <code>-rc</code>, <code>-alpha</code>, or <code>-beta</code> suffix. </p> </li> </ul> <pre><code>sudo wget -c https://github.com/splunk/splunk-connect-for-syslog/releases/download/&lt;latest release&gt;/baremetal.tar -O - | sudo tar -x -C /etc/syslog-ng\n</code></pre> <ul> <li>Install python requirements </li> </ul> <pre><code>sudo pip3 install -r /etc/syslog-ng/requirements.txt\n</code></pre> <ul> <li> <p>(Optional, for monitoring): Install <code>goss</code> and confirm that the version is v0.3.16 or newer.  <code>goss</code> installs in  <code>/usr/local/bin</code> by default, so ensure that 1) <code>entrypoint.sh</code> is modified to include <code>/usr/local/bin</code> in the full path, or 2) move the <code>goss</code> binary to <code>/bin</code> or <code>/usr/bin</code>. <pre><code>curl -L https://github.com/aelsabbahy/goss/releases/latest/download/goss-linux-amd64 -o /usr/local/bin/goss\nchmod +rx /usr/local/bin/goss\ncurl -L https://github.com/aelsabbahy/goss/releases/latest/download/dgoss -o /usr/local/bin/dgoss\n# Alternatively, using the latest\n# curl -L https://raw.githubusercontent.com/aelsabbahy/goss/latest/extras/dgoss/dgoss -o /usr/local/bin/dgoss\nchmod +rx /usr/local/bin/dgoss\n</code></pre></p> </li> <li> <p>There are two main options for running SC4S via systemd, the choice of which largely depends on administrator preference and orchestration methodology: 1) the <code>entrypoint.sh</code> script (identical to that used in the container) can be run directly via systemd, or 2) the script can be altered to preconfigure SC4S (after which only the syslog-ng are run via systemd). These are by no means the only ways to run BYOE \u2013 as the name implies, the method you choose will be based on your custom needs.</p> </li> <li> <p>To run the <code>entrypoint.sh</code> script directly in systemd, create the sc4s unit file <code>/lib/systemd/system/sc4s.service</code> and add the following content:</p> </li> </ul> <pre><code>[Unit]\nDescription=SC4S Syslog Daemon\nDocumentation=https://splunk-connect-for-syslog.readthedocs.io/en/latest/\nWants=network.target network-online.target\nAfter=network.target network-online.target\n\n[Service]\nType=simple\nExecStart=/etc/syslog-ng/entrypoint.sh\nExecReload=/bin/kill -HUP $MAINPID\nEnvironmentFile=/etc/syslog-ng/env_file\nStandardOutput=journal\nStandardError=journal\nRestart=on-abnormal\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ul> <li>To run <code>entrypoint.sh</code> as a \u201cpreconfigure\u201d script, modify the script by commenting out or removing the stanzas following the <code>OPTIONAL for BYOE</code> comments in the script.  This will prevent syslog-ng from being launched by the script. Then create the sc4s unit file <code>/lib/systemd/system/syslog-ng.service</code> and add the following content:</li> </ul> <pre><code>[Unit]\nDescription=System Logger Daemon\nDocumentation=man:syslog-ng(8)\nAfter=network.target\n\n[Service]\nType=notify\nExecStart=/usr/sbin/syslog-ng -F $SYSLOGNG_OPTS -p /var/run/syslogd.pid\nExecReload=/bin/kill -HUP $MAINPID\nEnvironmentFile=-/etc/default/syslog-ng\nEnvironmentFile=-/etc/sysconfig/syslog-ng\nStandardOutput=journal\nStandardError=journal\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ul> <li>Create the file <code>/etc/syslog-ng/env_file</code> and add the following environment variables (adjusting the URL/TOKEN appropriately):</li> </ul> <pre><code># The following \"path\" variables can differ from the container defaults specified in the entrypoint.sh script. \n# These are *optional* for most BYOE installations, which do not differ from the install location used.\n# in the container version of SC4S.  Failure to properly set these will cause startup failure.\n#SC4S_ETC=/etc/syslog-ng\n#SC4S_VAR=/etc/syslog-ng/var\n#SC4S_BIN=/bin\n#SC4S_SBIN=/usr/sbin\n#SC4S_TLS=/etc/syslog-ng/tls\n\n# General Options\nSC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://splunk.smg.aws:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=a778f63a-5dff-4e3c-a72c-a03183659e94\n\n# Uncomment the following line if using untrusted (self-signed) SSL certificates\n# SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> <ul> <li>Reload systemctl and restart syslog-ng (example here is shown for systemd option (1) above)</li> </ul> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable sc4s\nsudo systemctl start sc4s\n</code></pre>"},{"location":"gettingstarted/byoe-rhel8/#configure-sc4s-listening-ports","title":"Configure SC4S Listening Ports","text":"<p>Most enterprises use UDP/TCP port 514 as the default as their main listening port for syslog \u201csoup\u201d traffic, and TCP port 6514 for TLS. The standard SC4S configuration reflect these defaults.  These defaults can be changed by adding the following additional environment variables with appropriate values to the <code>env_file</code> above: <pre><code>SC4S_LISTEN_DEFAULT_TCP_PORT=514\nSC4S_LISTEN_DEFAULT_UDP_PORT=514\nSC4S_LISTEN_DEFAULT_RFC6587_PORT=601\nSC4S_LISTEN_DEFAULT_RFC5426_PORT=601\nSC4S_LISTEN_DEFAULT_RFC5425_PORT=5425\nSC4S_LISTEN_DEFAULT_TLS_PORT=6514\n</code></pre></p>"},{"location":"gettingstarted/byoe-rhel8/#dedicated-unique-listening-ports","title":"Dedicated (Unique) Listening Ports","text":"<p>For certain source technologies, categorization by message content is impossible due to the lack of a unique \u201cfingerprint\u201d in the data.  In other cases, a unique listening port is required for certain devices due to network requirements in the enterprise. For collection of such sources we provide a means of dedicating a unique listening port to a specific source.</p> <p>Refer to the \u201cSources\u201d documentation to identify the specific environment variables used to enable unique listening ports for the technology in use.</p>"},{"location":"gettingstarted/create-parser/","title":"Create a parser","text":"<p>The following is a step-by-step guide for adding new parsers. </p>"},{"location":"gettingstarted/create-parser/#why-create-a-parser","title":"Why create a parser?","text":"<p>Splunk Connect for Syslog can offload Splunk Indexers by performing operations that normally would have been done during index time, including linebreaking, source/sourcetype setting, and timestamping. Creating a parser also reduces the need of using corresponding add-ons on indexers.</p>"},{"location":"gettingstarted/create-parser/#before-you-start","title":"Before you start","text":"<ul> <li>Make sure you have read contribution standards.</li> <li>For more background information on how filters and parser work, and what suits you best, read about sources onboarding.</li> <li>Prepare your testing environment. With Python&gt;=3.9: <pre><code>pip3 install poetry\npoetry install\n</code></pre></li> <li>Prepare your testing command: <pre><code>poetry run pytest -v --tb=long \\\n--splunk_type=external \\\n--splunk_hec_token=&lt;HEC_TOKEN&gt; \\\n--splunk_host=&lt;HEC_ENDPOINT&gt; \\\n--sc4s_host=&lt;SC4S_IP&gt; \\\n--junitxml=test-results/test.xml \\\n-n &lt;NUMBER_OF_JOBS&gt; \\\n&lt;TEST&gt;\n</code></pre></li> <li>Create a new branch in the repository where you will apply your changes.</li> </ul>"},{"location":"gettingstarted/create-parser/#start-with-a-raw-log-message","title":"Start with a raw log message","text":"<p>If you already have a raw log message, you can skip this step. Otherwise, you need to extract one to have something to work with. You can do this in multiple ways; here is a brief description of two of them:</p>"},{"location":"gettingstarted/create-parser/#tcpdump","title":"tcpdump","text":"<p>You can use the <code>tcpdump</code> command to get incoming raw messages on a given port of your server.</p> <pre><code>tcpdump -n -s 0 -S -i any -v port 8088\n\ntcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n09:54:26.051644 IP (tos 0x0, ttl 64, id 29465, offset 0, flags [DF], proto UDP (17), length 466)\n10.202.22.239.41151 &gt; 10.202.33.242.syslog: SYSLOG, length: 438\nFacility local0 (16), Severity info (6)\nMsg: 2022-04-28T16:16:15.466731-04:00 NTNX-21SM6M510425-B-CVM audispd[32075]: node=ntnx-21sm6m510425-b-cvm type=SYSCALL msg=audit(1651176975.464:2828209): arch=c000003e syscall=2 success=yes exit=6 a0=7f2955ac932e a1=2 a2=3e8 a3=3 items=1 ppid=29680 pid=4684 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=964698 comm=\u201csshd\u201d exe=\u201c/usr/sbin/sshd\u201d subj=system_u:system_r:sshd_t:s0-s0:c0.c1023 key=\u201clogins\u201d\\0x0a\n</code></pre>"},{"location":"gettingstarted/create-parser/#wireshark","title":"Wireshark","text":"<p>Or you can read the logs using Wireshark from the .pcap file. From Wireshark go to Statistics-&gt;Conversations, then click on \u2018Follow Stream\u2019.</p> <p> Once you get your stream of messages, copy one of them. NOTE: In UDP there usually will not be any message separators.</p>"},{"location":"gettingstarted/create-parser/#save-raw-log-message-in-splunk-or-archive","title":"Save raw log message in Splunk or archive","text":"<p>See Obtaining \u201cOn-the-wire\u201d Raw Events.</p>"},{"location":"gettingstarted/create-parser/#create-a-unit-test","title":"Create a unit test","text":"<p>It is recommended to use the existing test case that is the most similar to your use case. The naming convention is <code>test_vendor_product.py</code> Afterwards, you need to make sure that your log is being parsed correctly by creating a test case.  Assuming you have a raw message like this: <pre><code>&lt;14&gt;1 2022-03-30T11:17:11.900862-04:00 host - - - - Carbon Black App Control event:  text=\"File 'c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\winpcap\\x86\\packet.dll' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP\\USER\" username=\"NT AUTHORITY\\SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"10.0.0.3\" process=\"c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\microsoft.tri.sensor.updater.exe\" file_path=\"c:\\program files\\azure advanced threat protection sensor\\2.175.15073.51407\\winpcap\\x86\\packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"8.5.4.3\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\"\n</code></pre> You need to: * make sure that the message is a valid python string, where escape characters are placed correctly. * anonymize the data. * rename functions. * update index, and sourcetype fields. * extract replace values with field names in test string.</p> <p>Here you can see proper test case for Vmware Carbonblack Protect device: <pre><code># Copyright 2019 Splunk, Inc.\n#\n# Use of this source code is governed by a BSD-2-clause-style\n# license that can be found in the LICENSE-BSD2 file or at\n# https://opensource.org/licenses/BSD-2-Clause\n\nimport shortuuid\nfrom jinja2 import Environment, select_autoescape\n\nfrom .sendmessage import sendsingle\nfrom .splunkutils import  splunk_single\nfrom .timeutils import time_operations\nimport datetime\n\nenv = Environment(autoescape=select_autoescape(default_for_string=False))\n# Below is a raw message\n# &lt;14&gt;1 2022-03-30T11:17:11.900862-04:00 host - - - - Carbon Black App Control event:  text=\"File 'c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\winpcap\\x86\\packet.dll' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP\\USER\" username=\"NT AUTHORITY\\SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"0.0.0.0\" process=\"c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\microsoft.tri.sensor.updater.exe\" file_path=\"c:\\program files\\azure advanced threat protection sensor\\0.0.0.0\\winpcap\\x86\\packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"0.0.0.0\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\"\n\n# Don't forget to rename the function\ndef test_vmware_carbonblack_protect(\n    record_property,  setup_splunk, setup_sc4s\n):\n    host = f\"{shortuuid.ShortUUID().random(length=5).lower()}-{shortuuid.ShortUUID().random(length=5).lower()}\"\n\n    dt = datetime.datetime.now()\n    iso, bsd, _, _, _, _, epoch = time_operations(dt)\n\n    # Tune time functions for Checkpoint\n    epoch = epoch[:-3]\n\n    mt = env.from_string(\n        # Extract mark, iso timestamp and host fields\n        # Make sure all needed characters are escaped\n        # If string contains single quotes wrap it in double qutes\n        '{{ mark }} {{ iso }} {{ host }} - - - - Carbon Black App Control event:  text=\"File \\'c:\\\\program files\\\\azure advanced threat protection sensor\\\\0.0.0.0\\\\winpcap\\\\x86\\\\packet.dll\\' [c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363] would have blocked if the rule was not in Report Only mode.\" type=\"Policy Enforcement\" subtype=\"Execution block (unapproved file)\" hostname=\"CORP\\\\USER\" username=\"NT AUTHORITY\\\\SYSTEM\" date=\"3/30/2022 3:16:40 PM\" ip_address=\"0.0.0.0\" process=\"c:\\\\program files\\\\azure advanced threat protection sensor\\\\0.0.0.0\\\\microsoft.tri.sensor.updater.exe\" file_path=\"c:\\\\program files\\\\azure advanced threat protection sensor\\\\0.0.0.0\\\\winpcap\\\\x86\\\\packet.dll\" file_name=\"packet.dll\" file_hash=\"c4e671bf409076a6bf0897e8a11e6f1366d4b21bf742c5e5e116059c9b571363\" policy=\"High Enforcement - Domain Controllers\" rule_name=\"Report read-only memory map operations on unapproved executables by .NET applications\" process_key=\"00000433-0000-23d8-01d8-44491b26f203\" server_version=\"0.0.0.0\" file_trust=\"-2\" file_threat=\"-2\" process_trust=\"-2\" process_threat=\"-2\" prevalence=\"50\"'\n    )\n    message = mt.render(mark=\"&lt;134&gt;1\", host=host, bsd=bsd, iso=iso, epoch=epoch)\n\n    sendsingle(message, setup_sc4s[0], setup_sc4s[1][514])\n\n    st = env.from_string(\n        # Make sure you changed index and sourcetype properly\n        'search _time={{ epoch }} index=epintel host=\"{{ host }}\" sourcetype=\"vmware:cb:protect\"'\n    )\n    search = st.render(epoch=epoch, bsd=bsd, host=host)\n\n    result_count, _ = splunk_single(setup_splunk, search)\n\n    record_property(\"host\", host)\n    record_property(\"resultCount\", result_count)\n    record_property(\"message\", message)\n\n    assert result_count == 1\n</code></pre> NOTE: It is a known issue that the test case will timeout when it starts. When it fails, just re-run it.</p> <p>Now run the test:</p> <p><code>poetry run pytest test/test_vendor_product.py</code></p> <p>This test will spin up a Splunk instance on your localhost and forward the parsed message there. Now the parsed log should appear in Splunk:  As you can see, at this moment, the message is being parsed as a generic *nix:syslog sourcetype. To assign it to the proper index and sourcetype you will need an actual parser. So far we have ensured that the fields in the messages are properly recognized.</p>"},{"location":"gettingstarted/create-parser/#create-a-parser_1","title":"Create a parser","text":"<p>Your parser needs to be declared in <code>package/etc/conf.d/conflib</code>. The naming convention is <code>app-type-vendor_product.conf</code>. If there is a similar parser existing already you can use it as a reference. In the parser, make sure you assign the proper sourcetype, index, vendor, product, and template. The template tells how your message should be parsed before sending it to Splunk. The most basic configuration will only <code>forward</code> raw log with correct metadata. Here is an example: <pre><code>block parser app-syslog-vmware_cb-protect() {\n    channel {\n        rewrite {\n            r_set_splunk_dest_default(\n                index(\"epintel\")\n                sourcetype('vmware:cb:protect')\n                vendor(\"vmware\")\n                product(\"cb-protect\")\n                template(\"t_msg_only\")\n            );\n        };\n    };\n};\napplication app-syslog-vmware_cb-protect[sc4s-syslog] {\n    filter {\n        message('Carbon Black App Control event:  '  type(string)  flags(prefix));\n    };  \n    parser { app-syslog-vmware_cb-protect(); };\n};\n</code></pre> Now all messages that start with the string <code>Carbon Black App Control event:</code> will be routed to the proper index and assigned the given sourcetype:  For more info about using message filtering go to sources documentation.</p> <p>If you wish to apply more transformations you will need to add the parser: <pre><code>block parser app-syslog-vmware_cb-protect() {\n    channel {\n        rewrite {\n            r_set_splunk_dest_default(\n                index(\"epintel\")\n                sourcetype('vmware:cb:protect')\n                vendor(\"vmware\")\n                product(\"cb-protect\")\n                template(\"t_kv_values\")\n            );\n        };\n\n        parser {\n            csv-parser(delimiters(chars('') strings(': '))\n                       columns('header', 'message')\n                       prefix('.tmp.')\n                       flags(greedy, drop-invalid));\n            kv-parser(\n                prefix(\".values.\")\n                pair-separator(\" \")\n                template('${.tmp.message}')\n            );\n        };\n    };\n};\napplication app-syslog-vmware_cb-protect[sc4s-syslog] {\n    filter {\n        message('Carbon Black App Control event:  '  type(string)  flags(prefix));\n    };  \n    parser { app-syslog-vmware_cb-protect(); };\n};\n</code></pre> In this case, we will extract all fields that are nested in the raw log message first by using <code>csv-parser</code> to split <code>Carbon Black App Control event</code> and the rest of message as a two separate fields named <code>header</code> and <code>message</code>. On top of that, we will use <code>kv-parser</code> to extract all key-value pairs  in the <code>message</code> field.</p> <p>The best way to test your parser is to run a previously created test case. If you need more debugging, use <code>docker ps</code> to see running containers, and <code>docker logs</code> to see what\u2019s happening to the parsed message.</p> <p>Once you are content with the results, you can commit your changes and open pull request. </p>"},{"location":"gettingstarted/docker-compose-MacOS/","title":"Install Docker Desktop for MacOS","text":"<p>Refer to the \u201cMacOS\u201d section in your Docker documentation to set up your Docker Desktop for MacOS. </p>"},{"location":"gettingstarted/docker-compose-MacOS/#perform-your-initial-sc4s-configuration","title":"Perform your initial SC4S configuration","text":"<p>You can run SC4S using either <code>docker-compose</code> or the <code>docker run</code> command in the command line. This topic focuses solely on using <code>docker-compose</code>.</p> <ol> <li> <p>Create a directory on the server for local configurations and disk buffering. Make it available to all administrators, for example: <code>/opt/sc4s/</code>. </p> </li> <li> <p>Create a <code>docker-compose.yml</code> file in your new directory, based on the provided template. By default, the latest container is automatically downloaded at each restart. As a best practice, consult this topic at the time of any new upgrade to check for any changes in the latest template. <pre><code>version: \"3.7\"\nservices:\n  sc4s:\n    deploy:\n      replicas: 2\n      restart_policy:\n        condition: on-failure\n    image: ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\n    ports:\n       - target: 514\n         published: 514\n         protocol: tcp\n       - target: 514\n         published: 514\n         protocol: udp\n       - target: 601\n         published: 601\n         protocol: tcp\n       - target: 6514\n         published: 6514\n         protocol: tcp\n    env_file:\n      - /opt/sc4s/env_file\n    volumes:\n      - /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\n      - splunk-sc4s-var:/var/lib/syslog-ng\n# Uncomment the following line if local disk archiving is desired\n#     - /opt/sc4s/archive:/var/lib/syslog-ng/archive:z\n# Map location of TLS custom TLS\n#     - /opt/sc4s/tls:/etc/syslog-ng/tls:z\n\nvolumes:\n  splunk-sc4s-var:\n</code></pre></p> </li> <li>In Docker Desktop, set the <code>/opt/sc4s</code> folder as shared.</li> <li> <p>Create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destinations. This volume also keeps track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. Be sure to account for disk space requirements for the Docker volume. This volume is located in <code>/var/lib/docker/volumes/</code> and could grow significantly if there is an extended outage to the SC4S destinations. See SC4S disk buffer configuration for more information. <pre><code>sudo docker volume create splunk-sc4s-var\n</code></pre></p> </li> <li> <p>Create the subdirectories: <code>/opt/sc4s/local</code>, <code>/opt/sc4s/archive</code>, and <code>/opt/sc4s/tls</code>. Make sure these directories match the volume mounts specified in<code>docker-compose.yml</code>.</p> </li> <li> <p>Create a file named <code>/opt/sc4s/env_file</code>.</p> </li> </ol> <p><pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> 6. Update the following environment variables and values in <code>/opt/sc4s/env_file</code>:</p> <ul> <li> <p>Update <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL</code> and <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN</code> to reflect the values for your environment. Do not configure HEC Acknowledgement when you deploy the HEC token on the Splunk side; syslog-ng http destination does not support this feature. </p> </li> <li> <p>The default number of <code>SC4S_DEST_SPLUNK_HEC_WORKERS</code> is 10. Consult the community if you feel the number of workers (threads) should deviate from this.</p> </li> <li> <p>Splunk Connect for Syslog defaults to secure configurations.  If you are not using trusted SSL certificates, be sure to uncomment the last line.</p> </li> </ul>"},{"location":"gettingstarted/docker-compose-MacOS/#create-unique-dedicated-listening-ports","title":"Create unique dedicated listening ports","text":"<p>Each listening port on the container must be mapped to a listening port on the host. Make sure to update the <code>docker-compose.yml</code> file when adding listening ports for new data sources.</p> <p>To configure unique ports:</p> <ol> <li>Modify the <code>/opt/sc4s/env_file</code> file to include the port-specific environment variables. See the Sources  documentation to identify the specific environment variables that are mapped to each data source vendor and technology.</li> <li>Modify the Docker Compose file that starts the SC4S container so that it reflects the additional listening ports you have created. You can amend the Docker Compose file with additional <code>target</code> stanzas in the <code>ports</code> section of the file (after the default ports). For example, the following additional <code>target</code> and <code>published</code> lines provide for 21 additional technology-specific UDP and TCP ports:</li> </ol> <pre><code>       - target: 5000-5020\n         published: 5000-5020\n         protocol: tcp\n       - target: 5000-5020\n         published: 5000-5020\n         protocol: udp\n</code></pre> <ol> <li>Restart SC4S using the command in the \u201cStart/Restart SC4S\u201d section in this topic.</li> </ol> <p>For more information about configuration refer to Docker and Podman basic configurations and detailed configuration.</p>"},{"location":"gettingstarted/docker-compose-MacOS/#startrestart-sc4s","title":"Start/Restart SC4S","text":"<p>From the catalog where you created compose file, execute:</p> <p><pre><code>docker-compose up\n</code></pre> Otherwise use <code>docker-compose</code> with <code>-f</code> flag pointing to the compose file <pre><code>docker-compose up -f /path/to/compose/file/docker-compose.yml\n</code></pre></p>"},{"location":"gettingstarted/docker-compose-MacOS/#stop-sc4s","title":"Stop SC4S","text":"<p>Execute:</p> <p><pre><code>docker-compose down \n</code></pre> or </p> <pre><code>docker-compose down -f /path/to/compose/file/docker-compose.yml\n</code></pre>"},{"location":"gettingstarted/docker-compose-MacOS/#verify-proper-operation","title":"Verify Proper Operation","text":"<p>SC4S performs automatic checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. Once these checks are complete, verify that SC4S is properly communicating with Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>When the startup process proceeds normally, you should see an event similar to the following:</p> <pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> <p>If you do not see this, try the following steps to troubleshoot:</p> <ol> <li>Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Check to see that the proper indexes are created in Splunk, and that the token has access to them.</li> <li>Ensure the proper operation of the load balancer if used.</li> <li>Check the SC4S startup process running:</li> </ol> <pre><code>docker logs &lt;container_name&gt;\n</code></pre> <p>You should see events similar to those below in the output:</p> <pre><code>syslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre> <p>If you do not see the output above, proceed to the \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.</p>"},{"location":"gettingstarted/docker-compose/","title":"Install Docker Desktop","text":"<p>Refer to your Docker documentation to set up your Docker Desktop. </p>"},{"location":"gettingstarted/docker-compose/#perform-your-initial-sc4s-configuration","title":"Perform your initial SC4S configuration","text":"<p>You can run SC4S with <code>docker-compose</code>, or in the command line using the command <code>docker run</code>.  Both options are described in this topic.</p> <ol> <li>Create a directory on the server for local configurations and disk buffering. Make it available to all administrators, for example: <code>/opt/sc4s/</code>. If you are using <code>docker-compose</code>, create a <code>docker-compose.yml</code> file in this directory using the template provided here. By default, the latest SC4S image is automatically downloaded at each restart. As a best practice, check back here regularly for any changes made to the latest template is incorporated into production before you relaunch with Docker Compose.</li> </ol> <pre><code>version: \"3.7\"\nservices:\n  sc4s:\n    deploy:\n      replicas: 2\n      restart_policy:\n        condition: on-failure\n    image: ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\n    ports:\n       - target: 514\n         published: 514\n         protocol: tcp\n       - target: 514\n         published: 514\n         protocol: udp\n       - target: 601\n         published: 601\n         protocol: tcp\n       - target: 6514\n         published: 6514\n         protocol: tcp\n    env_file:\n      - /opt/sc4s/env_file\n    volumes:\n      - /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\n      - splunk-sc4s-var:/var/lib/syslog-ng\n# Uncomment the following line if local disk archiving is desired\n#     - /opt/sc4s/archive:/var/lib/syslog-ng/archive:z\n# Map location of TLS custom TLS\n#     - /opt/sc4s/tls:/etc/syslog-ng/tls:z\n\nvolumes:\n  splunk-sc4s-var:\n</code></pre> <ol> <li>In Docker, set the <code>/opt/sc4s</code> folder as shared.</li> <li>Create a local volume that will contain the disk buffer files in the event of a communication failure to the upstream destinations. This volume also keeps track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. Be sure to account for disk space requirements for the Docker volume. This volume is located in <code>/var/lib/docker/volumes/</code> and could grow significantly if there is an extended outage to the SC4S destinations. See SC4S Disk Buffer Configuration in the Configuration topic for more information.</li> </ol> <pre><code>sudo docker volume create splunk-sc4s-var\n</code></pre> <ol> <li> <p>Create the subdirectories: <code>/opt/sc4s/local</code>, <code>/opt/sc4s/archive</code>, and <code>/opt/sc4s/tls</code>. If you are using the <code>docker-compose.yml</code> file, make sure these directories match the volume mounts specified in<code>docker-compose.yml</code>.</p> </li> <li> <p>Create a file named <code>/opt/sc4s/env_file</code>.</p> </li> </ol> <p><pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> 6. Update the following environment variables and values to <code>/opt/sc4s/env_file</code>:</p> <ul> <li>Update <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL</code> and <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN</code> to reflect the values for your environment. Do not configure HEC Acknowledgement when you deploy the HEC token on the Splunk side; syslog-ng http destination does not support this feature. </li> <li>The default number of <code>SC4S_DEST_SPLUNK_HEC_WORKERS</code> is 10. Consult the community if you feel the number of workers (threads) should deviate from this.</li> </ul> <p>NOTE:  Splunk Connect for Syslog defaults to secure configurations.  If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above.</p> <p>For more information about configuration, see Docker and Podman basic configurations and detailed configuration.</p>"},{"location":"gettingstarted/docker-compose/#start-or-restart-sc4s","title":"Start or restart SC4S","text":"<ul> <li>You can start SC4S directly if you are not using <code>docker-compose</code>.  Be sure to map the listening ports (<code>-p</code> arguments) according to your needs:</li> </ul> <pre><code>docker run -p 514:514 -p 514:514/udp -p 6514:6514 -p 5000-5020:5000-5020 -p 5000-5020:5000-5020/udp \\\n    --env-file=/opt/sc4s/env_file \\\n    --name SC4S \\\n    --rm ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\n</code></pre> <ul> <li>If you are using <code>docker compose</code>, from the catalog where you created compose file execute: <pre><code>docker compose up\n</code></pre></li> </ul> <p>Otherwise use <code>docker compose</code> with <code>-f</code> flag pointing to the compose file: <pre><code>docker compose up -f /path/to/compose/file/docker-compose.yml\n</code></pre></p>"},{"location":"gettingstarted/docker-compose/#stop-sc4s","title":"Stop SC4S","text":"<p>If the container is run directly from the CLI, stop the container using the <code>docker stop &lt;containerID&gt;</code> command.</p> <p>If using <code>docker compose</code>, execute:</p> <p><pre><code>docker compose down \n</code></pre> or </p> <pre><code>docker compose down -f /path/to/compose/file/docker-compose.yml\n</code></pre>"},{"location":"gettingstarted/docker-compose/#validate-your-configuration","title":"Validate your configuration","text":"<p>SC4S performs automatic checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. Once these checks are complete, verify that SC4S is properly communicating with Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>This should yield an event similar to the following when the startup process proceeds normally:</p> <pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> <p>If you do not see this, try the following steps to troubleshoot:</p> <ol> <li>Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Check to see that the proper indexes are created in Splunk, and that the token has access to them.</li> <li>Ensure the proper operation of the load balancer if used.</li> <li>Check the SC4S startup process running in the container.</li> </ol> <pre><code>docker logs SC4S\n</code></pre> <p>You should see events similar to those below in the output:</p> <pre><code>syslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre> <p>If you do not see the output above, see \u201cTroubleshoot SC4S server\u201d and \u201cTroubleshoot resources\u201d sections for more detailed information.</p>"},{"location":"gettingstarted/docker-podman-offline/","title":"Install a container while offline","text":"<p>You can stage SC4S by downloading the image so that it can be loaded on a host machine, for example on an airgapped system, without internet connectivity.</p> <ol> <li>Download the container image <code>oci_container.tgz</code> from our Github Page. The following example downloads v3.23.1, replace the URL with the latest release or pre-release version as desired:</li> </ol> <pre><code>sudo wget https://github.com/splunk/splunk-connect-for-syslog/releases/download/v3.23.1/oci_container.tar.gz\n</code></pre> <ol> <li>Distribute the container to the airgapped host machine using your preferred file transfer utility.</li> <li>Execute the following command, using Docker or Podman:</li> </ol> <pre><code>&lt;podman or docker&gt; load &lt; oci_container.tar.gz\n</code></pre> <ol> <li>Make a note of the container ID for the resulting load:</li> </ol> <pre><code>Loaded image: docker.pkg.github.com/splunk/splunk-connect-for-syslog/ci:90196f77f7525bc55b3b966b5fa1ce74861c0250\n</code></pre> <ol> <li> <p>Use the container ID to create a local label: <pre><code>&lt;podman or docker&gt; tag docker.pkg.github.com/splunk/splunk-connect-for-syslog/ci:90196f77f7525bc55b3b966b5fa1ce74861c0250 sc4slocal:latest\n</code></pre></p> </li> <li> <p>Use the local label <code>sc4slocal:latest</code> in the relevant unit or YAML file to launch SC4S by setting the <code>SC4S_IMAGE</code> environment variable in the unit file, or the relevant <code>image:</code> tag if you are using Docker Compose/Swarm. This label will cause the runtime to select the locally loaded image, and will not attempt to obtain the container image from the internet.</p> </li> </ol> <p><pre><code>Environment=\"SC4S_IMAGE=sc4slocal:latest\"\n</code></pre> 7. Remove the entry from the relevant unit file when your configuration uses systemd. This is because an external connection to pull the container is no longer needed or available:</p> <pre><code>ExecStartPre=/usr/bin/docker pull $SC4S_IMAGE\n</code></pre>"},{"location":"gettingstarted/docker-systemd-general/","title":"Install Docker CE","text":""},{"location":"gettingstarted/docker-systemd-general/#before-you-begin","title":"Before you begin","text":"<p>Before you start:</p> <ul> <li>Familiarize yourself with IPv4 forwarding</li> <li>Refer to installation guides for your docker configuration:<ul> <li>CentOS</li> <li>Ubuntu</li> <li>Debian</li> </ul> </li> </ul>"},{"location":"gettingstarted/docker-systemd-general/#initial-setup","title":"Initial Setup","text":"<p>This topic provides the most recent unit file. By default, the latest SC4S image is automatically downloaded at each restart. Consult this topic when you upgrade your SC4S installation and check for changes to the provided template unit file. Make sure these changes are incorporated into your configuration before you relaunch with systemd.</p> <ol> <li>Create the systemd unit file <code>/lib/systemd/system/sc4s.service</code> based on the provided template:</li> </ol> <pre><code>[Unit]\nDescription=SC4S Container\nWants=NetworkManager.service network-online.target docker.service\nAfter=NetworkManager.service network-online.target docker.service\nRequires=docker.service\n\n[Install]\nWantedBy=multi-user.target\n\n[Service]\nEnvironment=\"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\"\n\n# Required mount point for syslog-ng persist data (including disk buffer)\nEnvironment=\"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\"\n\n# Optional mount point for local overrides and configurations; see notes in docs\nEnvironment=\"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\"\n\n# Optional mount point for local disk archive (EWMM output) files\nEnvironment=\"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\"\n\n# Map location of TLS custom TLS\nEnvironment=\"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\"\n\nTimeoutStartSec=0\n\nExecStartPre=/usr/bin/docker pull $SC4S_IMAGE\n\n# Note: /usr/bin/bash will not be valid path for all OS\n# when startup fails on running bash check if the path is correct\nExecStartPre=/usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\"\n\nExecStart=/usr/bin/docker run \\\n        -e \"SC4S_CONTAINER_HOST=${SC4SHOST}\" \\\n        -v \"$SC4S_PERSIST_MOUNT\" \\\n        -v \"$SC4S_LOCAL_MOUNT\" \\\n        -v \"$SC4S_ARCHIVE_MOUNT\" \\\n        -v \"$SC4S_TLS_MOUNT\" \\\n        --env-file=/opt/sc4s/env_file \\\n        --network host \\\n        --name SC4S \\\n        --rm $SC4S_IMAGE\n\nRestart=on-abnormal\n</code></pre> <ol> <li>Execute the following command to create a local volume. This volume contains the disk buffer files in case of a communication failure to the upstream destinations:</li> </ol> <pre><code>sudo docker volume create splunk-sc4s-var\n</code></pre> <ol> <li> <p>Account for disk space requirements for the new Docker volume. The Docker volume can grow significantly if there is an extended outage to the SC4S destinations. This volume can be found at <code>/var/lib/docker/volumes/</code>. See SC4S Disk Buffer Configuration.</p> </li> <li> <p>Create the following subdirectories:</p> </li> </ol> <ul> <li><code>/opt/sc4s/local</code></li> <li><code>/opt/sc4s/archive</code></li> <li><code>/opt/sc4s/tls</code></li> </ul> <ol> <li>Create a file named <code>/opt/sc4s/env_file</code> and add the following environment variables and values:</li> </ol> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> <ol> <li> <p>Update <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL</code> and <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN</code> to reflect the correct values for your environment. Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side, the underlying syslog-ng HTTP destination does not support this feature. </p> </li> <li> <p>The default number of <code>SC4S_DEST_SPLUNK_HEC_WORKERS</code> is 10. Consult the community if you feel the number of workers should deviate from this.</p> </li> <li> <p>Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example in step 5.</p> </li> </ol> <p>For more information see Docker and Podman basic configurations and detailed configuration.</p>"},{"location":"gettingstarted/docker-systemd-general/#configure-sc4s-for-systemd","title":"Configure SC4S for systemd","text":"<p>To configure SC4S for systemd run the following commands:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable sc4s\nsudo systemctl start sc4s\n</code></pre>"},{"location":"gettingstarted/docker-systemd-general/#restart-sc4s","title":"Restart SC4S","text":"<p>To restart SC4S run the following command:</p> <pre><code>sudo systemctl restart sc4s\n</code></pre>"},{"location":"gettingstarted/docker-systemd-general/#implement-unit-file-changes","title":"Implement unit file changes","text":"<p>If you made changes to the configuration unit file, for example to configure with dedicated ports, you must stop SC4S and re-run the systemd configuration commands to implement your changes.</p> <pre><code>sudo systemctl stop sc4s\nsudo systemctl daemon-reload \nsudo systemctl enable sc4s\nsudo systemctl start sc4s\n</code></pre>"},{"location":"gettingstarted/docker-systemd-general/#validate-your-configuration","title":"Validate your configuration","text":"<p>SC4S performs checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct. Once the checks are complete, validate that SC4S properly communicate with Splunk. To do this, execute the following search in Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>You should see an event similar to the following:</p> <pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> <p>The startup process should proceed normally without syntax errors. If it does not, follow the steps below before proceeding to deeper-level troubleshooting:</p> <ol> <li>Verify that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Verify that your indexes are created in Splunk, and that your token has access to them.</li> <li>If you are using a load balancer, verify that it is operating properly.</li> <li>Execute the following command to check the SC4S startup process running in the container.</li> </ol> <pre><code>docker logs SC4S\n</code></pre> <p>You should see events similar to those below in the output:</p> <pre><code>syslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre> <ol> <li>If you do not see this output, see \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d for more information.</li> </ol>"},{"location":"gettingstarted/getting-started-runtime-configuration/","title":"Implement a Container Runtime and SC4S","text":""},{"location":"gettingstarted/getting-started-runtime-configuration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux host with Docker (CE 19.x or greater) or Podman enabled, depending on runtime choice (below).</li> <li>A network load balancer (NLB) configured for round-robin. Note: Special consideration may be required when more advanced products are used. The optimal configuration of the load balancer will round-robin each http POST request (not each connection).</li> <li>The host linux OS receive buffer size should be tuned to match the sc4s default to avoid dropping events (packets) at the network level. The default receive buffer for sc4s is set to 16 MB for UDP traffic, which should be OK for most environments.  To set the host OS kernel to match this, edit <code>/etc/sysctl.conf</code> using the following whole-byte values corresponding to 16 MB:</li> </ul> <p><pre><code>net.core.rmem_default = 17039360\nnet.core.rmem_max = 17039360\n</code></pre> and apply to the kernel: <pre><code>sysctl -p\n</code></pre> * Ensure the kernel is not dropping packets by periodically monitoring the buffer with the command <code>netstat -su | grep \"receive errors\"</code>. * NOTE: Failure to account for high-volume traffic (especially UDP) by tuning the kernel will result in message loss, which can be very unpredictable and difficult to detect. See this helpful discussion in the syslog-ng Professional Edition documentation regarding tuning syslog-ng in particular (via the SC4S_SOURCE_*_SO_RCVBUFF environment variable in sc4s) as well as overall host kernel tuning.  The default values for receive kernel buffers in most distros is 2 MB, which has proven inadequate for many.</p>"},{"location":"gettingstarted/getting-started-runtime-configuration/#ipv4-forwarding","title":"IPv4 Forwarding","text":"<p>In many distributions (e.g. CentOS provisioned in AWS), IPV4 forwarding is not enabled by default. This needs to be enabled for container networking to function properly.  The following is an example to check and  set this up; as usual this needs to be vetted with your enterprise security policy:</p> <p>To check: <code>sudo sysctl net.ipv4.ip_forward</code> To set: <code>sudo sysctl net.ipv4.ip_forward=1</code></p> <p>To ensure the change survives a reboot: </p> <ul> <li>sysctl settings are defined through files in <code>/usr/lib/sysctl.d/</code>, <code>/run/sysctl.d/</code>, and <code>/etc/sysctl.d/</code>. </li> <li>To override only specific settings, you can either add a file with a lexically later name in <code>/etc/sysctl.d/</code> and put following setting there: <pre><code>net.ipv4.ip_forward=1\n</code></pre></li> <li>or find this specific setting in one of existing configuration files (mentioned above) and set value to <code>1</code>.</li> </ul> <pre><code>net.ipv4.ip_forward=1\n</code></pre>"},{"location":"gettingstarted/getting-started-runtime-configuration/#select-a-container-runtime-and-sc4s-configuration","title":"Select a Container Runtime and SC4S Configuration","text":"Container Runtime and Orchestration Operating Systems MicroK8s Ubuntu with Microk8s Podman 1.7 &amp; 1.9 + systemd RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Docker CE 19 (and greater) + systemd RHEL or CentOS &gt;7.7 (best option), Debian or Ubuntu 18.04LTS Docker Desktop + Compose MacOS Docker Desktop + Compose RHEL or CentOS 8.1 &amp; 8.2 (best option) Bring your own Environment RHEL or CentOS 8.1 &amp; 8.2 (best option) Offline Container Installation RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Ansible+Docker Swarm RHEL 8.x or CentOS 8.x (best option), Debian or Ubuntu 18.04LTS Ansible+Podman RHEL 7.x/8.x or CentOS 7.x/8.x (best option), Debian or Ubuntu 20.10LTS(and higher) Ansible+Docker RHEL 7.x/8.x or CentOS 7.x/8.x (best option), Debian or Ubuntu 18.04LTS(and higher)"},{"location":"gettingstarted/getting-started-runtime-configuration/#docker-and-podman-basic-configurations","title":"Docker and Podman basic configurations","text":"<ul> <li>To run properly sc4s you need to create directories:<code>/opt/sc4s/local</code> <code>/opt/sc4s/archive</code> <code>/opt/sc4s/tls</code></li> <li> <p><code>/opt/sc4s/local</code> will be used as a mount point for local overrides and configurations. The empty <code>local</code> directory created above will populate with defaults and examples at the first invocation of SC4S for local configurations and context overrides. Do not change the directory structure of the files that are laid down; change (or add) only individual files if desired.  SC4S depends on the directory layout to read the local configurations properly.  See the notes below for which files will be preserved on restarts. In the <code>local/config/</code> directory there are four subdirectories that allow you to provide support for device types that are not provided out of the box in SC4S.  To get you started, there is an example log path template (<code>lp-example.conf.tmpl</code>) and a filter (<code>example.conf</code>) in the <code>log_paths</code> and <code>filters</code> subdirectories, respectively.  These should not be used directly, but copied as templates for your own log path development.  They will get overwritten at each SC4S start. In the <code>local/context</code> directory, if you change the \u201cnon-example\u201d version of a file (e.g. <code>splunk_metadata.csv</code>) the changes will be preserved on a restart.</p> </li> <li> <p><code>/opt/sc4s/archive</code> will be used as a mount point for local storage of syslog events (if the optional mount is uncommented above).  The events will be written in the syslog-ng EWMM format. See the \u201cconfiguration\u201d document for details on the directory structure the archive uses.</p> </li> <li> <p><code>/opt/sc4s/tls</code> will be used as a mount point for custom TLS certificates (if the optional mount is uncommented above).</p> </li> <li> <p>IMPORTANT:  When creating the directories above, ensure the directories created match the volume mounts specified in the sc4s.service unit file.  Failure to do this will cause SC4S to abort at startup.</p> </li> </ul>"},{"location":"gettingstarted/getting-started-runtime-configuration/#dedicated-unique-listening-ports","title":"Dedicated (Unique) Listening Ports","text":"<p>For certain source technologies, categorization by message content is impossible due to the lack of a unique \u201cfingerprint\u201d in the data.  In other cases, a unique listening port is required for certain devices due to network requirements in the enterprise. For collection of such sources, we provide a means of dedicating a unique listening port to a specific source.</p> <p>Follow this step to configure unique ports for one or more sources:</p> <ul> <li>Modify the <code>/opt/sc4s/env_file</code> file to include the port-specific environment variable(s). Refer to the \u201cSources\u201d documentation to identify the specific environment variables that are mapped to each data source vendor/technology.</li> </ul>"},{"location":"gettingstarted/getting-started-runtime-configuration/#modify-index-destinations-for-splunk","title":"Modify index destinations for Splunk","text":"<p>Log paths are preconfigured to utilize a convention of index destinations that are suitable for most customers.</p> <ul> <li>If changes need to be made to index destinations, navigate to the <code>/opt/sc4s/local/context</code> directory to start.</li> <li>Edit <code>splunk_metadata.csv</code> to review or change the index configuration as required for the data sources utilized in your environment. The key (1st column) in this file uses the syntax <code>vendor_product</code>.  Simply replace the index value (the 3rd column) in the desired row with the index appropriate for your Splunk installation. The \u201cSources\u201d document details the specific <code>vendor_product</code> keys (rows) in this table that pertain to the individual data source filters that are included with SC4S.</li> <li>Other Splunk metadata (e.g. source and sourcetype) can be overridden via this file as well.  This is an advanced topic, and further information is covered in the \u201cLog Path overrides\u201d section of the Configuration document.</li> </ul>"},{"location":"gettingstarted/getting-started-runtime-configuration/#configure-source-filtering-by-source-ip-or-host-name","title":"Configure source filtering by source IP or host name","text":"<p>Legacy sources and non-standard-compliant sources require configuration by source IP or hostname as included in the event. The following steps apply to support such sources. To identify sources that require this step, refer to the \u201csources\u201d section of this documentation. See documentation for your vendor/product to determine if specific configuration is required</p>"},{"location":"gettingstarted/getting-started-runtime-configuration/#configure-compliance-indexmetadata-overrides","title":"Configure compliance index/metadata overrides","text":"<p>In some cases, devices that have been properly sourcetyped need to be further categorized by compliance, geography, or other criterion. The two files <code>compliance_meta_by_source.conf</code> and <code>compliance_meta_by_source.csv</code> can be used for this purpose.  These operate similarly to the files above, where the <code>conf</code> file specifies a filter to uniquely identify the messages that should be overridden, and the <code>csv</code> file lists one or more metadata items that can be overridden based on the filter name.  This is an advanced topic, and further information is covered in the \u201cOverride index or metadata based on host, ip, or subnet\u201d section of the Configuration document.</p>"},{"location":"gettingstarted/getting-started-splunk-setup/","title":"Splunk setup","text":"<p>To ensure proper integration for SC4S and Splunk, perform the following tasks in your Splunk instance:</p> <ol> <li>Create your SC4S indexes in Splunk.</li> <li>Configure your HTTP event collector.</li> </ol>"},{"location":"gettingstarted/getting-started-splunk-setup/#step-1-create-indexes-within-splunk","title":"Step 1: Create indexes within Splunk","text":"<p>SC4S maps each sourcetype to the following indexes by default. You will also need to create these indexes in Splunk:</p> <ul> <li><code>email</code></li> <li><code>epav</code></li> <li><code>epintel</code></li> <li><code>fireeye</code></li> <li><code>gitops</code></li> <li><code>infraops</code></li> <li><code>netauth</code></li> <li><code>netdlp</code></li> <li><code>netdns</code></li> <li><code>netfw</code></li> <li><code>netids</code></li> <li><code>netlb</code></li> <li><code>netops</code></li> <li><code>netwaf</code></li> <li><code>netproxy</code></li> <li><code>netipam</code></li> <li><code>oswin</code></li> <li><code>oswinsec</code></li> <li><code>osnix</code></li> <li><code>print</code></li> <li><code>_metrics</code> (Optional opt-in for SC4S operational metrics; ensure this is created as a metrics index)</li> </ul> <p>If you use custom indexes in SC4S you must also create them in Splunk. See Create custom indexes for more information.</p>"},{"location":"gettingstarted/getting-started-splunk-setup/#step-2-configure-your-http-event-collector","title":"Step 2: Configure your HTTP event collector","text":"<p>See Use the HTTP event collector for HEC configuration instructions based on your Splunk type.</p> <p>Keep in mind the following best practices specific to HEC for SC4S:</p> <ul> <li>Make sure that the HEC token created for SC4S has permissions to write to <code>_metrics</code> and all event destination indexes.</li> <li>You can leave \u201cSelected Indexes\u201d blank on the token configuration page so that the token has access to all indexes, including the <code>lastChanceIndex</code>.  If you do populate this field, take extreme care to keep it up to date; an attempt to send data to an index that is not in this list results in a <code>400</code> error from the HEC endpoint. The <code>lastChanceIndex</code> will not be consulted if the index specified in the event is not configured on Splunk and the entire batch is then not sent to Splunk.</li> <li>SC4S traffic should be sent to HEC endpoints configured directly on the indexers rather than an intermediate tier of heavy forwarders.</li> <li>SC4S traffic must be sent to HEC endpoints that are configured directly on the indexers.  </li> </ul>"},{"location":"gettingstarted/getting-started-splunk-setup/#create-a-load-balancing-mechanism","title":"Create a load balancing mechanism","text":"<p>In some configurations, you should ensure output balancing from SC4S to Splunk indexers. To do this, you create a load balancing mechanism between SC4S and Splunk indexers. Note that this should not be confused with load balancing between sources and SC4S. </p> <p>When configuring your load balancing mechanism, keep in mind the following:</p> <ul> <li>Splunk Cloud provides an internal ELB on TCP 443.</li> <li>For Splunk Enterprise set up your Splunk HTTP Event Collector with the HEC endpoints behind a load balancer. </li> <li>An external load balancer simplifies long-term maintenance by eliminating the need to manually keep the list of HEC URLs specified in SC4S current. Set up a load balancer using virtual IP and configured for https round-robin without sticky session. </li> <li>If a load balancer is not available, you can configure a list of HEC endpoint URLs with native syslog-ng load balancing. For internal load balancing of syslog-ng you should:<ul> <li>Load balance ten or fewer indexers.</li> <li>Use HEC exclusively for syslog.</li> <li>Have SC4S extract timestamps from messages (default behavior) rather than use the time of receipt for the message.</li> </ul> </li> </ul>"},{"location":"gettingstarted/k8s-microk8s/","title":"Install and configure SC4S with Kubernetes","text":"<p>Splunk provides an implementation for SC4S deployment with MicroK8s using a single-server MicroK8s as the deployment model. Clustering has some tradeoffs and should be only considered on a deployment-specific basis.</p> <p>You can independently replicate the model deployment on different distributions of Kubernetes. Do not attempt this unless you have advanced understanding of Kubernetes and are willing and able to maintain this configuration regularly.</p> <p>SC4S with MicroK8s leverages features of MicroK8s:</p> <ul> <li>Uses MetalLB to preserve the source IP.</li> <li>Works with any of the following operating systems: Windows, CentOS, RHEL, Ubuntu, Debian.</li> </ul> <p>Splunk maintains container images, but it doesn\u2019t directly support or otherwise provide resolutions for issues within the runtime environment.</p>"},{"location":"gettingstarted/k8s-microk8s/#step-1-allocate-ip-addresses","title":"Step 1: Allocate IP addresses","text":"<p>This configuration requires as least two IP addresses: one for the host and one for the internal load balancer. We suggest allocating three IP addresses for the host and 5-10 IP addresses for later use.</p>"},{"location":"gettingstarted/k8s-microk8s/#step-2-install-microk8s","title":"Step 2: Install MicroK8s","text":"<p>To install MicroK8s: <pre><code>sudo snap install microk8s --classic --channel=1.24\nsudo usermod -a -G microk8s $USER\nsudo chown -f -R $USER ~/.kube\nsu - $USER\nmicrok8s status --wait-ready\n</code></pre></p>"},{"location":"gettingstarted/k8s-microk8s/#step-3-set-up-your-add-ons","title":"Step 3: Set up your add-ons","text":"<p>When you install <code>metallb</code> you will be prompted for one or more IPs to use as entry points. If you do not plan to enable clustering, then this IP may be the same IP as the host. If you do plan to enable clustering this IP should not be assigned to the host.</p> <p>A single IP in CIDR format is x.x.x.x/32. Use CIDR or range syntax.</p> <pre><code>microk8s enable dns \nmicrok8s enable community\nmicrok8s enable metallb \nmicrok8s enable rbac \nmicrok8s enable storage \nmicrok8s enable openebs \nmicrok8s enable helm3\nmicrok8s status --wait-ready\n</code></pre>"},{"location":"gettingstarted/k8s-microk8s/#step-4-add-an-sc4s-helm-repository","title":"Step 4: Add an SC4S Helm repository","text":"<p>To add an SC4S Helm repository:</p> <pre><code>microk8s helm3 repo add splunk-connect-for-syslog https://splunk.github.io/splunk-connect-for-syslog\nmicrok8s helm3 repo update\n</code></pre>"},{"location":"gettingstarted/k8s-microk8s/#step-5-create-a-valuesyaml-file","title":"Step 5: Create a <code>values.yaml</code> file","text":"<p>Create the configuration file <code>values.yaml</code>. You can provide HEC token as a Kubernetes secret or in plain text. </p>"},{"location":"gettingstarted/k8s-microk8s/#provide-the-hec-token-as-plain-text","title":"Provide the HEC token as plain text","text":"<ol> <li>Create <code>values.yaml</code> file:</li> </ol> <pre><code>#values.yaml\nsplunk:\n    hec_url: \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\"\n    hec_token: \"00000000-0000-0000-0000-000000000000\"\n    hec_verify_tls: \"yes\"\n</code></pre> <ol> <li>Install SC4S: <pre><code>microk8s helm3 install sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml\n</code></pre></li> </ol>"},{"location":"gettingstarted/k8s-microk8s/#provide-the-hec-token-as-secret","title":"Provide the HEC token as secret","text":"<ol> <li>Create <code>values.yaml</code> file:</li> </ol> <pre><code>#values.yaml\nsplunk:\n    hec_url: \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\"\n    hec_verify_tls: \"yes\"\n</code></pre> <ol> <li>Install SC4S: <pre><code>export HEC_TOKEN=\"00000000-0000-0000-0000-000000000000\"\nmicrok8s helm3 install sc4s --set splunk.hec_token=$HEC_TOKEN splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml\n</code></pre></li> </ol>"},{"location":"gettingstarted/k8s-microk8s/#update-or-upgrade-sc4s","title":"Update or upgrade SC4S","text":"<p>Whenever the image is upgraded or when changes are made to the <code>values.yaml</code> file and should be applied, run the command:</p> <pre><code>microk8s helm3 upgrade sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml\n</code></pre>"},{"location":"gettingstarted/k8s-microk8s/#install-and-configure-sc4s-for-high-availability-ha","title":"Install and configure SC4S for High Availability (HA)","text":"<p>Three identically-sized nodes are required for HA. See your Microk8s documentation for more information.</p> <ol> <li> <p>Update the configuration file: <pre><code>#values.yaml\nreplicaCount: 6 #2x node count\nsplunk:\n    hec_url: \"https://xxx.xxx.xxx.xxx:8088/services/collector/event\"\n    hec_token: \"00000000-0000-0000-0000-000000000000\"\n    hec_verify_tls: \"yes\"\n</code></pre></p> </li> <li> <p>Upgrade SC4S to apply the new configuration: <pre><code>microk8s helm3 upgrade sc4s splunk-connect-for-syslog/splunk-connect-for-syslog -f values.yaml\n</code></pre></p> </li> </ol>"},{"location":"gettingstarted/k8s-microk8s/#configure-your-sc4s-instances-through-valuesyaml","title":"Configure your SC4S instances through <code>values.yaml</code>","text":"<p>With helm-based deployment you cannot configure environment variables and  context files directly. Instead, use the <code>values.yaml</code> file to update your configuration, for example:</p> <pre><code>sc4s:\n  # Certificate as a k8s Secret with tls.key and tls.crt fields\n  # Ideally produced and managed by cert-manager.io\n  existingCert: example-com-tls\n  #\n  vendor_product:\n    - name: checkpoint\n      ports:\n        tcp: [9000] #Same as SC4S_LISTEN_CHECKPOINT_TCP_PORT=9000\n        udp: [9000]\n      options:\n        listen:\n          old_host_rules: \"yes\" #Same as SC4S_LISTEN_CHECKPOINT_OLD_HOST_RULES=yes\n\n    - name: infoblox\n      ports:\n        tcp: [9001, 9002]\n        tls: [9003]\n    - name: fortinet\n      ports:\n        ietf_udp:\n          - 9100\n          - 9101\n  context_files:\n    splunk_metadata.csv: |-\n      cisco_meraki,index,foo\n    host.csv: |-\n      192.168.1.1,foo\n      192.168.1.2,moon\n</code></pre> <p>Use the <code>config_files</code> and <code>context_files</code> variables to specify configuration and context files that are passed to SC4S.</p> <ul> <li><code>config_files</code>: This variable contains a dictionary that maps the name of the configuration file to its content in the form of a YAML block scalar.</li> <li><code>context_file</code>: This variable contains a dictionary that maps the name of the context files to its content in the form of a YAML block scalar. The context files <code>splunk_metadata.csv</code> and <code>host.csv</code> are passed with <code>values.yaml</code>: <pre><code>sc4s:\n  # Certificate as a k8s Secret with tls.key and tls.crt fields\n  # Ideally produced and managed by cert-manager.io\n  #\n  vendor_product:\n    - name: checkpoint\n      ports:\n        tcp: [9000] #Same as SC4S_LISTEN_CHECKPOINT_TCP_PORT=9000\n        udp: [9000]\n      options:\n        listen:\n          old_host_rules: \"yes\" #Same as SC4S_LISTEN_CHECKPOINT_OLD_HOST_RULES=yes\n\n    - name: fortinet\n      ports:\n        ietf_udp:\n          - 9100\n          - 9101\n  context_files:\n    splunk_metadata.csv: |+\n      cisco_meraki,index,foo\n      cisco_asa,index,bar\n  config_files:\n    app-workaround-cisco_asa.conf: |+\n      block parser app-postfilter-cisco_asa_metadata() {\n        channel {\n          rewrite {\n            unset(value('fields.sc4s_recv_time'));\n          };\n        };\n       };\n      application app-postfilter-cisco_asa_metadata[sc4s-postfilter] {\n        filter {\n          'cisco' eq \"${fields.sc4s_vendor}\"\n          and 'asa' eq \"${fields.sc4s_product}\"\n        };\n        parser { app-postfilter-cisco_asa_metadata(); };\n       };\n</code></pre></li> </ul>"},{"location":"gettingstarted/k8s-microk8s/#manage-resources","title":"Manage resources","text":"<p>You should expect your system to require two instances per node by default. Adjust requests and limits to allow each instance to use about 40% of each node, presuming no other workload is present. </p> <pre><code>resources:\n  limits:\n    cpu: 100m\n    memory: 128Mi\n  requests:\n    cpu: 100m\n    memory: 128Mi\n</code></pre>"},{"location":"gettingstarted/podman-systemd-general/","title":"Install podman","text":"<p>See Podman product installation docs for information about working with your Podman installation.</p> <p>Before performing the tasks described in this topic, make sure you are familiar with using IPv4 forwarding with SC4S. See IPv4 forwarding .</p>"},{"location":"gettingstarted/podman-systemd-general/#initial-setup","title":"Initial Setup","text":"<p>NOTE: Make sure to use the latest unit file, which is provided here, with the current release. By default, the latest container is automatically downloaded at each restart. As a best practice, check back here regularly for any changes made to the latest template unit file is incorporated into production before you relaunch with systemd.</p> <ol> <li>Create the systemd unit file <code>/lib/systemd/system/sc4s.service</code> based on the following template:</li> </ol> <pre><code>[Unit]\nDescription=SC4S Container\nWants=NetworkManager.service network-online.target\nAfter=NetworkManager.service network-online.target\n\n[Install]\nWantedBy=multi-user.target\n\n[Service]\nEnvironment=\"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\"\n\n# Required mount point for syslog-ng persist data (including disk buffer)\nEnvironment=\"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\"\n\n# Optional mount point for local overrides and configurations; see notes in docs\nEnvironment=\"SC4S_LOCAL_MOUNT=/opt/sc4s/local:/etc/syslog-ng/conf.d/local:z\"\n\n# Optional mount point for local disk archive (EWMM output) files\nEnvironment=\"SC4S_ARCHIVE_MOUNT=/opt/sc4s/archive:/var/lib/syslog-ng/archive:z\"\n\n# Map location of TLS custom TLS\nEnvironment=\"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\"\n\nTimeoutStartSec=0\n\nExecStartPre=/usr/bin/podman pull $SC4S_IMAGE\n\n# Note: /usr/bin/bash will not be valid path for all OS\n# when startup fails on running bash check if the path is correct\nExecStartPre=/usr/bin/bash -c \"/usr/bin/systemctl set-environment SC4SHOST=$(hostname -s)\"\n\nExecStart=/usr/bin/podman run \\\n        -e \"SC4S_CONTAINER_HOST=${SC4SHOST}\" \\\n        -v \"$SC4S_PERSIST_MOUNT\" \\\n        -v \"$SC4S_LOCAL_MOUNT\" \\\n        -v \"$SC4S_ARCHIVE_MOUNT\" \\\n        -v \"$SC4S_TLS_MOUNT\" \\\n        --env-file=/opt/sc4s/env_file \\\n        --health-cmd=\"/healthcheck.sh\" \\\n        --health-interval=10s --health-retries=6 --health-timeout=6s \\\n        --network host \\\n        --name SC4S \\\n        --rm $SC4S_IMAGE\n\nRestart=on-abnormal\n</code></pre> <ol> <li>Execute the following command to create a local volume, which contains the disk buffer files in the event of a communication failure, to the upstream destinations.  This volume will also be used to keep track of the state of syslog-ng between restarts, and in particular the state of the disk buffer. </li> </ol> <pre><code>sudo podman volume create splunk-sc4s-var\n</code></pre> <p>NOTE:  Be sure to account for disk space requirements for the podman volume you create. This volume will be located in <code>/var/lib/containers/storage/volumes/</code> and could grow significantly if there is an extended outage to the SC4S destinations (typically HEC endpoints). See the \u201cSC4S Disk Buffer Configuration\u201d section on the Configuration page for more info.</p> <ol> <li>Create the subdirectories:    * <code>/opt/sc4s/local</code>    * <code>/opt/sc4s/archive</code>    * <code>/opt/sc4s/tls</code> </li> <li>Create a file named <code>/opt/sc4s/env_file</code> and add the following environment variables and values:</li> </ol> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> <ol> <li>Update <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL</code> and <code>SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN</code> to reflect the correct values for your environment.  Do not configure HEC Acknowledgement when deploying the HEC token on the Splunk side; the underlying syslog-ng http destination does not support this feature. The default value for <code>SC4S_DEST_SPLUNK_HEC_WORKERS</code> is 10. Consult the community if you feel the number of workers (threads) should deviate from this.</li> </ol> <p>NOTE:  Splunk Connect for Syslog defaults to secure configurations. If you are not using trusted SSL certificates, be sure to uncomment the last line in the example above.</p> <p>For more information about configuration refer to Docker and Podman basic configurations and detailed configuration.</p>"},{"location":"gettingstarted/podman-systemd-general/#configure-sc4s-for-systemd-and-start-sc4s","title":"Configure SC4S for systemd and start SC4S","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl enable sc4s\nsudo systemctl start sc4s\n</code></pre>"},{"location":"gettingstarted/podman-systemd-general/#restart-sc4s","title":"Restart SC4S","text":"<pre><code>sudo systemctl restart sc4s\n</code></pre> <p>If you have made changes to the configuration unit file, for example, in order to configure dedicated ports, you must first stop SC4S and re-run the systemd configuration commands:</p> <pre><code>sudo systemctl stop sc4s\nsudo systemctl daemon-reload \nsudo systemctl enable sc4s\nsudo systemctl start sc4s\n</code></pre>"},{"location":"gettingstarted/podman-systemd-general/#stop-sc4s","title":"Stop SC4S","text":"<pre><code>sudo systemctl stop sc4s\n</code></pre>"},{"location":"gettingstarted/podman-systemd-general/#verify-proper-operation","title":"Verify Proper Operation","text":"<p>SC4S has a number of \u201cpreflight\u201d checks to ensure that the container starts properly and that the syntax of the underlying syslog-ng configuration is correct.  After this step is complete, verify SC4S is properly communicating with Splunk by executing the following search in Splunk:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> <p>This should yield an event similar to the following when the startup process proceeds normally (without syntax errors). </p> <pre><code>syslog-ng starting up; version='3.28.1'\n</code></pre> <p>If you do not see this, try the following before proceeding to deeper-level troubleshooting:</p> <ul> <li>Check to see that the URL, token, and TLS/SSL settings are correct, and that the appropriate firewall ports are open (8088 or 443).</li> <li>Check to see that the proper indexes are created in Splunk, and that the token has access to them.</li> <li>Ensure the proper operation of the load balancer if used.</li> <li>Execute the following command to check the sc4s startup process running in the container.</li> </ul> <pre><code>podman logs SC4S\n</code></pre> <p>You should see events similar to those below in the output:</p> <pre><code>syslog-ng checking config\nsc4s version=v1.36.0\nstarting goss\nstarting syslog-ng\n</code></pre> <p>If the output does not display, see \u201cTroubleshoot sc4s server\u201d and \u201cTroubleshoot resources\u201d for more information.</p>"},{"location":"gettingstarted/podman-systemd-general/#sc4s-non-root-operation","title":"SC4S non-root operation","text":""},{"location":"gettingstarted/podman-systemd-general/#note","title":"NOTE:","text":"<p>Operating as a non-root user makes it impossible to use standard ports 514 and 601. Many devices cannot alter their destination port, so this operation may only be appropriate for cases where accepting syslog data from the public internet cannot be avoided.</p>"},{"location":"gettingstarted/podman-systemd-general/#prequisites","title":"Prequisites","text":"<p><code>Podman</code> and <code>slirp4netns</code> must be installed.</p>"},{"location":"gettingstarted/podman-systemd-general/#setup","title":"Setup","text":"<ol> <li> <p>Increase the number of user namespaces. Execute the following with sudo privileges: <pre><code>$ echo \"user.max_user_namespaces=28633\" &gt; /etc/sysctl.d/userns.conf      \n$ sysctl -p /etc/sysctl.d/userns.conf\n</code></pre></p> </li> <li> <p>Create a non-root user from which to run SC4S and to prepare Podman for non-root operations: <pre><code>sudo useradd -m -d /home/sc4s -s /bin/bash sc4s\nsudo passwd sc4s  # type password here\nsudo su - sc4s\nmkdir -p /home/sc4s/local\nmkdir -p /home/sc4s/archive\nmkdir -p /home/sc4s/tls\npodman system migrate\n</code></pre></p> </li> <li> <p>Load the new environment variables. To do this, temporarily switch to any other user, and then log back in as the SC4S user. When logging in as the SC4S user, don\u2019t use the \u2018su\u2019 command, as it won\u2019t load the new variables. Instead, you can use, for example, the command \u2018ssh sc4s@localhost\u2019.</p> </li> <li> <p>Create unit file in <code>~/.config/systemd/user/sc4s.service</code> with the following content: <pre><code>[Unit]\nUser=sc4s\nDescription=SC4S Container\nWants=NetworkManager.service network-online.target\nAfter=NetworkManager.service network-online.target\n[Install]\nWantedBy=multi-user.target\n[Service]\nEnvironment=\"SC4S_IMAGE=ghcr.io/splunk/splunk-connect-for-syslog/container3:latest\"\n# Required mount point for syslog-ng persist data (including disk buffer)\nEnvironment=\"SC4S_PERSIST_MOUNT=splunk-sc4s-var:/var/lib/syslog-ng\"\n# Optional mount point for local overrides and configuration\nEnvironment=\"SC4S_LOCAL_MOUNT=/home/sc4s/local:/etc/syslog-ng/conf.d/local:z\"\n# Optional mount point for local disk archive (EWMM output) files\nEnvironment=\"SC4S_ARCHIVE_MOUNT=/home/sc4s/archive:/var/lib/syslog-ng/archive:z\"\n# Map location of TLS custom TLS\nEnvironment=\"SC4S_TLS_MOUNT=/home/sc4s/tls:/etc/syslog-ng/tls:z\"\nTimeoutStartSec=0\nExecStartPre=/usr/bin/podman pull $SC4S_IMAGE\n# Note: The path /usr/bin/bash may vary based on your operating system.\n# when startup fails on running bash check if the path is correct\nExecStartPre=/usr/bin/bash -c \"/usr/bin/systemctl --user set-environment SC4SHOST=$(hostname -s)\"\nExecStart=/usr/bin/podman run -p 2514:514 -p 2514:514/udp -p 6514:6514  \\\n        -e \"SC4S_CONTAINER_HOST=${SC4SHOST}\" \\\n        -v \"$SC4S_PERSIST_MOUNT\" \\\n        -v \"$SC4S_LOCAL_MOUNT\" \\\n        -v \"$SC4S_ARCHIVE_MOUNT\" \\\n        -v \"$SC4S_TLS_MOUNT\" \\\n        --env-file=/home/sc4s/env_file \\\n        --health-cmd=\"/healthcheck.sh\" \\\n        --health-interval=10s --health-retries=6 --health-timeout=6s \\\n        --network host \\\n        --name SC4S \\\n        --rm $SC4S_IMAGE\nRestart=on-abnormal\n</code></pre></p> </li> <li> <p>Create your <code>env_file</code> file at <code>/home/sc4s/env_file</code> <pre><code>SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=http://xxx.xxx.xxx.xxx:8088\nSC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx\n#Uncomment the following line if using untrusted SSL certificates\n#SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\nSC4S_LISTEN_DEFAULT_TCP_PORT=8514\nSC4S_LISTEN_DEFAULT_UDP_PORT=8514\nSC4S_LISTEN_DEFAULT_RFC5426_PORT=8601\nSC4S_LISTEN_DEFAULT_RFC6587_PORT=8601\n</code></pre></p> </li> </ol>"},{"location":"gettingstarted/podman-systemd-general/#run-service","title":"Run service","text":"<p>To run the service as a non-root user, run the <code>systemctl</code> command with <code>--user</code> flag: <pre><code>systemctl --user daemon-reload\nsystemctl --user enable sc4s\nsystemctl --user start sc4s\n</code></pre></p> <p>The remainder of the setup can be found in the main setup instructions.</p>"},{"location":"gettingstarted/quickstart_guide/","title":"Quickstart Guide","text":""},{"location":"gettingstarted/quickstart_guide/#splunk-setup","title":"Splunk setup","text":"<ol> <li> <p>Create the following default indexes that are used by SC4S:</p> <ul> <li><code>email</code></li> <li><code>epav</code></li> <li><code>fireeye</code></li> <li><code>gitops</code></li> <li><code>netauth</code></li> <li><code>netdlp</code></li> <li><code>netdns</code></li> <li><code>netfw</code></li> <li><code>netids</code></li> <li><code>netops</code></li> <li><code>netwaf</code></li> <li><code>netproxy</code></li> <li><code>netipam</code></li> <li><code>oswinsec</code></li> <li><code>osnix</code></li> <li><code>_metrics</code> (Optional opt-in for SC4S operational metrics; ensure this is created as a metrics index)</li> </ul> </li> <li> <p>Create a HEC token for SC4S. When filling out the form for the token, leave the \u201cSelected Indexes\u201d pane blank and specify that a  <code>lastChanceIndex</code> be created so that all data received by SC4S will have a target destination in Splunk.</p> </li> </ol>"},{"location":"gettingstarted/quickstart_guide/#sc4s-setup-using-rhel-76","title":"SC4S setup (using RHEL 7.6)","text":"<ol> <li>Set the host OS kernel to match the default receiver buffer of SC4S, which is set to 16MB.</li> </ol> <p>a. Add the following to <code>/etc/sysctl.conf</code>:</p> <pre><code>```\nnet.core.rmem_default = 17039360\nnet.core.rmem_max = 17039360\n```\n</code></pre> <p>b. Apply to the kernel:</p> <pre><code>```\nsysctl -p\n```\n</code></pre> <ol> <li> <p>Ensure the kernel is not dropping packets:</p> <pre><code>netstat -su | grep \"receive errors\"\n</code></pre> </li> <li> <p>Create the systemd unit file <code>/lib/systemd/system/sc4s.service</code>.</p> </li> <li> <p>Copy and paste from the SC4S sample unit file (Docker) or SC4S sample unit file (Podman).</p> </li> <li> <p>Install Podman or Docker:</p> <p><pre><code>sudo yum -y install podman\n</code></pre> or <pre><code>sudo yum install docker-engine -y\n</code></pre></p> </li> <li> <p>Create a Podman/Docker local volume that will contain the disk buffer files and other SC4S state files (choose one in the command below):</p> <pre><code>sudo podman|docker volume create splunk-sc4s-var\n</code></pre> </li> <li> <p>Create directories to be used as a mount point for local overrides and configurations:</p> <p><code>mkdir /opt/sc4s/local</code></p> <p><code>mkdir /opt/sc4s/archive</code></p> <p><code>mkdir /opt/sc4s/tls</code></p> </li> <li> <p>Create the environment file <code>/opt/sc4s/env_file</code> and replace the HEC_URL and HEC_TOKEN as necessary:</p> <pre><code>  SC4S_DEST_SPLUNK_HEC_DEFAULT_URL=https://your.splunk.instance:8088\n  SC4S_DEST_SPLUNK_HEC_DEFAULT_TOKEN=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n  #Uncomment the following line if using untrusted SSL certificates\n  #SC4S_DEST_SPLUNK_HEC_DEFAULT_TLS_VERIFY=no\n</code></pre> </li> <li> <p>Configure SC4S for systemd and start SC4S:</p> <p><code>sudo systemctl daemon-reload</code></p> <p><code>sudo systemctl enable sc4s</code></p> <p><code>sudo systemctl start sc4s</code></p> </li> <li> <p>Check podman/docker logs for errors:</p> <pre><code>sudo podman|docker logs SC4S\n</code></pre> </li> <li> <p>Search on Splunk for successful installation of SC4S:</p> <pre><code>index=* sourcetype=sc4s:events \"starting up\"\n</code></pre> </li> <li> <p>Send sample data to default udp port 514 of SC4S host:</p> <pre><code>echo \u201cHello SC4S\u201d &gt; /dev/udp/&lt;SC4S_ip&gt;/514\n</code></pre> </li> </ol>"},{"location":"sources/","title":"Introduction","text":"<p>When using Splunk Connect for Syslog to onboard a data source, the syslog-ng \u201capp-parser\u201d performs the operations that are traditionally performed at index-time by the corresponding Technical Add-on installed there. These index-time operations include linebreaking, source/sourcetype setting and timestamping. For this reason, if a data source is exclusively onboarded using SC4S then you will not need to install its corresponding Add-On on the indexers. You must, however, install the Add-on on the search head(s) for the user communities interested in this data source.</p> <p>SC4S is designed to process \u201csyslog\u201d referring to IETF RFC standards 5424, legacy BSD syslog, RFC3164 (Not a standard document), and many \u201calmost\u201d syslog formats.</p> <p>When possible data sources are identified and processed based on characteristics of the event that make them unique as compared to other events for example. Cisco devices using IOS will include \u201d : %\u201d followed by a string. While Arista EOS devices will use a valid RFC3164 header with a value in the \u201cPROGRAM\u201d position with \u201c%\u201d as the first char in the \u201cMESSAGE\u201d portion. This allows two similar event structures to be processed correctly.</p> <p>When identification by message content alone is not possible for example the \u201csshd\u201d program field is commonly used across vendors additional \u201chint\u201d or guidance configuration allows SC4S to better classify events. The hints can be applied by definition of a specific port which will be used as a property of the event or by configuration of a host name/ip pattern. For example \u201cVMWARE VSPHERE\u201d products have a number of \u201cPROGRAM\u201d fields which can be used to identify vmware specific events in the syslog stream and these can be properly sourcetyped automatically however because \u201csshd\u201d is not unique it will be treated as generic \u201cos:nix\u201d events until further configuration is applied. The administrator can take one of two actions to refine the processing for vmware</p> <ul> <li>Define a specific port for vmware and reconfigure sources to use the defined port \u201cSC4S_LISTEN_VMWARE_VSPHERE_TCP=9000\u201d. Any events arriving on port 9000 will now have a metadata field attached \u201c.netsource.sc4s_vendor_product=VMWARE_VSPHERE\u201d</li> <li>Define a \u201capp-parser\u201d to apply the metadata field by using a syslog-ng filter to apply the metadata field.</li> </ul>"},{"location":"sources/#supporting-previously-unknown-sources","title":"Supporting previously unknown sources","text":"<p>Many log sources can be supported using one of the flexible options available without specific code known as app-parsers.</p> <p>New supported sources are added regularly. Please submit an issue with a description of the vend/product. Configuration information an a compressed pcap (.zip) from a non-production environment to request support for a new source.</p> <p>Many sources can be self supported. While we encourage sharing new sources via the github project to promote consistency and develop best-practices there is no requirement to engage in the community.</p> <ul> <li>Sources that are compliant with RFC 5424,RFC 5425, RFC 5426, or RFC 6587 can be onboarded as simple sources</li> <li>Sources \u201ccompatible\u201d with RFC3164 Note incorrect use of the syslog version, or \u201ccreative\u201d formats in the time stamp or other fields may prevent use as simple sources</li> <li>Common Event Format CEF Also known as ArcSight format</li> <li>Log Extended Format LEEF</li> </ul>"},{"location":"sources/#almost-syslog","title":"Almost Syslog","text":"<p>Sources sending legacy non conformant 3164 like streams can be assisted by the creation of an \u201cAlmost Syslog\u201d Parser. In an such a parser the goal is to process the syslog header allowing other parsers to correctly parse and handle the event. The following example is take from a currently supported format where the source product used epoch in the time stamp field.</p> <pre><code>    #Example event\n    #&lt;134&gt;1 1563249630.774247467 devicename security_event ids_alerted signature=1:28423:1 \n    # In the example note the vendor incorrectly included \"1\" following PRI defined in RFC5424 as indicating a compliant message\n    # The parser must remove the 1 before properly parsing\n    # The epoch time is captured by regex\n    # The epoch time is converted back into an RFC3306 date and provided to the parser\n    block parser syslog_epoch-parser() {    \n    channel {\n            filter { \n                message('^(\\&lt;\\d+\\&gt;)(?:1(?= ))? ?(\\d{10,13}(?:\\.\\d+)?) (.*)', flags(store-matches));\n            };  \n            parser {             \n                date-parser(\n                    format('%s.%f', '%s')\n                    template(\"$2\")\n                );\n            };\n            parser {\n                syslog-parser(\n\n                    flags(assume-utf8, expect-hostname, guess-timezone)\n                    template(\"$1 $S_ISODATE $3\")\n                    );\n            };\n            rewrite(set_rfc3164_epoch);                       \n\n    };\n    };\n    application syslog_epoch[sc4s-almost-syslog] {\n        parser { syslog_epoch-parser(); };   \n    };\n</code></pre>"},{"location":"sources/#standard-syslog-using-message-parsing","title":"Standard Syslog using message parsing","text":"<p>Syslog data conforming to RFC3164 or complying with RFC standards mentioned above can be processed with an app-parser allowing the use of the default port rather than requiring custom ports the following example take from a currently supported source uses the value of \u201cprogram\u201d to identify the source as this program value is unique. Care must be taken to write filter conditions strictly enough to not conflict with similar sources</p> <pre><code>block parser alcatel_switch-parser() {    \n channel {\n        rewrite {\n            r_set_splunk_dest_default(\n                index('netops')\n                sourcetype('alcatel:switch')\n                vendor('alcatel')\n                product('switch')\n                template('t_hdr_msg')\n            );              \n        };       \n\n\n   };\n};\napplication alcatel_switch[sc4s-syslog] {\n filter { \n        program('swlogd' type(string) flags(prefix));\n    }; \n    parser { alcatel_switch-parser(); };   \n};\n</code></pre>"},{"location":"sources/#standard-syslog-vendor-product-by-source","title":"Standard Syslog vendor product by source","text":"<p>In some cases standard syslog is also generic and can not be disambiguated from other sources by message content alone. When this happens and only a single source type is desired the \u201csimple\u201d option above is valid but requires managing a port. The following example allows use of a named port OR the vendor product by source configuration.</p> <pre><code>block parser dell_poweredge_cmc-parser() {    \n channel {\n\n        rewrite {\n            r_set_splunk_dest_default(\n                index('infraops')\n                sourcetype('dell:poweredge:cmc:syslog')\n                vendor('dell')\n                product('poweredge')\n                class('cmc')\n            );              \n        };       \n   };\n};\napplication dell_poweredge_cmc[sc4s-network-source] {\n filter { \n        (\"${.netsource.sc4s_vendor_product}\" eq \"dell_poweredge_cmc\"\n        or \"${SOURCE}\" eq \"s_DELL_POWEREDGE_CMC\")\n         and \"${fields.sc4s_vendor_product}\" eq \"\"\n    };    \n\n    parser { dell_poweredge_cmc-parser(); };   \n};\n</code></pre>"},{"location":"sources/#filtering-events-from-output","title":"Filtering events from output","text":"<p>In some cases specific events may be considered \u201cnoise\u201d and functionality must be implemented to prevent forwarding of these events to Splunk In version 2.0.0 of SC4S a new feature was implemented to improve the ease of use and efficiency of this progress.</p> <p>The following example will \u201cnull_queue\u201d or drop cisco IOS device events at the debug level. Note Cisco does not use the PRI to indicate DEBUG a message filter is required.</p> <pre><code>block parser cisco_ios_debug-postfilter() {\n    channel {\n        #In this case the outcome is drop the event other logic such as adding indexed fields or editing the message is possible\n        rewrite(r_set_dest_splunk_null_queue);\n   };\n};\napplication cisco_ios_debug-postfilter[sc4s-postfilter] {\n filter {\n        \"${fields.sc4s_vendor}\" eq \"cisco\" and\n        \"${fields.sc4s_product}\" eq \"ios\"\n        #Note regex reads as\n        # start from first position\n        # Any atleast 1 char that is not a `-`\n        # constant '-7-'\n        and message('^%[^\\-]+-7-');\n    };\n    parser { cisco_ios_debug-postfilter(); };\n};\n</code></pre>"},{"location":"sources/#another-example-to-drop-events-based-on-src-and-action-values-in-message","title":"Another example to drop events based on \u201csrc\u201d and \u201caction\u201d values in  message","text":"<pre><code>#filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-checkpoint_drop\n\nblock parser app-dest-rewrite-checkpoint_drop-d_fmt_hec_default() {    \n    channel {\n        rewrite(r_set_dest_splunk_null_queue);\n    };\n};\n\napplication app-dest-rewrite-checkpoint_drop-d_fmt_hec_default[sc4s-lp-dest-format-d_hec_fmt] {\n    filter {\n        match('checkpoint' value('fields.sc4s_vendor') type(string))\n        and match('syslog' value('fields.sc4s_product') type(string))\n\n        and match('Drop' value('.SDATA.sc4s@2620.action') type(string))\n        and match('12.' value('.SDATA.sc4s@2620.src') type(string) flags(prefix) );\n\n    };    \n    parser { app-dest-rewrite-checkpoint_drop-d_fmt_hec_default(); };   \n};\n</code></pre>"},{"location":"sources/#the-sc4s-fallback-sourcetype","title":"The SC4S \u201cfallback\u201d sourcetype","text":"<p>If SC4S receives an event on port 514 which has no soup filter, that event will be given a \u201cfallback\u201d sourcetype. If you see events in Splunk with the fallback sourcetype, then you should figure out what source the events are from and determine why these events are not being sourcetyped correctly. The most common reason for events categorized as \u201cfallback\u201d is the lack of a SC4S filter for that source, and in some cases a misconfigured relay which alters the integrity of the message format. In most cases this means a new SC4S filter must be developed. In this situation you can either build a filter or file an issue with the community to request help.</p> <p>The \u201cfallback\u201d sourcetype is formatted in JSON to allow the administrator to see the constituent syslog-ng \u201cmacros\u201d (fields) that have been automatically parsed by the syslog-ng server An RFC3164 (legacy BSD syslog) \u201con the wire\u201d raw message is usually (but unfortunately not always) comprised of the following syslog-ng macros, in this order and spacing:</p> <pre><code>&lt;$PRI&gt; $HOST $LEGACY_MSGHDR$MESSAGE\n</code></pre> <p>These fields can be very useful in building a new filter for that sourcetype.  In addition, the indexed field <code>sc4s_syslog_format</code> is helpful in determining if the incoming message is standard RFC3164. A value of anything other than <code>rfc3164</code> or <code>rfc5424_strict</code> indicates a vendor perturbation of standard syslog, which will warrant more careful examination when building a filter.</p>"},{"location":"sources/#splunk-connect-for-syslog-and-splunk-metadata","title":"Splunk Connect for Syslog and Splunk metadata","text":"<p>A key aspect of SC4S is to properly set Splunk metadata prior to the data arriving in Splunk (and before any TA processing takes place.  The filters will apply the proper index, source, sourcetype, host, and timestamp metadata automatically by individual data source.  Proper values for this metadata (including a recommended index) are included with all \u201cout-of-the-box\u201d log paths included with SC4S and are chosen to properly interface with the corresponding TA in Splunk.  The administrator will need to ensure all recommended indexes be created to accept this data if the defaults are not changed.</p> <p>It is understood that default values will need to be changed in many installations.  Each source documented in this section has a table entitled \u201cSourcetype and Index Configuration\u201d, which highlights the default index and sourcetype for each source.  See the section \u201cSC4S metadata configuration\u201d in the \u201cConfiguration\u201d page for more information on how to override the default values in this table.</p>"},{"location":"sources/#unique-listening-ports","title":"Unique listening ports","text":"<p>SC4S supports unique listening ports for each source technology/log path (e.g. Cisco ASA), which is useful when the device is sending data on a port different from the typical default syslog port (UDP port 514).  In some cases, when the source device emits data that is not able to be distinguished from other device types, a unique port is sometimes required.  The specific environment variables used for setting \u201cunique ports\u201d are outlined in each source document in this section.</p> <p>Using the default ports as unique listening ports is discouraged since it can lead to unintended consequences. There were cases of customers using port 514 as the unique listening port dedicated for a particular vendor and then sending other events to the same port, which caused some of those events to be misclassified.</p> <p>In most cases only one \u201cunique port\u201d is needed for each source.  However, SC4S also supports multiple network listening ports per source, which can be useful for a narrow set of compliance use cases. When configuring a source port variable to enable multiple ports, use a comma-separated list with no spaces (e.g. <code>SC4S_LISTEN_CISCO_ASA_UDP_PORT=5005,6005</code>).</p>"},{"location":"sources/#filtering-by-an-extra-product-description","title":"Filtering by an extra product description","text":"<p>Due to the fact that unique listening port feature differentiate vendor and product based on the first two underscore characters (\u2018_\u2019), it is possible  to filter events by an extra string added to the product. For example in case of having several devices of the same type sending logs over different ports it is possible to route it to different indexes based only on port value while retaining proper vendor and product fields. In general, it follows convention: <pre><code>SC4S_LISTEN_{VENDOR}_{PRODUCT}_{PROTOCOL}_PORT={PORT VALUE 1},{PORT VALUE 2}...\n</code></pre> But for special use cases it can be extended to: <pre><code>SC4S_LISTEN_{VENDOR}_{PRODUCT}_{ADDITIONAL_STRING}_{PROTOCOL}_PORT={PORT VALUE},{PORT VALUE 2}...\n</code></pre> This feature removes the need for complex pre/post filters.</p> <p>Example: <pre><code>SC4S_LISTEN_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01-001_UDP_PORT=18514\n\nsets:\nvendor = &lt; example vendor &gt;\nproduct = &lt; example product &gt;\ntag = .source.s_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01-001\n</code></pre> <pre><code>SC4S_LISTEN_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01-002_UDP_PORT=28514\n\nsets:\nvendor = &lt; example vendor &gt;\nproduct = &lt; example product &gt;\ntag = .source.s_EAMPLEVENDOR_EXAMPLEPRODUCT_GROUP01-002\n</code></pre></p>"},{"location":"sources/base/cef/","title":"Common Event Format (CEF)","text":""},{"location":"sources/base/cef/#product-various-products-that-send-cef-format-messages-via-syslog","title":"Product - Various products that send CEF-format messages via syslog","text":"<p>Each CEF product should have their own source entry in this documentation set.  In a departure from normal configuration, all CEF products should use the \u201cCEF\u201d version of the unique port and archive environment variable settings (rather than a unique one per product), as the CEF log path handles all products sending events to SC4S in the CEF format. Examples of this include Arcsight, Imperva, and Cyberark.  Therefore, the CEF environment variables for unique port, archive, etc. should be set only once.</p> <p>If your deployment has multiple CEF devices that send to more than one port, set the CEF unique port variable(s) as a comma-separated list.  See Unique Listening Ports for details.</p> <p>The source documentation included below is a reference baseline for any product that sends data using the CEF log path.</p> Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm"},{"location":"sources/base/cef/#splunk-metadata-with-cef-events","title":"Splunk Metadata with CEF events","text":"<p>The keys (first column) in <code>splunk_metadata.csv</code> for CEF data sources have a slightly different meaning than those for non-CEF ones. The typical <code>vendor_product</code> syntax is instead replaced by checks against specific columns of the CEF event \u2013 namely the first, second, and fourth columns following the leading <code>CEF:0</code> (\u201ccolumn 0\u201d). These specific columns refer to the CEF  <code>device_vendor</code>, <code>device_product</code>, and <code>device_event_class</code>, respectively.  The third column, <code>device_version</code>, is not used for metadata assignment.</p> <p>SC4S sets metadata based on the first two columns, and (optionally) the fourth.  While the key (first column) in the <code>splunk_metadata</code> file for non-CEF sources uses a \u201cvendor_product\u201d syntax that is arbitrary, the syntax for this key for CEF events is based on the actual contents of columns 1,2 and 4 from the CEF event, namely:</p> <p><code>device_vendor</code>_<code>device_product</code>_<code>device_class</code></p> <p>The final <code>device_class</code> portion is optional.  Therefore, CEF entries in <code>splunk_metadata</code> can have a key representing the vendor and product, and others representing a vendor and product coupled with one or more additional classes.  This allows for more granular metadata assignment (or overrides).</p> <p>Here is a snippet of a sample Imperva CEF event that includes a CEF device class entry (which is \u201cFirewall\u201d):</p> <pre><code>Apr 19 10:29:53 3.3.3.3 CEF:0|Imperva Inc.|SecureSphere|12.0.0|Firewall|SSL Untraceable Connection|Medium|\n</code></pre> <p>and the corresponding match in <code>splunk_metadata.csv</code>:</p> <pre><code>Imperva Inc._SecureSphere_Firewall,sourcetype,imperva:waf:firewall:cef\n</code></pre>"},{"location":"sources/base/cef/#default-sourcetype","title":"Default Sourcetype","text":"sourcetype notes cef Common sourcetype"},{"location":"sources/base/cef/#default-source","title":"Default Source","text":"source notes Varies Varies"},{"location":"sources/base/cef/#default-index-configuration","title":"Default Index Configuration","text":"key source index notes Vendor_Product Varies main none"},{"location":"sources/base/cef/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/base/cef/#options","title":"Options","text":"Variable default description SC4S_LISTEN_CEF_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_CEF_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_CEF_TLS_PORT empty string Enable a TLS  port for this specific vendor product using a comma-separated list of port numbers SC4S_DEST_CEF_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source"},{"location":"sources/base/leef/","title":"Log Extended Event Format (LEEF)","text":""},{"location":"sources/base/leef/#product-various-products-that-send-leef-v1-and-v2-format-messages-via-syslog","title":"Product - Various products that send LEEF V1 and V2 format messages via syslog","text":"<p>Each LEEF product should have their own source entry in this documentation set by vendor.  In a departure from normal configuration, all LEEF products should use the \u201cLEEF\u201d version of the unique port and archive environment variable settings (rather than a unique one per product), as the LEEF log path handles all products sending events to SC4S in the LEEF format. Examples of this include QRadar itself as well as other legacy systems.  Therefore, the LEEF environment variables for unique port, archive, etc. should be set only once.</p> <p>If your deployment has multiple LEEF devices that send to more than one port, set the LEEF unique port variable(s) as a comma-separated list.  See Unique Listening Ports for details.</p> <p>The source documentation included below is a reference baseline for any product that sends data using the LEEF log path.</p> <p>Some vendors implement LEEF v2.0 format events incorrectly, omitting the required \u201ckey=value\u201d separator field from the LEEF header, thus forcing the consumer to assume the default tab <code>\\t</code> character. SC4S will correctly process this omission, but will not correctly process other non-compliant formats.</p> <p>The LEEF format allows for the inclusion of a field <code>devTime</code> containing the device timestamp and allows the sender to also specify the format of this timestamp in another field called <code>devTimeFormat</code>, which uses the Java Time format. SC4S uses syslog-ng strptime format which is not directly translatable to the Java Time format. Therefore, SC4S has provided support for the following common formats.  If needed, additional time formats can be requested via an issue on github.</p> <pre><code>    '%s.%f',\n    '%s',\n    '%b %d %H:%M:%S.%f',\n    '%b %d %H:%M:%S',\n    '%b %d %Y %H:%M:%S.%f',\n    '%b %e %Y %H:%M:%S',\n    '%b %e %H:%M:%S.%f',\n    '%b %e %H:%M:%S',\n    '%b %e %Y %H:%M:%S.%f',\n    '%b %e %Y %H:%M:%S'  \n</code></pre> Ref Link Splunk Add-on LEEF None Product Manual https://www.ibm.com/support/knowledgecenter/SS42VS_DSM/com.ibm.dsm.doc/c_LEEF_Format_Guide_intro.html"},{"location":"sources/base/leef/#splunk-metadata-with-leef-events","title":"Splunk Metadata with LEEF events","text":"<p>The keys (first column) in <code>splunk_metadata.csv</code> for LEEF data sources have a slightly different meaning than those for non-LEEF ones. The typical <code>vendor_product</code> syntax is instead replaced by checks against specific columns of the LEEF event \u2013 namely the first and second, columns following the leading <code>LEEF:VERSION</code> (\u201ccolumn 0\u201d). These specific columns refer to the LEEF  <code>device_vendor</code>, and <code>device_product</code>, respectively.</p> <p><code>device_vendor</code>_<code>device_product</code></p> <p>Here is a snippet of a sample LANCOPE event in LEEF 2.0 format:</p> <pre><code>&lt;111&gt;Apr 19 10:29:53 3.3.3.3 LEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=192.0.2.0^dst=172.50.123.1^sev=5^cat=anomaly^srcPort=81^dstPort=21^usrName=joe.black\n</code></pre> <p>and the corresponding match in <code>splunk_metadata.csv</code>:</p> <pre><code>Lancope_StealthWatch,source,lancope:stealthwatch\n</code></pre>"},{"location":"sources/base/leef/#default-sourcetype","title":"Default Sourcetype","text":"sourcetype notes LEEF:1 Common sourcetype for all LEEF v1 events LEEF:2:<code>&lt;separator&gt;</code> Common sourcetype for all LEEF v2 events <code>separator</code> is the printable literal or hex value of the separator used in the event"},{"location":"sources/base/leef/#default-source","title":"Default Source","text":"source notes <code>vendor</code>:<code>product</code> Varies"},{"location":"sources/base/leef/#default-index-configuration","title":"Default Index Configuration","text":"key source index notes Vendor_Product Varies main none"},{"location":"sources/base/leef/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/base/leef/#options","title":"Options","text":"Variable default description SC4S_LISTEN_LEEF_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_LEEF_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_LEEF_TLS_PORT empty string Enable a TLS  port for this specific vendor product using a comma-separated list of port numbers SC4S_DEST_LEEF_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source"},{"location":"sources/base/nix/","title":"Generic *NIX","text":"<p>Many appliance vendor utilize Linux and BSD distributions as the foundation of the solution. When configured to log via syslog, these devices\u2019 OS logs (from a security perspective) can be monitored using the common Splunk Nix TA.</p> <p>Note: This is NOT a replacement for or alternative to the Splunk Universal forwarder on Linux and Unix. For general-purpose server applications, the Universal Forwarder offers more comprehensive collection of events and metrics appropriate for both security and operations use cases.</p> Ref Link Splunk Add-on https://splunkbase.splunk.com/app/833/"},{"location":"sources/base/nix/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes nix:syslog None"},{"location":"sources/base/nix/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes nix_syslog nix:syslog osnix none"},{"location":"sources/base/nix/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/base/nix/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer.</li> <li>Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source.</li> </ul>"},{"location":"sources/base/nix/#options","title":"Options","text":"Variable default description SC4S_DEST_NIX_SYSLOG_ARCHIVE no Enable archive to disk for this specific source SC4S_DEST_NIX_SYSLOG_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source"},{"location":"sources/base/simple/","title":"Simple Log path by port","text":"<p>The SIMPLE source configuration allows configuration of a log path for SC4S using a single port to a single index/sourcetype combination to quickly onboard new sources that have not been formally supported in the product. Source data must use RFC5424 or a common variant of RFC3164 formatting.</p> <ul> <li>NOTE:  This is an interim step that should be used only to quickly onboard well-formatted data that is being sent over a unique port.  A dedicated log path should be developed for the data source to facilitate further parsing and enrichment, as well as allowing the potential sending of this data source over the default (514) listening port.</li> </ul>"},{"location":"sources/base/simple/#splunk-metadata-with-simple-events","title":"Splunk Metadata with SIMPLE events","text":"<p>The keys (first column) in <code>splunk_metadata.csv</code> for SIMPLE data sources is a user-created key using the <code>vendor_product</code> convention. For example, to on-board a new product <code>first firewall</code> using a source type of <code>first:firewall</code> and index <code>netfw</code>, add the following two lines to the configuration file as shown:</p> <pre><code>first_firewall,index,netfw\nfirst_firewall,sourcetype,first:firewall\n</code></pre>"},{"location":"sources/base/simple/#options","title":"Options","text":"<p>For the variables below, replace <code>VENDOR_PRODUCT</code> with the key (converted to upper case) used in the <code>splunk_metadata.csv</code>. Based on the example above, to establish a tcp listener for <code>first firewall</code> we would use <code>SC4S_LISTEN_SIMPLE_FIRST_FIREWALL_TCP_PORT</code>.</p> Variable default description SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_SIMPLE_VENDOR_PRODUCT_TLS_PORT empty string Enable a TLS  port for this specific vendor product using a comma-separated list of port numbers SC4S_ARCHIVE_SIMPLE_VENDOR_PRODUCT no Enable archive to disk for this specific source SC4S_DEST_SIMPLE_VENDOR_PRODUCT_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source"},{"location":"sources/base/simple/#important-notes","title":"Important Notes","text":"<ul> <li><code>SIMPLE</code> data sources must use RFC5424 or a common variant of RFC3164 formatting.</li> <li>Each <code>SIMPLE</code> data source must listen on its own unique port list.  Port overlap with other sources, either <code>SIMPLE</code> ones or those served by regular log paths, are not allowed and will cause an error at startup.</li> <li>The key(s) chosen for <code>splunk_metadata.csv</code> must be in the form <code>vendor_product</code> (lower case).</li> <li>These same keys can be used for a regular SC4S log path developed in the future.</li> <li>The <code>SIMPLE</code> environment variables must have a core of <code>VENDOR_PRODUCT</code> (upper case).</li> <li>Take care to remove the <code>SIMPLE</code> form of these <code>LISTEN</code> variables after a regular SC4S log path is developed for a given source. You can, of course, continue to listen for this source on the same unique ports after having developed the new log path, but use the <code>SC4S_LISTEN_&lt;VENDOR_PRODUCT&gt;_&lt;protocol&gt;_PORT</code> form of the variable to ensure the newly developed log path will listen on the specified unique ports.</li> </ul>"},{"location":"sources/vendor/AVI/","title":"Common","text":""},{"location":"sources/vendor/AVI/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/AVI/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual https://avinetworks.com/docs/latest/syslog-formats/"},{"location":"sources/vendor/AVI/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes avi:events None"},{"location":"sources/vendor/AVI/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes avi_vantage avi:events netops none"},{"location":"sources/vendor/Alcatel/Switch/","title":"Switch","text":""},{"location":"sources/vendor/Alcatel/Switch/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Alcatel/Switch/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Alcatel/Switch/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes alcatel:switch None"},{"location":"sources/vendor/Alcatel/Switch/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes alcatel_switch alcatel:switch netops none"},{"location":"sources/vendor/Alsid/Alsid/","title":"Alsid","text":"<p>The product has been purchased and republished under a new product name by Tenable this configuration  is obsolete.</p>"},{"location":"sources/vendor/Alsid/Alsid/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Alsid/Alsid/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/5173/ Product Manual unknown"},{"location":"sources/vendor/Alsid/Alsid/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes alsid:syslog None"},{"location":"sources/vendor/Alsid/Alsid/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes alsid_syslog alsid:syslog oswinsec none"},{"location":"sources/vendor/Arista/","title":"EOS","text":""},{"location":"sources/vendor/Arista/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Arista/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Arista/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes arista:eos:* None"},{"location":"sources/vendor/Arista/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes arista_eos arista:eos netops none arista_eos_$PROCESSNAME arista:eosq netops The \u201cprocess\u201d field is used from the event"},{"location":"sources/vendor/Aruba/ap/","title":"Access Points","text":""},{"location":"sources/vendor/Aruba/ap/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter (Partial)</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Aruba/ap/#links","title":"Links","text":"Ref Link"},{"location":"sources/vendor/Aruba/ap/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes aruba:syslog Dynamically  Created"},{"location":"sources/vendor/Aruba/ap/#index-configuration","title":"Index Configuration","text":"key index notes aruba_ap netops none"},{"location":"sources/vendor/Aruba/ap/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-aruba_ap.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-aruba_ap[sc4s-vps] {\n filter { \n        host(\"aruba-ap-*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('aruba')\n            product('ap')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Aruba/clearpass/","title":"Clearpass","text":""},{"location":"sources/vendor/Aruba/clearpass/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Aruba/clearpass/#links","title":"Links","text":"Ref Link"},{"location":"sources/vendor/Aruba/clearpass/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes aruba:clearpass Dynamically  Created"},{"location":"sources/vendor/Aruba/clearpass/#index-configuration","title":"Index Configuration","text":"key index notes aruba_clearpass netops none aruba_clearpass_endpoint-profile netops none aruba_clearpass_alert netops none aruba_clearpass_endpoint-audit-record netops none aruba_clearpass_policy-server-session netops none aruba_clearpass_post-auth-monit-config netops none aruba_clearpass_snmp-session-log netops none aruba_clearpass_radius-session netops none aruba_clearpass_system-event netops none aruba_clearpass_tacacs-accounting-detail netops none aruba_clearpass_tacacs-accounting-record netops none"},{"location":"sources/vendor/Aruba/clearpass/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-aruba_clearpass.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-aruba_clearpass[sc4s-vps] {\n filter { \n        host(\"aruba-cp-*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('aruba')\n            product('clearpass')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Avaya/","title":"SIP Manager","text":""},{"location":"sources/vendor/Avaya/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514/UDP</li> <li>Vendor source is not conformant to RFC3194 by improperly sending unescaped <code>\\n</code> Use of TCP will cause dataloss</li> </ul>"},{"location":"sources/vendor/Avaya/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Avaya/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes avaya:avaya None"},{"location":"sources/vendor/Avaya/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes avaya_sipmgr avaya:avaya main none"},{"location":"sources/vendor/Aviatrix/aviatrix/","title":"Aviatrix","text":""},{"location":"sources/vendor/Aviatrix/aviatrix/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Aviatrix/aviatrix/#product-switches","title":"Product - Switches","text":"Ref Link Splunk Add-on \u2013 Product Manual Link"},{"location":"sources/vendor/Aviatrix/aviatrix/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes aviatrix:cloudx-cli None aviatrix:kernel None aviatrix:cloudxd None aviatrix:avx-nfq None aviatrix:avx-gw-state-sync None aviatrix:perfmon None"},{"location":"sources/vendor/Aviatrix/aviatrix/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes aviatrix_cloudx-cli aviatrix:cloudx-cli netops none aviatrix_kernel aviatrix:kernel netops none aviatrix_cloudxd aviatrix:cloudxd netops none aviatrix_avx-nfq aviatrix:avx-nfq netops none aviatrix_avx-gw-state-sync aviatrix:avx-gw-state-sync netops none aviatrix_perfmon aviatrix:perfmon netops none"},{"location":"sources/vendor/Barracuda/waf/","title":"WAF (Cloud)","text":""},{"location":"sources/vendor/Barracuda/waf/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC 5424 Framed</li> </ul>"},{"location":"sources/vendor/Barracuda/waf/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual https://campus.barracuda.com/product/WAAS/doc/79462622/log-export"},{"location":"sources/vendor/Barracuda/waf/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes barracuda:tr none"},{"location":"sources/vendor/Barracuda/waf/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes barracuda_waf barracuda:web:firewall netwaf None"},{"location":"sources/vendor/Barracuda/waf/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-barracuda_syslog.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-barracuda_syslog[sc4s-vps] {\n filter {      \n        netmask(169.254.100.1/24)\n        or host(\"barracuda\" type(string) flags(ignore-case))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('barracuda')\n            product('syslog')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Barracuda/waf_on_prem/","title":"Barracuda WAF (On Premises)","text":""},{"location":"sources/vendor/Barracuda/waf_on_prem/#key-facts","title":"Key facts","text":"<ul> <li>RFC 5424 Framed with non-standard ISO timestamp: <code>%Y-%m-%d %H:%M:%S.%f %z</code></li> <li>MSG Format based filter</li> </ul>"},{"location":"sources/vendor/Barracuda/waf_on_prem/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3776 Product Manual https://campus.barracuda.com/product/webapplicationfirewall/doc/92767349/exporting-log-formats/"},{"location":"sources/vendor/Barracuda/waf_on_prem/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes barracuda:system program(\u201cSYS\u201d) barracuda:waf program(\u201cWF\u201d) barracuda:web program(\u201cTR\u201d) barracuda:audit program(\u201cAUDIT\u201d) barracuda:firewall program(\u201cNF\u201d)"},{"location":"sources/vendor/Barracuda/waf_on_prem/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes barracuda_system barracuda:system netwaf None barracuda_waf barracuda:waf netwaf None barracuda_web barracuda:web netwaf None barracuda_audit barracuda:audit netwaf None barracuda_firewall barracuda:firewall netwaf None"},{"location":"sources/vendor/BeyondTrust/sra/","title":"Secure Remote Access (Bomgar)","text":""},{"location":"sources/vendor/BeyondTrust/sra/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/BeyondTrust/sra/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/BeyondTrust/sra/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes beyondtrust:sra None"},{"location":"sources/vendor/BeyondTrust/sra/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes beyondtrust_sra beyondtrust:sra infraops none"},{"location":"sources/vendor/BeyondTrust/sra/#options","title":"Options","text":"Variable default description SC4S_DEST_BEYONDTRUST_SRA_SPLUNK_HEC_FMT JSON Restructure data from vendor format to json for splunk destinations set to \u201cNONE\u201d for native format SC4S_DEST_BEYONDTRUST_SRA_SYSLOG_FMT SDATA Restructure data from vendor format to SDATA for SYSLOG destinations set to \u201cNONE\u201d for native ormat"},{"location":"sources/vendor/Broadcom/brightmail/","title":"Brightmail","text":""},{"location":"sources/vendor/Broadcom/brightmail/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Broadcom/brightmail/#links","title":"Links","text":"Ref Link Splunk Add-on TBD Product Manual https://support.symantec.com/us/en/article.howto38250.html"},{"location":"sources/vendor/Broadcom/brightmail/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes symantec:smg Requires version TA 3.6"},{"location":"sources/vendor/Broadcom/brightmail/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes symantec_brightmail symantec:smg email none"},{"location":"sources/vendor/Broadcom/brightmail/#options","title":"Options","text":"Variable default description SC4S_SOURCE_FF_SYMANTEC_BRIGHTMAIL_GROUPMSG yes Email processing events generated by the bmserver process will be grouped by host+program+pid+msg ID into a single event SC4S_DEST_SYMANTEC_BRIGHTMAIL_SPLUNK_HEC_FMT empty if \u201cJSON\u201d and GROUPMSG is enabled format the event in json SC4S_DEST_SYMANTEC_BRIGHTMAIL_SYSLOG_FMT empty if \u201cSDATA\u201d and GROUPMSG is enabled format the event in rfc5424 sdata"},{"location":"sources/vendor/Broadcom/dlp/","title":"Symantec DLP","text":""},{"location":"sources/vendor/Broadcom/dlp/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Broadcom/dlp/#links","title":"Links","text":"Ref Link Splunk Add-on Symatec DLP https://splunkbase.splunk.com/app/3029/ Source doc https://knowledge.broadcom.com/external/article/159509/generating-syslog-messages-from-data-los.html"},{"location":"sources/vendor/Broadcom/dlp/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes symantec:dlp:syslog None"},{"location":"sources/vendor/Broadcom/dlp/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes symantec_dlp symantec:dlp:syslog netdlp none"},{"location":"sources/vendor/Broadcom/dlp/#option-1-correct-source-syslog-formats","title":"Option 1: Correct Source syslog formats","text":""},{"location":"sources/vendor/Broadcom/dlp/#syslog-alert-response","title":"Syslog Alert Response","text":"<p>Login to Symantec DLP and edit the Syslog Response rule. The default configuration will appear as follows</p> <pre><code>$POLICY$^^$INCIDENT_ID$^^$SUBJECT$^^$SEVERITY$^^$MATCH_COUNT$^^$RULES$^^$SENDER$^^$RECIPIENTS$^^$BLOCKED$^^$FILE_NAME$^^$PARENT_PATH$^^$SCAN$^^$TARGET$^^$PROTOCOL$^^$INCIDENT_SNAPSHOT$\n</code></pre> <p>DO NOT replace the text prepend the following literal</p> <pre><code>SymantecDLPAlert: \n</code></pre> <p>Result note the space between \u2018:\u2019 and \u2018$\u2019</p> <pre><code>SymantecDLPAlert: $POLICY$^^$INCIDENT_ID$^^$SUBJECT$^^$SEVERITY$^^$MATCH_COUNT$^^$RULES$^^$SENDER$^^$RECIPIENTS$^^$BLOCKED$^^$FILE_NAME$^^$PARENT_PATH$^^$SCAN$^^$TARGET$^^$PROTOCOL$^^$INCIDENT_SNAPSHOT$\n</code></pre>"},{"location":"sources/vendor/Broadcom/dlp/#syslog-system-events","title":"Syslog System events","text":"<ul> <li>Navigate to the installed directory, for example <code>&lt;drive&gt;:\\SymantecDLP\\Protect\\config</code> directory on Windows or the <code>/opt/SymantecDLP/Protect/config</code> directory on Linux.</li> <li>Open the <code>Manager.properties</code> file.</li> <li>Comment out any uncommented line starting with <code>systemevent.syslog.format</code></li> <li>Add the following line <code>systemevent.syslog.format= {0.EN_US} SymantecDLP: {1.EN_US} - {2.EN_US}</code></li> <li>Restart symantec DLP</li> </ul>"},{"location":"sources/vendor/Broadcom/dlp/#option-2-manual-vendor-product-by-source-parser-configuration","title":"Option 2: Manual Vendor Product by source Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-symantec_dlp.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-symantec_dlp[sc4s-vps] {\n filter {      \n        #netmask(169.254.100.1/24)\n        #host(\"-esx-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('symantec')\n            product('dlp')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Broadcom/ep/","title":"Symantec Endpoint Protection (SEPM)","text":""},{"location":"sources/vendor/Broadcom/ep/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> <li>KNOWN DATA LOSS ISSUE - The implementation of the syslog output component causes a \u201cburst\u201d behavior when run on schedule this burst can be larger than the udp buffer size on the source and or destination (sc4s) there is no possible workaround and the use of the Splunk Universal Forwarder to monitor file based output is recommended.</li> </ul>"},{"location":"sources/vendor/Broadcom/ep/#product-symantec-endpoint-protection","title":"Product - Symantec Endpoint Protection","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2772/ Product Manual https://techdocs.broadcom.com/content/broadcom/techdocs/us/en/symantec-security-software/endpoint-security-and-management/endpoint-protection/all/Monitoring-Reporting-and-Enforcing-Compliance/viewing-logs-v7522439-d37e464/exporting-data-to-a-syslog-server-v8442743-d15e1107.html"},{"location":"sources/vendor/Broadcom/ep/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes symantec:ep:syslog Warning the syslog method of accepting EP logs has been reported to show high data loss and is not Supported by Splunk symantec:ep:admin:syslog none symantec:ep:agent:syslog none symantec:ep:agt:system:syslog none symantec:ep:behavior:syslog none symantec:ep:packet:syslog none symantec:ep:policy:syslog none symantec:ep:proactive:syslog none symantec:ep:risk:syslog none symantec:ep:scan:syslog none symantec:ep:scm:system:syslog none symantec:ep:security:syslog none symantec:ep:traffic:syslog none"},{"location":"sources/vendor/Broadcom/ep/#index-configuration","title":"Index Configuration","text":"key index notes symantec_ep epav none"},{"location":"sources/vendor/Broadcom/proxy/","title":"ProxySG/ASG","text":"<p>Symantec now Broadcom ProxySG/ASG is formerly known as the \u201cBluecoat\u201d proxy</p> <p>Broadcom products are inclusive of products formerly marketed under Symantec and Bluecoat brands.</p>"},{"location":"sources/vendor/Broadcom/proxy/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>The standard/default bluecoat syslog configurations are NOT supported a SC4S specific configuration is provided below</li> <li>RFC5424 without IETF Frame must use 514/TCP or 6514/TLS</li> </ul>"},{"location":"sources/vendor/Broadcom/proxy/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2758/ Product Manual https://support.symantec.com/us/en/article.tech242216.html"},{"location":"sources/vendor/Broadcom/proxy/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes bluecoat:proxysg:access:kv Requires version TA 3.8.1 bluecoat:proxysg:access:syslog Requires version TA 3.8.1"},{"location":"sources/vendor/Broadcom/proxy/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes bluecoat_proxy bluecoat:proxysg:access:syslog netops none bluecoat_proxy_splunkkv bluecoat:proxysg:access:kv netproxy none"},{"location":"sources/vendor/Broadcom/proxy/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Refer to the Splunk TA documentation for the specific customer format required for proxy configuration<ul> <li>Select TCP or SSL transport option</li> <li>Ensure the format of the event is customized as follows</li> </ul> </li> </ul> <pre><code>&lt;111&gt;1 $(date)T$(x-bluecoat-hour-utc):$(x-bluecoat-minute-utc):$(x-bluecoat-second-utc)Z $(s-computername) ProxySG - splunk_format - c-ip=$(c-ip) rs-Content-Type=$(quot)$(rs(Content-Type))$(quot)  cs-auth-groups=$(cs-auth-groups) cs-bytes=$(cs-bytes) cs-categories=$(cs-categories) cs-host=$(cs-host) cs-ip=$(cs-ip) cs-method=$(cs-method) cs-uri-port=$(cs-uri-port) cs-uri-scheme=$(cs-uri-scheme) cs-User-Agent=$(quot)$(cs(User-Agent))$(quot) cs-username=$(cs-username) dnslookup-time=$(dnslookup-time) duration=$(duration) rs-status=$(rs-status) rs-version=$(rs-version) s-action=$(s-action) s-ip=$(s-ip) service.name=$(service.name) service.group=$(service.group) s-supplier-ip=$(s-supplier-ip) s-supplier-name=$(s-supplier-name) sc-bytes=$(sc-bytes) sc-filter-result=$(sc-filter-result) sc-status=$(sc-status) time-taken=$(time-taken) x-exception-id=$(x-exception-id) x-virus-id=$(x-virus-id) c-url=$(quot)$(url)$(quot) cs-Referer=$(quot)$(cs(Referer))$(quot) c-cpu=$(c-cpu) connect-time=$(connect-time) cs-auth-groups=$(cs-auth-groups) cs-headerlength=$(cs-headerlength) cs-threat-risk=$(cs-threat-risk) r-ip=$(r-ip) r-supplier-ip=$(r-supplier-ip) rs-time-taken=$(rs-time-taken) rs-server=$(rs(server)) s-connect-type=$(s-connect-type) s-icap-status=$(s-icap-status) s-sitename=$(s-sitename) s-source-port=$(s-source-port) s-supplier-country=$(s-supplier-country) sc-Content-Encoding=$(sc(Content-Encoding)) sr-Accept-Encoding=$(sr(Accept-Encoding)) x-auth-credential-type=$(x-auth-credential-type) x-cookie-date=$(x-cookie-date) x-cs-certificate-subject=$(x-cs-certificate-subject) x-cs-connection-negotiated-cipher=$(x-cs-connection-negotiated-cipher) x-cs-connection-negotiated-cipher-size=$(x-cs-connection-negotiated-cipher-size) x-cs-connection-negotiated-ssl-version=$(x-cs-connection-negotiated-ssl-version) x-cs-ocsp-error=$(x-cs-ocsp-error) x-cs-Referer-uri=$(x-cs(Referer)-uri) x-cs-Referer-uri-address=$(x-cs(Referer)-uri-address) x-cs-Referer-uri-extension=$(x-cs(Referer)-uri-extension) x-cs-Referer-uri-host=$(x-cs(Referer)-uri-host) x-cs-Referer-uri-hostname=$(x-cs(Referer)-uri-hostname) x-cs-Referer-uri-path=$(x-cs(Referer)-uri-path) x-cs-Referer-uri-pathquery=$(x-cs(Referer)-uri-pathquery) x-cs-Referer-uri-port=$(x-cs(Referer)-uri-port) x-cs-Referer-uri-query=$(x-cs(Referer)-uri-query) x-cs-Referer-uri-scheme=$(x-cs(Referer)-uri-scheme) x-cs-Referer-uri-stem=$(x-cs(Referer)-uri-stem) x-exception-category=$(x-exception-category) x-exception-category-review-message=$(x-exception-category-review-message) x-exception-company-name=$(x-exception-company-name) x-exception-contact=$(x-exception-contact) x-exception-details=$(x-exception-details) x-exception-header=$(x-exception-header) x-exception-help=$(x-exception-help) x-exception-last-error=$(x-exception-last-error) x-exception-reason=$(x-exception-reason) x-exception-sourcefile=$(x-exception-sourcefile) x-exception-sourceline=$(x-exception-sourceline) x-exception-summary=$(x-exception-summary) x-icap-error-code=$(x-icap-error-code) x-rs-certificate-hostname=$(x-rs-certificate-hostname) x-rs-certificate-hostname-category=$(x-rs-certificate-hostname-category) x-rs-certificate-observed-errors=$(x-rs-certificate-observed-errors) x-rs-certificate-subject=$(x-rs-certificate-subject) x-rs-certificate-validate-status=$(x-rs-certificate-validate-status) x-rs-connection-negotiated-cipher=$(x-rs-connection-negotiated-cipher) x-rs-connection-negotiated-cipher-size=$(x-rs-connection-negotiated-cipher-size) x-rs-connection-negotiated-ssl-version=$(x-rs-connection-negotiated-ssl-version) x-rs-ocsp-error=$(x-rs-ocsp-error) cs-uri-extension=$(cs-uri-extension) cs-uri-path=$(cs-uri-path) cs-uri-query=$(quot)$(cs-uri-query)$(quot) c-uri-pathquery=$(c-uri-pathquery)\n</code></pre>"},{"location":"sources/vendor/Broadcom/sslva/","title":"SSL Visibility Appliance","text":""},{"location":"sources/vendor/Broadcom/sslva/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Broadcom/sslva/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual https://knowledge.broadcom.com/external/article/168879/when-sending-session-logs-from-ssl-visib.html"},{"location":"sources/vendor/Broadcom/sslva/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes broadcom:sslva none"},{"location":"sources/vendor/Broadcom/sslva/#index-configuration","title":"Index Configuration","text":"key index notes broadcom_sslva netproxy none"},{"location":"sources/vendor/Brocade/switch/","title":"Switch","text":""},{"location":"sources/vendor/Brocade/switch/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Brocade/switch/#product-switches","title":"Product - Switches","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Brocade/switch/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes brocade:syslog None"},{"location":"sources/vendor/Brocade/switch/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes brocade_syslog brocade:syslog netops none"},{"location":"sources/vendor/Brocade/switch/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app_parsers/app-vps-brocade_syslog.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-brocade_syslog[sc4s-vps] {\n filter { \n        host(\"^test_brocade-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('brocade')\n            product('syslog')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Buffalo/","title":"Terastation","text":""},{"location":"sources/vendor/Buffalo/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Buffalo/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Buffalo/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes buffalo:terastation None"},{"location":"sources/vendor/Buffalo/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes buffalo_terastation buffalo:terastation infraops none"},{"location":"sources/vendor/Buffalo/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-buffalo_terastation.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-buffalo_terastation[sc4s-vps] {\n filter { \n        host(\"^test_buffalo_terastation-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('buffalo')\n            product('terastation')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Checkpoint/firewallos/","title":"Firewall OS","text":"<p>Firewall OS format is by devices supporting a direct Syslog output</p>"},{"location":"sources/vendor/Checkpoint/firewallos/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual unknown"},{"location":"sources/vendor/Checkpoint/firewallos/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cp_log:fw:syslog None"},{"location":"sources/vendor/Checkpoint/firewallos/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes checkpoint_fw cp_log:fw:syslog netops none"},{"location":"sources/vendor/Checkpoint/firewallos/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-checkpoint_fw.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-checkpoint_fw[sc4s-vps] {\n filter { \n        host(\"^checkpoint_fw-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('checkpoint')\n            product('fw')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Checkpoint/logexporter_5424/","title":"Log Exporter (Syslog)","text":""},{"location":"sources/vendor/Checkpoint/logexporter_5424/#key-facts","title":"Key Facts","text":"<ul> <li>As of 2/1/2022, the Log Exporter configuration provided by Checkpoint is defective and produces invalid data. The configuration below is REQUIRED.</li> <li>MSG format-based filter</li> <li>RFC5424 IETF Syslog without frame \u2013 use port <code>514/TCP</code>.</li> </ul> Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4293 Product Manual https://sc1.checkpoint.com/documents/App_for_Splunk/html_frameset.htm"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cp_log:syslog None"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes checkpoint_syslog cp_log:syslog netops none"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#source-and-index-configuration","title":"Source and Index Configuration","text":"<p>Checkpoint Software blades with a CIM mapping have been sub-grouped into sources to allow routing to appropriate indexes. All other source metadata is left as their defaults.</p> key source index notes checkpoint_syslog_dlp dlp netdlp none checkpoint_syslog_email email email none checkpoint_syslog_firewall firewall netfw none checkpoint_syslog_sessions sessions netops none checkpoint_syslog_web web netproxy none checkpoint_syslog_audit audit netops none checkpoint_syslog_endpoint endpoint netops none checkpoint_syslog_network network netops checkpoint_syslog_ids ids netids checkpoint_syslog_ids_malware ids_malware netids"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#source-configuration","title":"Source Configuration","text":""},{"location":"sources/vendor/Checkpoint/logexporter_5424/#splunk-side","title":"Splunk Side","text":"<ul> <li>Install the Splunk Add-on on the search head(s) for the users interested in this data source. If SC4S is used exclusively, the add-on is not required on the target indexer or heavy forwarder.</li> <li>Review and update the <code>splunk_metadata.csv</code> file and set the <code>index</code> and <code>sourcetype</code> as required for the data source.</li> </ul>"},{"location":"sources/vendor/Checkpoint/logexporter_5424/#checkpoint-side","title":"Checkpoint Side","text":"<ol> <li>Go to the <code>cp</code> terminal and use the <code>expert</code> command to log-in in expert mode.</li> <li>Ensure the built-in variable <code>$EXPORTERDIR</code> shell variable is defined with:</li> </ol> <pre><code>echo \"$EXPORTERDIR\"\n</code></pre> <ol> <li>Create a new Log Exporter target in <code>$EXPORTERDIR/targets</code> with:</li> </ol> <pre><code>LOG_EXPORTER_NAME='SyslogToSplunk' # Name this something unique but meaningful\nTARGET_SERVER='example.internal' # The indexer or heavy forwarder to send logs to. Can be an FQDN or an IP address.\nTARGET_PORT='514' # Syslog defaults to 514\nTARGET_PROTOCOL='tcp' # IETF Syslog is specifically TCP\n\ncp_log_export add name \"$LOG_EXPORTER_NAME\" target-server \"$TARGET_SERVER\" target-port \"$TARGET_PORT\" protocol \"$TARGET_PROTOCOL\" format 'syslog'\n</code></pre> <ol> <li>Make a global copy of the built-in Syslog format definition with:</li> </ol> <pre><code>cp \"$EXPORTERDIR/conf/SyslogFormatDefinition.xml\" \"$EXPORTERDIR/conf/SplunkRecommendedFormatDefinition.xml\"\n</code></pre> <ol> <li>Edit <code>$EXPORTERDIR/conf/SplunkRecommendedFormatDefinition.xml</code> by modifying the <code>start_message_body</code>, <code>fields_separatator</code>, and <code>field_value_separatator</code> keys as shown below.    a. Note: The misspelling of \u201cseparator\u201d as \u201cseparatator\u201d is intentional, and is to line up with both Checkpoint\u2019s documentation and parser implementation.</li> </ol> <pre><code>&lt;start_message_body&gt;[sc4s@2620 &lt;/start_message_body&gt;\n&lt;!-- ... --&gt;\n&lt;fields_separatator&gt; &lt;/fields_separatator&gt;\n&lt;!-- ... --&gt;\n&lt;field_value_separatator&gt;=&lt;/field_value_separatator&gt;\n</code></pre> <ol> <li>Copy the new format config to your new target\u2019s <code>conf</code> directory with:</li> </ol> <pre><code>cp \"$EXPORTERDIR/conf/SplunkRecommendedFormatDefinition.xml\"  \"$EXPORTERDIR/targets/$LOG_EXPORTER_NAME/conf\"\n</code></pre> <ol> <li>Edit <code>$EXPORTERDIR/targets/$LOG_EXPORTER_NAME/targetConfiguration.xml</code> by adding the reference to the <code>$EXPORTERDIR/targets/$LOG_EXPORTER_NAME/conf/SplunkRecommendedFormatDefinition.xml</code> under the key <code>&lt;formatHeaderFile&gt;</code>.    a. For example, if <code>$EXPORTERDIR</code> is <code>/opt/CPrt-R81/log_exporter</code> and <code>$LOG_EXPORTER_NAME</code> is <code>SyslogToSplunk</code>, the absolute path will become:</li> </ol> <pre><code>&lt;formatHeaderFile&gt;/opt/CPrt-R81/log_exporter/targets/SyslogToSplunk/conf/SplunkRecommendedFormatDefinition.xml&lt;/formatHeaderFile&gt;\n</code></pre> <ol> <li>Restart the new log exporter with:</li> </ol> <pre><code>cp_log_export restart name \"$LOG_EXPORTER_NAME\"\n</code></pre> <ol> <li>Warning: If you\u2019re migrating from the old Splunk Syslog format, make sure that the older format\u2019s log exporter is disabled, as it would lead to data duplication.</li> </ol>"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/","title":"Log Exporter (Splunk)","text":"<p>The \u201cSplunk Format\u201d is legacy and should not be used for new deployments see Log Exporter (Syslog)</p>"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#key-facts","title":"Key Facts","text":"<ul> <li>Format is not conformant to RFC3164 avoid use</li> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul> <p>The Splunk <code>host</code> field will be derived as follows using the first match</p> <ul> <li>Use the hostname field</li> <li>Use the first CN component of origin_sic_name/originsicname</li> <li>If host is not set from CN use the <code>hostname</code> field</li> <li>If host is not set use the BSD syslog header host</li> </ul> <p>If the host is in the format <code>&lt;host&gt;-v_&lt;bladename&gt;</code> use <code>bladename</code> for host</p>"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4293/ Product Manual https://sc1.checkpoint.com/documents/App_for_Splunk/html_frameset.htm"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cp_log None"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes checkpoint_splunk cp_log netops none"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#source-and-index-configuration","title":"Source and Index Configuration","text":"<p>Checkpoint Software blades with CIM mapping have been sub-grouped into sources to allow routing to appropriate indexes. All other source meta data is left at default</p> key source index notes checkpoint_splunk_dlp dlp netdlp none checkpoint_splunk_email email email none checkpoint_splunk_firewall firewall netfw none checkpoint_splunk_os program:${program} netops none checkpoint_splunk_sessions sessions netops none checkpoint_splunk_web web netproxy none checkpoint_splunk_audit audit netops none checkpoint_splunk_endpoint endpoint netops none checkpoint_splunk_network network netops checkpoint_splunk_ids ids netids checkpoint_splunk_ids_malware ids_malware netids"},{"location":"sources/vendor/Checkpoint/logexporter_legacy/#options","title":"Options","text":"Variable default description SC4S_LISTEN_CHECKPOINT_SPLUNK_NOISE_CONTROL no Suppress any duplicate product+loguid pairs processed within 2 seconds of the last matching event SC4S_LISTEN_CHECKPOINT_SPLUNK_OLD_HOST_RULES empty string when set to <code>yes</code> reverts host name selection order to originsicname\u2013&gt;origin_sic_name\u2013&gt;hostname"},{"location":"sources/vendor/Cisco/cisco_ace/","title":"Application Control Engine (ACE)","text":""},{"location":"sources/vendor/Cisco/cisco_ace/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ace/#links","title":"Links","text":"Ref Link Splunk Add-on None"},{"location":"sources/vendor/Cisco/cisco_ace/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ace None"},{"location":"sources/vendor/Cisco/cisco_ace/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_ace cisco:ace netops none"},{"location":"sources/vendor/Cisco/cisco_acs/","title":"Cisco Access Control System (ACS)","text":""},{"location":"sources/vendor/Cisco/cisco_acs/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul> Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1811/ Product Manual https://community.cisco.com/t5/security-documents/acs-5-x-configuring-the-external-syslog-server/ta-p/3143143"},{"location":"sources/vendor/Cisco/cisco_acs/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:acs Aggregation used"},{"location":"sources/vendor/Cisco/cisco_acs/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_acs cisco:acs netauth None"},{"location":"sources/vendor/Cisco/cisco_acs/#splunk-setup-and-configuration","title":"Splunk Setup and Configuration","text":"<ul> <li>Replace the following extract using Splunk local configuration. Impacts version 1.5.0 of the addond</li> </ul> <pre><code>EXTRACT-AA-signature = CSCOacs_(?&lt;signature&gt;\\S+):?\n# Note the value of this config is empty to disable\nEXTRACT-AA-syslog_message = \nEXTRACT-acs_message_header2 = ^CSCOacs_\\S+\\s+(?&lt;log_session_id&gt;\\S+)\\s+(?&lt;total_segments&gt;\\d+)\\s+(?&lt;segment_number&gt;\\d+)\\s+(?&lt;acs_message&gt;.*)\n</code></pre>"},{"location":"sources/vendor/Cisco/cisco_asa/","title":"ASA/FTD (Firepower)","text":""},{"location":"sources/vendor/Cisco/cisco_asa/#key-facts","title":"Key facts","text":"<ul> <li>Note Splunk \u201cASA\u201d TA is also used for FTD appliances</li> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_asa/#links","title":"Links","text":"Ref Link Splunk Add-on for ASA (No long supports FWSM and PIX) https://splunkbase.splunk.com/app/1620/ Cisco eStreamer for Splunk https://splunkbase.splunk.com/app/1629/ Product Manual https://www.cisco.com/c/en/us/support/docs/security/pix-500-series-security-appliances/63884-config-asa-00.html"},{"location":"sources/vendor/Cisco/cisco_asa/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:asa cisco FTD Firepower will also use this source type except those noted below cisco:ftd cisco FTD Firepower will also use this source type except those noted below cisco:fwsm Splunk has cisco:pix cisco PIX will also use this source type except those noted below cisco:firepower:syslog FTD Unified events see https://www.cisco.com/c/en/us/td/docs/security/firepower/Syslogs/b_fptd_syslog_guide.pdf"},{"location":"sources/vendor/Cisco/cisco_asa/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_asa cisco:asa netfw none cisco_fwsm cisco:fwsm netfw none cisco_pix cisco:pix netfw none cisco_firepower cisco:firepower:syslog netids none cisco_ftd cisco:ftd netfw none"},{"location":"sources/vendor/Cisco/cisco_asa/#source-setup-and-configuration","title":"Source Setup and Configuration","text":"<ul> <li>Follow vendor configuration steps per Product Manual above ensure:</li> <li>Log Level is 6 \u201cInformational\u201d</li> <li>Protocol is TCP/IP</li> <li>permit-hostdown is on</li> <li>device-id is hostname and included</li> <li>timestamp is included</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_dna/","title":"Digital Network Area(DNA)","text":""},{"location":"sources/vendor/Cisco/cisco_dna/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>rfc5424 default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_dna/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_dna/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:dna None"},{"location":"sources/vendor/Cisco/cisco_dna/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_dna cisco:dna netops None"},{"location":"sources/vendor/Cisco/cisco_dna/#sc4s-options","title":"SC4S Options","text":"Variable default description SC4S_SOURCE_CISCO_DNA_FIXHOST yes Current firmware incorrectly sends the value of the syslog server host name (destination) in the host field if this is ever corrected this value will need to be set back to no we suggest using yes"},{"location":"sources/vendor/Cisco/cisco_esa/","title":"Email Security Appliance (ESA)","text":""},{"location":"sources/vendor/Cisco/cisco_esa/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_esa/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1761/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/esa/esa14-0/user_guide/b_ESA_Admin_Guide_14-0.pdf"},{"location":"sources/vendor/Cisco/cisco_esa/#esa-log-configuration","title":"ESA Log Configuration","text":"<p>If feasible for you, you can use following log configuration on the ESA. The log name configured on the ESA can then be parsed easily by sc4s.</p> ESA Log Name ESA Log Type sc4s_gui_logs HTTP Logs sc4s_mail_logs IronPort Text Mail Logs sc4s_amp AMP Engine Logs sc4s_audit_logs Audit Logs sc4s_antispam Anti-Spam Logs sc4s_content_scanner Content Scanner Logs sc4s_error_logs IronPort Text Mail Logs (Loglevel: Critical) sc4s_system_logs System Logs"},{"location":"sources/vendor/Cisco/cisco_esa/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:esa:http The HTTP logs of Cisco IronPort ESA record information about the secure HTTP services enabled on the interface. cisco:esa:textmail Text mail logs of Cisco IronPort ESA record email information and status. cisco:esa:amp Advanced Malware Protection (AMP) of Cisco IronPort ESA records malware detection and blocking, continuous analysis, and retrospective alerting details. cisco:esa:authentication These logs record successful user logins and unsuccessful login attempts. cisco:esa:cef The Consolidated Event Logs summarizes each message event in a single log line. cisco:esa:error_logs Error logs of Cisco IronPort ESA records error that occurred for ESA configurations or internal issues. cisco:esa:content_scanner Content scanner logs of Cisco IronPort ESA scans messages that contain password-protected attachments for malicious activity and data privacy. cisco:esa:antispam Anti-spam logs record the status of the anti-spam scanning feature of your system, including the status on receiving updates of the latest anti-spam rules. Also, any logs related to the Context Adaptive Scanning Engine are logged here. cisco:esa:system_logs System logs record the boot information, virtual appliance license expiration alerts, DNS status information, and comments users typed using commit command."},{"location":"sources/vendor/Cisco/cisco_esa/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_esa cisco:esa:http email None cisco_esa cisco:esa:textmail email None cisco_esa cisco:esa:amp email None cisco_esa cisco:esa:authentication email None cisco_esa cisco:esa:cef email None cisco_esa cisco:esa:error_logs email None cisco_esa cisco:esa:content_scanner email None cisco_esa cisco:esa:antispam email None cisco_esa cisco:esa:system_logs email None"},{"location":"sources/vendor/Cisco/cisco_esa/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-cisco_esa.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-cisco_esa[sc4s-vps] {\n filter { \n        host(\"^esa-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('cisco')\n            product('esa')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Cisco/cisco_imc/","title":"Cisco Integrated Management Controller (IMC)","text":""},{"location":"sources/vendor/Cisco/cisco_imc/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul> Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_imc/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ucm None"},{"location":"sources/vendor/Cisco/cisco_imc/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_cimc cisco:infraops infraops None"},{"location":"sources/vendor/Cisco/cisco_ios/","title":"Cisco Networking (IOS and Compatible)","text":"<p>Cisco Network Products of multiple types share common logging characteristics the following types are known to be compatible:</p> <ul> <li>Cisco AireOS (AP &amp; WLC)</li> <li>Cisco APIC/ACI</li> <li>Cisco IOS</li> <li>Cisco IOS-XR</li> <li>Cisco IOS-XE</li> <li>Cisco NX-OS</li> <li>Cisco FX-OS</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ios/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ios/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1467/ IOS Manual https://www.cisco.com/c/en/us/td/docs/switches/lan/catalyst2960/software/release/12-2_55_se/configuration/guide/scg_2960/swlog.html NX-OS Manual https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus9000/sw/6-x/system_management/configuration/guide/b_Cisco_Nexus_9000_Series_NX-OS_System_Management_Configuration_Guide/sm_5syslog.html Cisco ACI https://community.cisco.com/legacyfs/online/attachments/document/technote-aci-syslog_external-v1.pdf Cisco WLC &amp; AP https://www.cisco.com/c/en/us/support/docs/wireless/4100-series-wireless-lan-controllers/107252-WLC-Syslog-Server.html#anc8 Cisco IOS-XR https://www.cisco.com/c/en/us/td/docs/iosxr/cisco8000/system-monitoring/73x/b-system-monitoring-cg-cisco8k-73x/implementing_system_logging.html"},{"location":"sources/vendor/Cisco/cisco_ios/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ios This source type is also used for NX-OS, ACI and WLC product lines cisco:xr This source type is used for Cisco IOS XR"},{"location":"sources/vendor/Cisco/cisco_ios/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_ios cisco:ios netops none cisco_xr cisco:xr netops none"},{"location":"sources/vendor/Cisco/cisco_ios/#filter-type","title":"Filter type","text":"<ul> <li>Cisco IOS products can be identified by message parsing alone</li> <li>Cisco WLC, and ACI products must be identified by host or ip assignment update the filter <code>f_cisco_ios</code> as required</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ios/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>IOS Follow vendor configuration steps per Product Manual above ensure:</li> <li>Ensure a reliable NTP server is set and synced</li> <li>Log Level is 6 \u201cInformational\u201d</li> <li>Protocol is TCP/IP</li> <li>permit-hostdown is on</li> <li>device-id is hostname and included</li> <li>timestamp is included</li> <li>NX-OS Follow vendor configuration steps per Product Manual above ensure:</li> <li>Ensure a reliable NTP server is set and synced</li> <li>Log Level is 6 \u201cInformational\u201d user may select alternate levels by module based on use cases</li> <li>Protocol is TCP/IP</li> <li>device-id is hostname and included</li> <li>timestamp is included and millisecond accuracy selected</li> <li>ACI Logging configuration of the ACI product often varies by use case.</li> <li>Ensure NTP sync is configured and active</li> <li>Ensure proper host names are configured</li> <li>WLC</li> <li>Ensure NTP sync is configured and active</li> <li>Ensure proper host names are configured</li> <li>For security use cases per AP logging is required</li> </ul> <p>If you want to send raw logs to splunk (without any drop) then only use this feature Please set following property in env_file: <pre><code>SC4S_ENABLE_CISCO_IOS_RAW_MSG=yes\n</code></pre> Restart SC4S and it will send entire message without any drop.</p> <ul> <li>NOTE: Please use this feature only when there is a special need to get entire raw message. This is not supported by splunk.</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ise/","title":"Cisco ise","text":""},{"location":"sources/vendor/Cisco/cisco_ise/#cisco-identity-services-engine-ise","title":"Cisco Identity Services Engine (ISE)","text":""},{"location":"sources/vendor/Cisco/cisco_ise/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ise/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1915/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/ise/syslog/Cisco_ISE_Syslogs/m_IntrotoSyslogs.html"},{"location":"sources/vendor/Cisco/cisco_ise/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ise:syslog Aggregation used"},{"location":"sources/vendor/Cisco/cisco_ise/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_ise cisco:ise:syslog netauth None"},{"location":"sources/vendor/Cisco/cisco_meraki/","title":"Cisco meraki","text":""},{"location":"sources/vendor/Cisco/cisco_meraki/#meraki-mr-ms-mx","title":"Meraki (MR, MS, MX)","text":""},{"location":"sources/vendor/Cisco/cisco_meraki/#key-facts","title":"Key facts","text":"<ul> <li>Cisco Meraki messages are not distinctive, which means that it\u2019s impossible to parse the sourcetype based on the log message.</li> <li>Because of the above you should either configure known Cisco Meraki hosts in SC4S, or open unique ports for Cisco Meraki devices.</li> <li>Splunk Add-on for Cisco Meraki 2.1.0 doesn\u2019t support syslog. Use TA-meraki instead. <code>TA-meraki 1.1.5</code> requires sourcetype <code>meraki</code>.</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_meraki/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3018 Product Manual https://documentation.meraki.com/zGeneral_Administration/Monitoring_and_Reporting/Syslog_Server_Overview_and_Configuration"},{"location":"sources/vendor/Cisco/cisco_meraki/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes meraki:accesspoints Not compliant with the Splunk Add-on meraki:securityappliances Not compliant with the Splunk Add-on meraki:switches Not compliant with the Splunk Add-on meraki For all Meraki devices. Compliant with the Splunk Add-on"},{"location":"sources/vendor/Cisco/cisco_meraki/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes meraki_accesspoints meraki:accesspoints netfw meraki_securityappliances meraki:securityappliances netfw meraki_switches meraki:switches netfw cisco_meraki meraki netfw"},{"location":"sources/vendor/Cisco/cisco_meraki/#parser-configuration","title":"Parser Configuration","text":"<ol> <li> <p>Either by defining Cisco Meraki hosts: <pre><code>#/opt/sc4s/local/config/app_parsers/app-vps-cisco_meraki.conf\n#File name provided is a suggestion it must be globally unique\n\nblock parser app-vps-test-cisco_meraki() {\n    channel {\n        if {\n            filter { host(\"^test-mx-\") };\n            parser { \n                p_set_netsource_fields(\n                    vendor('meraki')\n                    product('securityappliances')\n                ); \n            };\n        } elif {\n            filter { host(\"^test-mr-\") };\n            parser { \n                p_set_netsource_fields(\n                    vendor('meraki')\n                    product('accesspoints')\n                ); \n            };\n        } elif {\n            filter { host(\"^test-ms-\") };\n            parser { \n                p_set_netsource_fields(\n                    vendor('meraki')\n                    product('switches')\n                ); \n            };\n        } else {\n            parser { \n                p_set_netsource_fields(\n                    vendor('cisco')\n                    product('meraki')\n                ); \n            };\n        };\n    }; \n};\n\n\napplication app-vps-test-cisco_meraki[sc4s-vps] {\n    filter {\n        host(\"^test-meraki-\")\n        or host(\"^test-mx-\")\n        or host(\"^test-mr-\")\n        or host(\"^test-ms-\")\n    };\n    parser { app-vps-test-cisco_meraki(); };\n};\n</code></pre></p> </li> <li> <p>Or by a unique port: <pre><code># /opt/sc4s/env_file\nSC4S_LISTEN_CISCO_MERAKI_UDP_PORT=5004\nSC4S_LISTEN_MERAKI_SECURITYAPPLIANCES_UDP_PORT=5005\nSC4S_LISTEN_MERAKI_ACCESSPOINTS_UDP_PORT=5006\nSC4S_LISTEN_MERAKI_SWITCHES_UDP_PORT=5007\n</code></pre></p> </li> </ol>"},{"location":"sources/vendor/Cisco/cisco_mm/","title":"Meeting Management","text":""},{"location":"sources/vendor/Cisco/cisco_mm/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_mm/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_mm/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:mm:system:* final component take from the program field of the message header cisco:mm:audit Requires setup of vendor product by source see below"},{"location":"sources/vendor/Cisco/cisco_mm/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_mm_system cisco:mm:system:* netops None cisco_mm_audit cisco:mm:audit netops None"},{"location":"sources/vendor/Cisco/cisco_mm/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-cisco_mm.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-cisco_mm[sc4s-vps] {\n filter { \n        host('^test-cmm-')\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('cisco')\n            product('mm')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Cisco/cisco_ms/","title":"Meeting Server","text":""},{"location":"sources/vendor/Cisco/cisco_ms/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ms/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_ms/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ms None"},{"location":"sources/vendor/Cisco/cisco_ms/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_ms cisco:ms netops None"},{"location":"sources/vendor/Cisco/cisco_ms/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-cisco_ms.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-cisco_ms[sc4s-vps] {\n filter { \n        host('^test-cms-')\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('cisco')\n            product('ms')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Cisco/cisco_tvcs/","title":"TelePresence Video Communication Server (TVCS)","text":""},{"location":"sources/vendor/Cisco/cisco_tvcs/#links","title":"Links","text":"Ref Link Product Manual https://www.cisco.com/c/en/us/products/unified-communications/telepresence-video-communication-server-vcs/index.html"},{"location":"sources/vendor/Cisco/cisco_tvcs/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:vcs none"},{"location":"sources/vendor/Cisco/cisco_tvcs/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_tvcs cisco:tvcs main none"},{"location":"sources/vendor/Cisco/cisco_ucm/","title":"Unified Communications Manager (UCM)","text":""},{"location":"sources/vendor/Cisco/cisco_ucm/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ucm/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_ucm/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ucm None"},{"location":"sources/vendor/Cisco/cisco_ucm/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_ucm cisco:ucm ucm None"},{"location":"sources/vendor/Cisco/cisco_ucshx/","title":"Unified Computing System (UCS)","text":""},{"location":"sources/vendor/Cisco/cisco_ucshx/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_ucshx/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_ucshx/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:ucs None"},{"location":"sources/vendor/Cisco/cisco_ucshx/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_ucs cisco:ucs infraops None"},{"location":"sources/vendor/Cisco/cisco_viptela/","title":"Viptela","text":""},{"location":"sources/vendor/Cisco/cisco_viptela/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_viptela/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual multiple"},{"location":"sources/vendor/Cisco/cisco_viptela/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cisco:viptela None"},{"location":"sources/vendor/Cisco/cisco_viptela/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_viptela cisco:viptela netops None"},{"location":"sources/vendor/Cisco/cisco_wsa/","title":"Web Security Appliance (WSA)","text":""},{"location":"sources/vendor/Cisco/cisco_wsa/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_wsa/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1747/ Product Manual https://www.cisco.com/c/en/us/td/docs/security/wsa/wsa11-7/user_guide/b_WSA_UserGuide_11_7.html"},{"location":"sources/vendor/Cisco/cisco_wsa/#sourcetypes","title":"Sourcetypes","text":"<p>| cisco:wsa:l4tm      | The L4TM logs of Cisco IronPort WSA record sites added to the L4TM block and allow lists.                                                                                                    | | cisco:wsa:squid      | The access logs of Cisco IronPort WSA version prior to 11.7 record Web Proxy client history in squid.                                                                                           | | cisco:wsa:squid:new     | The access logs of Cisco IronPort WSA version since 11.7 record Web Proxy client history in squid.                                                                                           | | cisco:wsa:w3c:recommended     | The access logs of Cisco IronPort WSA version since 12.5 record Web Proxy client history in W3C.                                                                                           |</p>"},{"location":"sources/vendor/Cisco/cisco_wsa/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cisco_wsa cisco:wsa:l4tm netproxy None cisco_wsa cisco:wsa:squid netproxy None cisco_wsa cisco:wsa:squid:new netproxy None cisco_wsa cisco:wsa:w3c:recommended netproxy None"},{"location":"sources/vendor/Cisco/cisco_wsa/#filter-type","title":"Filter type","text":"<p>IP, Netmask or Host</p>"},{"location":"sources/vendor/Cisco/cisco_wsa/#source-setup-and-configuration","title":"Source Setup and Configuration","text":"<ul> <li>Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer.</li> <li>WSA Follow vendor configuration steps per Product Manual.</li> <li>Ensure host and timestamp are included.</li> </ul>"},{"location":"sources/vendor/Cisco/cisco_wsa/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-cisco_wsa.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-cisco_wsa[sc4s-vps] {\n filter { \n        host(\"^wsa-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('cisco')\n            product('wsa')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Citrix/netscaler/","title":"Netscaler ADC/SDX","text":""},{"location":"sources/vendor/Citrix/netscaler/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Citrix/netscaler/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2770/ Product Manual https://docs.citrix.com/en-us/citrix-adc/12-1/system/audit-logging/configuring-audit-logging.html"},{"location":"sources/vendor/Citrix/netscaler/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes citrix:netscaler:syslog None citrix:netscaler:appfw None citrix:netscaler:appfw:cef None"},{"location":"sources/vendor/Citrix/netscaler/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes citrix_netscaler citrix:netscaler:syslog netfw none citrix_netscaler citrix:netscaler:appfw netfw none citrix_netscaler citrix:netscaler:appfw:cef netfw none"},{"location":"sources/vendor/Citrix/netscaler/#source-setup-and-configuration","title":"Source Setup and Configuration","text":"<ul> <li>Follow vendor configuration steps per Product Manual above. Ensure the data format selected is \u201cDDMMYYYY\u201d</li> </ul>"},{"location":"sources/vendor/Clearswift/","title":"WAF (Cloud)","text":""},{"location":"sources/vendor/Clearswift/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC 5424 Framed</li> </ul>"},{"location":"sources/vendor/Clearswift/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual https://clearswifthelp.clearswift.com/SEG/472/en/Content/Sections/SystemsCenter/SYCLogList.htm"},{"location":"sources/vendor/Clearswift/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes <code>clearswift:${PROGRAM}</code> none"},{"location":"sources/vendor/Clearswift/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes clearswift <code>clearswift:${PROGRAM}</code> email None"},{"location":"sources/vendor/Clearswift/#parser-configuration","title":"Parser Configuration","text":"<p>```c</p>"},{"location":"sources/vendor/Clearswift/#optsc4slocalconfigapp-parsersapp-vps-clearswiftconf","title":"/opt/sc4s/local/config/app-parsers/app-vps-clearswift.conf","text":""},{"location":"sources/vendor/Clearswift/#file-name-provided-is-a-suggestion-it-must-be-globally-unique","title":"File name provided is a suggestion it must be globally unique","text":"<p>application app-vps-clearswift[sc4s-vps] {     filter {         host(\u201ctest-clearswift-\u201d type(string) flags(prefix))     };     parser {         p_set_netsource_fields(             vendor(\u2018clearswift\u2019)             product(\u2018clearswift\u2019)         );     }; };</p>"},{"location":"sources/vendor/Cohesity/cluster/","title":"Cluster","text":""},{"location":"sources/vendor/Cohesity/cluster/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cohesity/cluster/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Cohesity/cluster/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cohesity:cluster:audit None cohesity:cluster:dataprotection None cohesity:api:audit None cohesity:alerts None"},{"location":"sources/vendor/Cohesity/cluster/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes cohesity_cluster_audit cohesity:cluster:audit infraops none cohesity_api_audit cohesity:api:audit infraops none cohesity_cluster_dataprotection cohesity:cluster:dataprotection infraops none cohesity_alerts cohesity:alerts infraops none"},{"location":"sources/vendor/CyberArk/epv/","title":"Vendor - CyberArk","text":""},{"location":"sources/vendor/CyberArk/epv/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/CyberArk/epv/#product-epv","title":"Product - EPV","text":"Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/2891/ Add-on Manual https://docs.splunk.com/Documentation/AddOns/latest/CyberArk/About"},{"location":"sources/vendor/CyberArk/epv/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cyberark:epv:cef None"},{"location":"sources/vendor/CyberArk/epv/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes Cyber-Ark_Vault cyberark:epv:cef netauth none"},{"location":"sources/vendor/CyberArk/pta/","title":"PTA","text":""},{"location":"sources/vendor/CyberArk/pta/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/CyberArk/pta/#links","title":"Links","text":"Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/2891/ Add-on Manual https://docs.splunk.com/Documentation/AddOns/latest/CyberArk/About Product Manual https://docs.cyberark.com/PAS/Latest/en/Content/PTA/CEF-Based-Format-Definition.htm"},{"location":"sources/vendor/CyberArk/pta/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cyberark:pta:cef None"},{"location":"sources/vendor/CyberArk/pta/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes CyberArk_PTA cyberark:pta:cef main none"},{"location":"sources/vendor/Cylance/protect/","title":"Protect","text":""},{"location":"sources/vendor/Cylance/protect/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>None conformant legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Cylance/protect/#links","title":"Links","text":"Ref Link Splunk Add-on CyberArk https://splunkbase.splunk.com/app/3709/"},{"location":"sources/vendor/Cylance/protect/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes syslog_protect Catchall syslog_threat_classification None syslog_audit_log None syslog_exploit None syslog_app_control None syslog_threat None syslog_device None syslog_device_control None syslog_script_control None syslog_optics None"},{"location":"sources/vendor/Cylance/protect/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes cylance_protect syslog_protect epintel none cylance_protect_auditlog syslog_audit_log epintel none cylance_protect_threatclassification syslog_threat_classification epintel none cylance_protect_exploitattempt syslog_exploit epintel none cylance_protect_appcontrol syslog_app_control epintel none cylance_protect_threat syslog_threat epintel none cylance_protect_device syslog_device epintel none cylance_protect_devicecontrol syslog_device_control epintel none cylance_protect_scriptcontrol syslog_protect epintel none cylance_protect_scriptcontrol syslog_script_control epintel none cylance_protect_optics syslog_optics epintel none"},{"location":"sources/vendor/DARKTRACE/darktrace/","title":"Darktrace","text":""},{"location":"sources/vendor/DARKTRACE/darktrace/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/DARKTRACE/darktrace/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/DARKTRACE/darktrace/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes darktrace none darktrace:audit none"},{"location":"sources/vendor/DARKTRACE/darktrace/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes darktrace_syslog darktrace netids None darktrace_audit darktrace_audit netids None"},{"location":"sources/vendor/Dell/avamar/","title":"Dell Avamar","text":""},{"location":"sources/vendor/Dell/avamar/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Dell/avamar/#links","title":"Links","text":"Ref Link Splunk Add-on na Add-on Manual https://www.delltechnologies.com/asset/en-us/products/data-protection/technical-support/docu91832.pdf"},{"location":"sources/vendor/Dell/avamar/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes dell:avamar:msc None"},{"location":"sources/vendor/Dell/avamar/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes dell_avamar_cms dell:avamar:msc netops none"},{"location":"sources/vendor/Dell/cmc/","title":"CMC (VRTX)","text":""},{"location":"sources/vendor/Dell/cmc/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Dell/cmc/#links","title":"Links","text":"Ref Link Splunk Add-on na Add-on Manual https://www.dell.com/support/manuals/en-us/dell-chassis-management-controller-v3.10-dell-poweredge-vrtx/cmcvrtx31ug/overview?guid=guid-84595265-d37c-4765-8890-90f629737b17"},{"location":"sources/vendor/Dell/cmc/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes dell:poweredge:cmc:syslog None"},{"location":"sources/vendor/Dell/cmc/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes dell_poweredge_cmc dell:poweredge:cmc:syslog infraops none"},{"location":"sources/vendor/Dell/cmc/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-dell_cmc.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-dell_cmc[sc4s-vps] {\n filter { \n        host(\"test-dell-cmc-\" type(string) flags(prefix))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('dell')\n            product('poweredge_cmc')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Dell/emc_powerswitchn/","title":"EMC Powerswitch N Series","text":""},{"location":"sources/vendor/Dell/emc_powerswitchn/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Dell/emc_powerswitchn/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual https://dl.dell.com/manuals/common/networking_nxxug_en-us.pdf"},{"location":"sources/vendor/Dell/emc_powerswitchn/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes dell:emc:powerswitch:n None"},{"location":"sources/vendor/Dell/emc_powerswitchn/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes dellemc_powerswitch_n all netops none"},{"location":"sources/vendor/Dell/emc_powerswitchn/#parser-configuration","title":"Parser Configuration","text":"<ol> <li> <p>Through sc4s-vps <pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-dell_switch_n.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-dell_switch_n[sc4s-vps] {\n filter { \n        host(\"test-dell-switch-n-\" type(string) flags(prefix))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('dellemc')\n            product('powerswitch_n')\n        ); \n    };   \n};\n</code></pre></p> </li> <li> <p>or through unique port <pre><code># /opt/sc4s/env_file \nSC4S_LISTEN_DELLEMC_POWERSWITCH_N_UDP_PORT=5005\n</code></pre></p> </li> </ol>"},{"location":"sources/vendor/Dell/idrac/","title":"iDrac","text":""},{"location":"sources/vendor/Dell/idrac/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Dell/idrac/#links","title":"Links","text":"Ref Link Splunk Add-on na Add-on Manual https://www.dell.com/support/manuals/en-au/dell-opnmang-sw-v8.1/eemi_13g_v1.2-v1/introduction?guid=guid-8f22a1a9-ac01-43d1-a9d2-390ca6708d5e&amp;lang=en-us"},{"location":"sources/vendor/Dell/idrac/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes dell:poweredge:idrac:syslog None"},{"location":"sources/vendor/Dell/idrac/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes dell_poweredge_idrac dell:poweredge:idrac:syslog infraops none"},{"location":"sources/vendor/Dell/rsa_secureid/","title":"RSA SecureID","text":""},{"location":"sources/vendor/Dell/rsa_secureid/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Dell/rsa_secureid/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2958/ Product Manual https://docs.splunk.com/Documentation/AddOns/released/RSASecurID/Aboutthisaddon"},{"location":"sources/vendor/Dell/rsa_secureid/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes rsa:securid:syslog Catchall; used if a more specific source type can not be identified rsa:securid:admin:syslog None rsa:securid:runtime:syslog None nix:syslog None"},{"location":"sources/vendor/Dell/rsa_secureid/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes dell-rsa_secureid all netauth none dell-rsa_secureid_trace rsa:securid:trace netauth none dell-rsa_secureid nix:syslog osnix uses os_nix key of not configured bye host/ip/port"},{"location":"sources/vendor/Dell/rsa_secureid/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app_parsers/app-vps-dell_rsa_secureid.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-dell_rsa_secureid[sc4s-vps] {\n filter { \n        host(\"test_rsasecureid*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('dell')\n            product('rsa_secureid')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Dell/sonic/","title":"Dell Networking SONiC","text":""},{"location":"sources/vendor/Dell/sonic/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> </ul>"},{"location":"sources/vendor/Dell/sonic/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual link"},{"location":"sources/vendor/Dell/sonic/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes dell:sonic None"},{"location":"sources/vendor/Dell/sonic/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes dell_sonic dell:sonic netops none"},{"location":"sources/vendor/Dell/sonic/#parser-configuration","title":"Parser Configuration","text":"<ol> <li> <p>Through sc4s-vps <pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-dell_sonic.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-dell_sonic[sc4s-vps] {\n filter { \n        host(\"sonic\" type(string) flags(prefix))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('dell')\n            product('sonic')\n        ); \n    };   \n};\n</code></pre></p> </li> <li> <p>or through unique port <pre><code># /opt/sc4s/env_file \nSC4S_LISTEN_DELL_SONIC_UDP_PORT=5005\n</code></pre></p> </li> </ol>"},{"location":"sources/vendor/Dell/sonicwall/","title":"Sonicwall","text":""},{"location":"sources/vendor/Dell/sonicwall/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Dell/sonicwall/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/6203/"},{"location":"sources/vendor/Dell/sonicwall/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes dell:sonicwall None"},{"location":"sources/vendor/Dell/sonicwall/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes dell_sonicwall-firewall dell:sonicwall netfw none"},{"location":"sources/vendor/Dell/sonicwall/#options","title":"Options","text":"Variable default description SC4S_DEST_DELL_SONICWALL-FIREWALL_SPLUNK_HEC_FMT JSON Restructure data from vendor format to json for splunk destinations set to \u201cNONE\u201d for native format SC4S_DEST_DELL_SONICWALL-FIREWALL_SYSLOG_FMT SDATA Restructure data from vendor format to SDATA for SYSLOG destinations set to \u201cNONE\u201d for native format"},{"location":"sources/vendor/Dell/sonicwall/#note","title":"Note:","text":"<p>The sourcetype has been changed in version 2.35.0 making it compliant with corresponding TA.</p>"},{"location":"sources/vendor/F5/bigip/","title":"BigIP","text":""},{"location":"sources/vendor/F5/bigip/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> <li>Needs host to be defined in log header similarly like in this issue.</li> </ul>"},{"location":"sources/vendor/F5/bigip/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2680/ Product Manual unknown"},{"location":"sources/vendor/F5/bigip/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes f5:bigip:syslog None f5:bigip:irule None f5:bigip:ltm:http:irule None f5:bigip:gtm:dns:request:irule None f5:bigip:gtm:dns:response:irule None f5:bigip:ltm:failed:irule None f5:bigip:asm:syslog None f5:bigip:apm:syslog None nix:syslog None f5:bigip:ltm:access_json User defined configuration via irule producing a RFC5424 syslog event with json content within the message field <code>&lt;111&gt;1 2020-05-28T22:48:15Z foo.example.com F5 - access_json - {\"event_type\":\"HTTP_REQUEST\", \"src_ip\":\"10.66.98.41\"}</code> This source type requires a customer specific Splunk Add-on for utility value"},{"location":"sources/vendor/F5/bigip/#index-configuration","title":"Index Configuration","text":"key index notes f5_bigip netops none f5_bigip_irule netops none f5_bigip_asm netwaf none f5_bigip_apm netops none f5_bigip_nix netops if <code>f_f5_bigip</code> is not set the index osnix will be used f5_bigip_access_json netops none"},{"location":"sources/vendor/F5/bigip/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-f5_bigip.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-f5_bigip[sc4s-vps] {\n filter { \n        \"${HOST}\" eq \"f5_bigip\"\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('f5')\n            product('bigip')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/FireEye/cms/","title":"CMS","text":""},{"location":"sources/vendor/FireEye/cms/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/FireEye/cms/#links","title":"Links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/"},{"location":"sources/vendor/FireEye/cms/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes fe_cef_syslog"},{"location":"sources/vendor/FireEye/cms/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes FireEye_CMS fe_cef_syslog fireeye"},{"location":"sources/vendor/FireEye/emps/","title":"eMPS","text":""},{"location":"sources/vendor/FireEye/emps/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/FireEye/emps/#links","title":"Links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/"},{"location":"sources/vendor/FireEye/emps/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes fe_cef_syslog"},{"location":"sources/vendor/FireEye/emps/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes FireEye_eMPS fe_cef_syslog fireeye"},{"location":"sources/vendor/FireEye/etp/","title":"etp","text":""},{"location":"sources/vendor/FireEye/etp/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/FireEye/etp/#links","title":"Links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/"},{"location":"sources/vendor/FireEye/etp/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes fe_etp source does not provide host name constant \u201cetp.fireeye.com\u201d is use regardless of region"},{"location":"sources/vendor/FireEye/etp/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes FireEye_ETP fe_etp fireeye"},{"location":"sources/vendor/FireEye/hx/","title":"hx","text":""},{"location":"sources/vendor/FireEye/hx/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/FireEye/hx/#links","title":"Links","text":"Ref Link Technology Add-On for FireEye https://splunkbase.splunk.com/app/1904/"},{"location":"sources/vendor/FireEye/hx/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes hx_cef_syslog"},{"location":"sources/vendor/FireEye/hx/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes fireeye_hx hx_cef_syslog fireeye"},{"location":"sources/vendor/Forcepoint/","title":"Email Security","text":""},{"location":"sources/vendor/Forcepoint/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Forcepoint/#links","title":"Links","text":"Ref Link Splunk Add-on none Product Manual none"},{"location":"sources/vendor/Forcepoint/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes forcepoint:email:kv None"},{"location":"sources/vendor/Forcepoint/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes forcepoint_email forcepoint:email:kv email none"},{"location":"sources/vendor/Forcepoint/webprotect/","title":"Webprotect (Websense)","text":""},{"location":"sources/vendor/Forcepoint/webprotect/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Forcepoint/webprotect/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2966/ Product Manual http://www.websense.com/content/support/library/web/v85/siem/siem.pdf"},{"location":"sources/vendor/Forcepoint/webprotect/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes websense:cg:kv None"},{"location":"sources/vendor/Forcepoint/webprotect/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes forcepoint_webprotect websense:cg:kv netproxy none forcepoint_ websense:cg:kv netproxy if the log is in format of  vendor=Forcepoint product= , the key will will be forcepoint_random"},{"location":"sources/vendor/Fortinet/fortimail/","title":"FortiWMail","text":""},{"location":"sources/vendor/Fortinet/fortimail/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Fortinet/fortimail/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3249"},{"location":"sources/vendor/Fortinet/fortimail/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes fml:&lt;type&gt; type value is determined from the message"},{"location":"sources/vendor/Fortinet/fortimail/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes fortinet_fortimail_&lt;type&gt; fml:&lt;type&gt; email type value is determined from the message"},{"location":"sources/vendor/Fortinet/fortios/","title":"Fortios","text":""},{"location":"sources/vendor/Fortinet/fortios/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Fortinet/fortios/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2846/ Product Manual https://docs.fortinet.com/product/fortigate/6.2"},{"location":"sources/vendor/Fortinet/fortios/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes fgt_log Catch-all sourcetype; not used by the TA fgt_traffic None fgt_utm None fgt_event None"},{"location":"sources/vendor/Fortinet/fortios/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes fortinet_fortios_traffic fgt_traffic netfw none fortinet_fortios_utm fgt_utm netfw none fortinet_fortios_event fgt_event netops none fortinet_fortios_log fgt_log netops none"},{"location":"sources/vendor/Fortinet/fortios/#source-setup-and-configuration","title":"Source Setup and Configuration","text":"<ul> <li>Refer to the admin manual for specific details of configuration to send Reliable syslog using RFC 3195 format, a typical logging configuration will include the following features.</li> </ul> <pre><code>config log memory filter\n\nset forward-traffic enable\n\nset local-traffic enable\n\nset sniffer-traffic disable\n\nset anomaly enable\n\nset voip disable\n\nset multicast-traffic enable\n\nset dns enable\n\nend\n\nconfig system global\n\nset cli-audit-log enable\n\nend\n\nconfig log setting\n\nset neighbor-event enable\n\nend\n</code></pre>"},{"location":"sources/vendor/Fortinet/fortios/#options","title":"Options","text":"Variable default description SC4S_OPTION_FORTINET_SOURCETYPE_PREFIX fgt Notice starting with version 1.6 of the fortinet add-on and app the sourcetype required changes from <code>fgt_*</code> to <code>fortinet_*</code> this is a breaking change to use the new sourcetype set this variable to <code>fortigate</code> in the env_file"},{"location":"sources/vendor/Fortinet/fortiweb/","title":"FortiWeb","text":""},{"location":"sources/vendor/Fortinet/fortiweb/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Fortinet/fortiweb/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4679/ Product Manual https://docs.fortinet.com/product/fortiweb/6.3"},{"location":"sources/vendor/Fortinet/fortiweb/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes fgt_log Catch-all sourcetype; not used by the TA fwb_traffic None fwb_attack None fwb_event None"},{"location":"sources/vendor/Fortinet/fortiweb/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes fortinet_fortiweb_traffic fwb_traffic netfw none fortinet_fortiweb_attack fwb_attack netids none fortinet_fortiweb_event fwb_event netops none fortinet_fortiweb_log fwb_log netops none"},{"location":"sources/vendor/Fortinet/fortiweb/#source-setup-and-configuration","title":"Source Setup and Configuration","text":"<ul> <li>Refer to the admin manual for specific details of configuration to send Reliable syslog using RFC 3195 format, a typical logging configuration will include the following features.</li> </ul> <pre><code>config log syslog-policy\n\nedit splunk  \n\nconfig syslog-server-list \n\nedit 1\n\nset server x.x.x.x\n\nset port 514 (Example. Should be the same as default or dedicated port selected for sc4s)   \n\nend\n\nend\n\nconfig log syslogd\n\nset policy splunk\n\nset status enable\n\nend\n</code></pre>"},{"location":"sources/vendor/GitHub/","title":"Enterprise Server","text":""},{"location":"sources/vendor/GitHub/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/GitHub/#links","title":"Links","text":"Ref Link Splunk Add-on Product Manual"},{"location":"sources/vendor/GitHub/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes github:enterprise:audit The audit logs of GitHub Enterprise server have information about audites actions performed by github user."},{"location":"sources/vendor/GitHub/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes github_ent github:enterprise:audit gitops None"},{"location":"sources/vendor/HAProxy/syslog/","title":"HAProxy","text":""},{"location":"sources/vendor/HAProxy/syslog/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/HAProxy/syslog/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3135/"},{"location":"sources/vendor/HAProxy/syslog/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes haproxy:tcp Default syslog format haproxy:splunk:http Splunk\u2019s documented custom format. Note: detection is based on <code>client_ip</code> prefix in message"},{"location":"sources/vendor/HAProxy/syslog/#index-configuration","title":"Index Configuration","text":"key index notes haproxy_syslog netlb none"},{"location":"sources/vendor/HPe/ilo/","title":"ILO (4+)","text":""},{"location":"sources/vendor/HPe/ilo/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/HPe/ilo/#links","title":"Links","text":""},{"location":"sources/vendor/HPe/ilo/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes hpe:ilo none"},{"location":"sources/vendor/HPe/ilo/#index-configuration","title":"Index Configuration","text":"key index notes hpe_ilo infraops none"},{"location":"sources/vendor/HPe/jedirect/","title":"Jedirect","text":""},{"location":"sources/vendor/HPe/jedirect/#jetdirect","title":"JetDirect","text":""},{"location":"sources/vendor/HPe/jedirect/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/HPe/jedirect/#links","title":"Links","text":"Ref Link"},{"location":"sources/vendor/HPe/jedirect/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes hpe:jetdirect none"},{"location":"sources/vendor/HPe/jedirect/#index-configuration","title":"Index Configuration","text":"key index notes hpe_jetdirect print none"},{"location":"sources/vendor/HPe/procurve/","title":"Procurve Switch","text":"<p>HP Procurve switches have multiple log formats used.</p>"},{"location":"sources/vendor/HPe/procurve/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/HPe/procurve/#links","title":"Links","text":"Ref Link Switch https://support.hpe.com/hpesc/public/docDisplay?docId=a00091844en_us Switch (A Series) (Flex) https://techhub.hpe.com/eginfolib/networking/docs/switches/12500/5998-4870_nmm_cg/content/378584395.htm"},{"location":"sources/vendor/HPe/procurve/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes hpe:procurve none"},{"location":"sources/vendor/HPe/procurve/#index-configuration","title":"Index Configuration","text":"key index notes hpe_procurve netops none"},{"location":"sources/vendor/IBM/datapower/","title":"Data power","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4662/"},{"location":"sources/vendor/IBM/datapower/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes ibm:datapower:syslog Common sourcetype ibm:datapower:* * is taken from the event sourcetype"},{"location":"sources/vendor/IBM/datapower/#index-configuration","title":"Index Configuration","text":"key source index notes ibm_datapower na inifraops none"},{"location":"sources/vendor/IBM/datapower/#parser-configuration","title":"Parser Configuration","text":"<p>Parser configuration is conditional only required if additional events are produced by the device that do not match the default configuration.</p> <pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-ibm_datapower.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-ibm_datapower[sc4s-vps] {\n filter { \n        host(\"^test-ibmdp-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('ibm')\n            product('datapower')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/ISC/bind/","title":"bind","text":"<p>This source type is often re-implemented by specific add-ons such as infoblox or bluecat if a more specific source type is desired see that source documentation for instructions</p>"},{"location":"sources/vendor/ISC/bind/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/ISC/bind/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2876/"},{"location":"sources/vendor/ISC/bind/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes isc:bind none"},{"location":"sources/vendor/ISC/bind/#index-configuration","title":"Index Configuration","text":"key index notes isc_bind isc:bind none"},{"location":"sources/vendor/ISC/dhcpd/","title":"dhcpd","text":"<p>This source type is often re-implemented by specific add-ons such as infoblox or bluecat if a more specific source type is desired see that source documentation for instructions</p>"},{"location":"sources/vendor/ISC/dhcpd/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/ISC/dhcpd/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3010/"},{"location":"sources/vendor/ISC/dhcpd/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes isc:dhcp none"},{"location":"sources/vendor/ISC/dhcpd/#index-configuration","title":"Index Configuration","text":"key index notes isc_dhcp isc:dhcp none"},{"location":"sources/vendor/ISC/dhcpd/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/vendor/ISC/dhcpd/#options","title":"Options","text":"<p>None</p>"},{"location":"sources/vendor/ISC/dhcpd/#verification","title":"Verification","text":"<p>An active site will generate frequent events use the following search to check for new events</p> <p>Verify timestamp, and host values match as expected</p> <pre><code>index=&lt;asconfigured&gt; (sourcetype=isc:dhcp\")\n</code></pre>"},{"location":"sources/vendor/Imperva/incapusla/","title":"Incapsula","text":""},{"location":"sources/vendor/Imperva/incapusla/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Imperva/incapusla/#links","title":"Links","text":"Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on Source Specific https://bitbucket.org/SPLServices/ta-cef-imperva-incapsula/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm"},{"location":"sources/vendor/Imperva/incapusla/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cef Common sourcetype"},{"location":"sources/vendor/Imperva/incapusla/#source","title":"Source","text":"sourcetype notes Imperva:Incapsula Common sourcetype"},{"location":"sources/vendor/Imperva/incapusla/#index-configuration","title":"Index Configuration","text":"key source index notes Incapsula_SIEMintegration Imperva:Incapsula netwaf none"},{"location":"sources/vendor/Imperva/waf/","title":"On-Premises WAF (SecureSphere WAF)","text":""},{"location":"sources/vendor/Imperva/waf/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Imperva/waf/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2874/ Product Manual https://community.microfocus.com/dcvta86296/attachments/dcvta86296/partner-documentation-h-o/22/2/Imperva_SecureSphere_11_5_CEF_Config_Guide_2018.pdf"},{"location":"sources/vendor/Imperva/waf/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes imperva:waf none imperva:waf:firewall:cef none imperva:waf:security:cef none"},{"location":"sources/vendor/Imperva/waf/#index-configuration","title":"Index Configuration","text":"key index notes Imperva Inc._SecureSphere netwaf none"},{"location":"sources/vendor/InfoBlox/","title":"NIOS","text":"<p>Warning: Despite the TA indication this data source is CIM compliant all versions of NIOS including the most recent available as of 2019-12-17 do not support the DNS data model correctly. For DNS security use cases use Splunk Stream instead.</p>"},{"location":"sources/vendor/InfoBlox/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/InfoBlox/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2934/ Product Manual https://docs.infoblox.com/display/ILP/NIOS?preview=/8945695/43728387/NIOS_8.4_Admin_Guide.pdf"},{"location":"sources/vendor/InfoBlox/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes infoblox:dns None infoblox:dhcp None infoblox:threat None nix:syslog None"},{"location":"sources/vendor/InfoBlox/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes infoblox_nios_dns infoblox:dns netdns none infoblox_nios_dhcp infoblox:dhcp netipam none infoblox_nios_threat infoblox:threatprotect netids none infoblox_nios_audit infoblox:audit netops none infoblox_nios_fallback infoblox:port netops none"},{"location":"sources/vendor/InfoBlox/#options","title":"Options","text":"Variable default description SC4S_LISTEN_INFOBLOX_NIOS_UDP_PORT empty Vendor specific port SC4S_LISTEN_INFOBLOX_NIOS_TCP_PORT empty Vendor specific port"},{"location":"sources/vendor/InfoBlox/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-infoblox_nios.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-infoblox_nios[sc4s-vps] {\n filter { \n        host(\"infoblox-*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('infoblox')\n            product('nios')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Juniper/junos/","title":"JunOS","text":""},{"location":"sources/vendor/Juniper/junos/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Juniper/junos/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2847/ JunOS TechLibrary https://www.juniper.net/documentation/en_US/junos/topics/example/syslog-messages-configuring-qfx-series.html"},{"location":"sources/vendor/Juniper/junos/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes juniper:junos:firewall None juniper:junos:firewall:structured None juniper:junos:idp None juniper:junos:idp:structured None juniper:junos:aamw:structured None juniper:junos:secintel:structured None juniper:junos:snmp None"},{"location":"sources/vendor/Juniper/junos/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes juniper_junos_legacy juniper:legacy netops none juniper_junos_flow juniper:junos:firewall netfw none juniper_junos_utm juniper:junos:firewall netfw none juniper_junos_firewall juniper:junos:firewall netfw none juniper_junos_ids juniper:junos:firewall netids none juniper_junos_idp juniper:junos:idp netids none juniper_junos_snmp juniper:junos:snmp netops none juniper_junos_structured_fw juniper:junos:firewall:structured netfw none juniper_junos_structured_ids juniper:junos:firewall:structured netids none juniper_junos_structured_utm juniper:junos:firewall:structured netfw none juniper_junos_structured_idp juniper:junos:idp:structured netids none juniper_junos_structured_aamw juniper:junos:aamw:structured netfw none juniper_junos_structured_secintel juniper:junos:secintel:structured netfw none"},{"location":"sources/vendor/Juniper/netscreen/","title":"Netscreen","text":""},{"location":"sources/vendor/Juniper/netscreen/#netscreen","title":"Netscreen","text":""},{"location":"sources/vendor/Juniper/netscreen/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Juniper/netscreen/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2847/ Netscreen Manual http://kb.juniper.net/InfoCenter/index?page=content&amp;id=KB4759"},{"location":"sources/vendor/Juniper/netscreen/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes netscreen:firewall None"},{"location":"sources/vendor/Juniper/netscreen/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes juniper_netscreen netscreen:firewall netfw none"},{"location":"sources/vendor/Kaspersky/es/","title":"Enterprise Security RFC5424","text":""},{"location":"sources/vendor/Kaspersky/es/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC5424</li> </ul>"},{"location":"sources/vendor/Kaspersky/es/#links","title":"Links","text":"Ref Link Splunk Add-on non"},{"location":"sources/vendor/Kaspersky/es/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes kaspersky:syslog:es Where PROGRAM starts with KES kaspersky:syslog None"},{"location":"sources/vendor/Kaspersky/es/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes kaspersky_syslog kaspersky:syslog epav none kaspersky_syslog_es kaspersky:syslog:es epav none"},{"location":"sources/vendor/Kaspersky/es_cef/","title":"Enterprise Security CEF","text":"<p>The TA link provided has commented out the CEF support as of 2022-03-18 manual edits are required</p>"},{"location":"sources/vendor/Kaspersky/es_cef/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC5424</li> </ul>"},{"location":"sources/vendor/Kaspersky/es_cef/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4656/"},{"location":"sources/vendor/Kaspersky/es_cef/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes kaspersky:cef kaspersky:klaud kaspersky:klsrv kaspersky:gnrl kaspersky:klnag kaspersky:klprci kaspersky:klbl"},{"location":"sources/vendor/Kaspersky/es_cef/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes KasperskyLab_SecurityCenter all epav none"},{"location":"sources/vendor/Kaspersky/es_leef/","title":"Enterprise Security Leef","text":"<p>Leef format has not been tested samples needed</p>"},{"location":"sources/vendor/Kaspersky/es_leef/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> </ul>"},{"location":"sources/vendor/Kaspersky/es_leef/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4656/"},{"location":"sources/vendor/Kaspersky/es_leef/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes kaspersky:cef kaspersky:klaud kaspersky:klsrv kaspersky:gnrl kaspersky:klnag kaspersky:klprci kaspersky:klbl"},{"location":"sources/vendor/Kaspersky/es_leef/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes KasperskyLab_SecurityCenter all epav none"},{"location":"sources/vendor/Liveaction/liveaction_livenx/","title":"Liveaction - livenx","text":""},{"location":"sources/vendor/Liveaction/liveaction_livenx/#key-facts","title":"Key facts","text":"<ul> <li>Default port 514</li> </ul>"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual None"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes liveaction:livenx none"},{"location":"sources/vendor/Liveaction/liveaction_livenx/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes liveaction_livenx liveaction:livenx netops None"},{"location":"sources/vendor/McAfee/epo/","title":"EPO","text":""},{"location":"sources/vendor/McAfee/epo/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Source requires use of TLS legacy BSD port 6514</li> <li>TLS Certificate must be trusted by EPO instance</li> </ul>"},{"location":"sources/vendor/McAfee/epo/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/5085/ Product Manual https://kc.mcafee.com/corporate/index?page=content&amp;id=KB87927"},{"location":"sources/vendor/McAfee/epo/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes mcafee:epo:syslog none"},{"location":"sources/vendor/McAfee/epo/#source","title":"Source","text":"source notes policy_auditor_vulnerability_assessment Policy Auditor Vulnerability Assessment events mcafee_agent McAfee Agent events mcafee_endpoint_security McAfee Endpoint Security events"},{"location":"sources/vendor/McAfee/epo/#index-configuration","title":"Index Configuration","text":"key index notes mcafee_epo epav none"},{"location":"sources/vendor/McAfee/epo/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/vendor/McAfee/epo/#options","title":"Options","text":"Variable default description SC4S_LISTEN_MCAFEE_EPO_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_DEST_MCAFEE_EPO_ARCHIVE no Enable archive to disk for this specific source SC4S_DEST_MCAFEE_EPO_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source SC4S_SOURCE_TLS_ENABLE no This must be set to yes so that SC4S listens for encrypted syslog from ePO"},{"location":"sources/vendor/McAfee/epo/#additional-setup","title":"Additional setup","text":"<p>You must create a certificate for the SC4S server to receive encrypted syslog from ePO. A self-signed certificate is fine. Generate a self-signed certificate on the SC4S host:</p> <p><code>openssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 -keyout /opt/sc4s/tls/server.key -out /opt/sc4s/tls/server.pem</code></p> <p>Uncomment the following line in <code>/lib/systemd/system/sc4s.service</code> to allow the docker container to use the certificate:</p> <p><code>Environment=\"SC4S_TLS_MOUNT=/opt/sc4s/tls:/etc/syslog-ng/tls:z\"</code></p>"},{"location":"sources/vendor/McAfee/epo/#troubleshooting","title":"Troubleshooting","text":"<p>from the command line of the SC4S host, run this: <code>openssl s_client -connect localhost:6514</code></p> <p>The message:</p> <pre><code>socket: Bad file descriptor\nconnect:errno=9\n</code></pre> <p>indicates that SC4S is not listening for encrypted syslog. Note that a <code>netstat</code> may show the port open, but it is not accepting encrypted traffic as configured.</p> <p>It may take several minutes for the syslog option to be available in the <code>registered servers</code> dropdown.</p>"},{"location":"sources/vendor/McAfee/nsp/","title":"Network Security Platform","text":""},{"location":"sources/vendor/McAfee/nsp/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/McAfee/nsp/#links","title":"Links","text":"Ref Link Product Manual https://docs.mcafee.com/bundle/network-security-platform-10.1.x-product-guide/page/GUID-373C1CA6-EC0E-49E1-8858-749D1AA2716A.html"},{"location":"sources/vendor/McAfee/nsp/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes mcafee:nsp none"},{"location":"sources/vendor/McAfee/nsp/#source","title":"Source","text":"source notes mcafee:nsp:alert Alert/Attack Events mcafee:nsp:audit Audit Event or User Activity Events mcafee:nsp:fault Fault Events mcafee:nsp:firewall Firewall Events"},{"location":"sources/vendor/McAfee/nsp/#index-configuration","title":"Index Configuration","text":"key index notes mcafee_nsp netids none"},{"location":"sources/vendor/McAfee/wg/","title":"Wg","text":""},{"location":"sources/vendor/McAfee/wg/#web-gateway","title":"Web Gateway","text":""},{"location":"sources/vendor/McAfee/wg/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/McAfee/wg/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3009/ Product Manual https://kc.mcafee.com/corporate/index?page=content&amp;id=KB77988&amp;actp=RSS"},{"location":"sources/vendor/McAfee/wg/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes mcafee:wg:kv none"},{"location":"sources/vendor/McAfee/wg/#index-configuration","title":"Index Configuration","text":"key index notes mcafee_wg netproxy none"},{"location":"sources/vendor/Microfocus/arcsight/","title":"Arcsight Internal Agent","text":""},{"location":"sources/vendor/Microfocus/arcsight/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Microfocus/arcsight/#links","title":"Links","text":"Ref Link Splunk Add-on CEF https://github.com/splunk/splunk-add-on-for-cef/downloads/"},{"location":"sources/vendor/Microfocus/arcsight/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cef Common sourcetype"},{"location":"sources/vendor/Microfocus/arcsight/#source","title":"Source","text":"source notes ArcSight:ArcSight Internal logs"},{"location":"sources/vendor/Microfocus/arcsight/#index-configuration","title":"Index Configuration","text":"key source index notes ArcSight_ArcSight ArcSight:ArcSight main none"},{"location":"sources/vendor/Microfocus/windows/","title":"Arcsight Microsoft Windows (CEF)","text":""},{"location":"sources/vendor/Microfocus/windows/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Microfocus/windows/#links","title":"Links","text":"Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-microsoft-windows-for-splunk/downloads/ Product Manual https://docs.imperva.com/bundle/cloud-application-security/page/more/log-configuration.htm"},{"location":"sources/vendor/Microfocus/windows/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cef Common sourcetype"},{"location":"sources/vendor/Microfocus/windows/#source","title":"Source","text":"source notes CEFEventLog:System or Application Event Windows Application and System Event Logs CEFEventLog:Microsoft Windows Windows Security Event Logs"},{"location":"sources/vendor/Microfocus/windows/#index-configuration","title":"Index Configuration","text":"key source index notes Microsoft_System or Application Event CEFEventLog:System or Application Event oswin none Microsoft_Microsoft Windows CEFEventLog:Microsoft Windows oswinsec none"},{"location":"sources/vendor/Microsoft/","title":"Cloud App Security (MCAS)","text":""},{"location":"sources/vendor/Microsoft/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Microsoft/#links","title":"Links","text":"Ref Link Splunk Add-on CEF https://bitbucket.org/SPLServices/ta-cef-for-splunk/downloads/ Splunk Add-on Source Specific none Product Manual https://docs.microsoft.com/en-us/cloud-app-security/siem"},{"location":"sources/vendor/Microsoft/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cef Common sourcetype"},{"location":"sources/vendor/Microsoft/#source","title":"Source","text":"source notes microsoft:cas Common sourcetype"},{"location":"sources/vendor/Microsoft/#index-configuration","title":"Index Configuration","text":"key source index notes MCAS_SIEM_Agent microsoft:cas main none"},{"location":"sources/vendor/Mikrotik/routeros/","title":"RouterOS","text":""},{"location":"sources/vendor/Mikrotik/routeros/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> <li>RouterOS will send ISC Bind and ISC DHCPD events</li> </ul>"},{"location":"sources/vendor/Mikrotik/routeros/#links","title":"Links","text":""},{"location":"sources/vendor/Mikrotik/routeros/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes routeros none"},{"location":"sources/vendor/Mikrotik/routeros/#index-configuration","title":"Index Configuration","text":"key index notes mikrotik_routeros netops none mikrotik_routeros_fw netfw Used for events with forward:"},{"location":"sources/vendor/Mikrotik/routeros/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-mikrotik_routeros.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-mikrotik_routeros[sc4s-vps] {\n filter { \n        host(\"test-mrtros-\" type(string) flags(prefix))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('mikrotik')\n            product('routeros')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/NetApp/ontap/","title":"OnTap","text":""},{"location":"sources/vendor/NetApp/ontap/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/NetApp/ontap/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3418/ Product Manual unknown"},{"location":"sources/vendor/NetApp/ontap/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes netapp:ems None"},{"location":"sources/vendor/NetApp/ontap/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes netapp_ontap netapp:ems infraops none"},{"location":"sources/vendor/NetApp/storage-grid/","title":"StorageGRID","text":""},{"location":"sources/vendor/NetApp/storage-grid/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> <li>Community requested parser</li> </ul>"},{"location":"sources/vendor/NetApp/storage-grid/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3895/ Product Manual unknown"},{"location":"sources/vendor/NetApp/storage-grid/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes grid:auditlog None grid:rest:api None"},{"location":"sources/vendor/NetApp/storage-grid/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes netapp_grid grid:auditlog infraops none netapp_grid grid:rest:api infraops none"},{"location":"sources/vendor/NetScout/arbor_edge/","title":"DatAdvantage","text":""},{"location":"sources/vendor/NetScout/arbor_edge/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/NetScout/arbor_edge/#links","title":"Links","text":"Ref Link TA https://github.com/arbor/TA_netscout_aed"},{"location":"sources/vendor/NetScout/arbor_edge/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes netscout:aed"},{"location":"sources/vendor/NetScout/arbor_edge/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes NETSCOUT_Arbor Edge Defense netscout:aed netids NETSCOUT_Arbor Networks APS netscout:aed netids"},{"location":"sources/vendor/Netmotion/mobilityserver/","title":"Mobility Server","text":""},{"location":"sources/vendor/Netmotion/mobilityserver/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> </ul>"},{"location":"sources/vendor/Netmotion/mobilityserver/#links","title":"Links","text":"Ref Link Splunk Add-on none Product Manual unknown"},{"location":"sources/vendor/Netmotion/mobilityserver/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes netmotion:mobilityserver:* The third segment of the source type is constructed from the sdid field of the syslog sdata"},{"location":"sources/vendor/Netmotion/mobilityserver/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes netmotion_mobility-server_* netmotion:mobilityserver:* netops none"},{"location":"sources/vendor/Netmotion/reporting/","title":"Reporting","text":""},{"location":"sources/vendor/Netmotion/reporting/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Netmotion/reporting/#links","title":"Links","text":"Ref Link Splunk Add-on none Product Manual unknown"},{"location":"sources/vendor/Netmotion/reporting/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes netmotion:reporting None"},{"location":"sources/vendor/Netmotion/reporting/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes netmotion_reporting netmotion:reporting netops none"},{"location":"sources/vendor/Netwrix/endpoint_protector/","title":"Endpoint Protector by CoSoSys","text":""},{"location":"sources/vendor/Netwrix/endpoint_protector/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC5424 default port 514</li> <li>IETF Framed syslog must use port 601</li> </ul>"},{"location":"sources/vendor/Netwrix/endpoint_protector/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual na"},{"location":"sources/vendor/Netwrix/endpoint_protector/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes netwrix:epp None"},{"location":"sources/vendor/Netwrix/endpoint_protector/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes netwrix_epp netwrix:epp netops None"},{"location":"sources/vendor/Novell/netiq/","title":"NetIQ","text":""},{"location":"sources/vendor/Novell/netiq/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Novell/netiq/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Novell/netiq/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes novell:netiq none"},{"location":"sources/vendor/Novell/netiq/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes novell_netiq novell_netiq netauth None"},{"location":"sources/vendor/Nutanix/cvm/","title":"Nutanix_CVM_Audit","text":""},{"location":"sources/vendor/Nutanix/cvm/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Community requested filter</li> <li>Only CVM log supported</li> </ul>"},{"location":"sources/vendor/Nutanix/cvm/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Nutanix/cvm/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes nutanix:syslog CVM logs nutanix:syslog:audit CVM system audit logs   Considering the message host format is default ntnx-xxxx-cvm"},{"location":"sources/vendor/Nutanix/cvm/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes nutanix_syslog nutanix:syslog infraops none nutanix_syslog_audit nutanix:syslog:audit infraops none"},{"location":"sources/vendor/Ossec/ossec/","title":"Ossec","text":""},{"location":"sources/vendor/Ossec/ossec/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Ossec/ossec/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2808/ Product Manual https://www.ossec.net/docs/index.html"},{"location":"sources/vendor/Ossec/ossec/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes ossec The add-on supports data from the following sources: File Integrity Management (FIM) data, FTP data, su data, ssh data, Windows data, including audit and logon information"},{"location":"sources/vendor/Ossec/ossec/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes ossec_ossec ossec main None"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/","title":"Cortext","text":""},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Cortex requires TLS and uses IETF Framed SYSLOG default port is 6587</li> </ul> Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes pan:* pan:xsoar none"},{"location":"sources/vendor/PaloaltoNetworks/cortexxdr/#index-configuration","title":"Index Configuration","text":"key index notes Palo Alto Networks_Palo Alto Networks Cortex XSOAR epintel none"},{"location":"sources/vendor/PaloaltoNetworks/panos/","title":"panos","text":""},{"location":"sources/vendor/PaloaltoNetworks/panos/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter from NGFW, PANORAMA OR CORTEX data lake</li> <li>Legacy BSD Format default port 514 used by default. \u201cDefault TCP/UDP\u201d is 30% slower than preferred IETF Framed</li> <li>IMPORTANT IETF Framed syslog must use port 601</li> </ul>"},{"location":"sources/vendor/PaloaltoNetworks/panos/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/ Product Manual https://docs.paloaltonetworks.com/pan-os/9-0/pan-os-admin/monitoring/use-syslog-for-monitoring/configure-syslog-monitoring.html"},{"location":"sources/vendor/PaloaltoNetworks/panos/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes pan:log None pan:globalprotect none pan:traffic None pan:threat None pan:system None pan:config None pan:hipmatch None pan:correlation None pan:userid None"},{"location":"sources/vendor/PaloaltoNetworks/panos/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes pan_panos_log pan:log netops none pan_panos_globalprotect pan:globalprotect netfw none pan_panos_traffic pan:traffic netfw none pan_panos_threat pan:threat netproxy none pan_panos_system pan:system netops none pan_panos_config pan:config netops none pan_panos_hipmatch pan:hipmatch netops none pan_panos_correlation pan:correlation netops none pan_panos_userid pan:userid netauth none"},{"location":"sources/vendor/PaloaltoNetworks/panos/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/vendor/PaloaltoNetworks/panos/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer.</li> <li>Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source.</li> <li>Refer to the admin manual for specific details of configuration</li> <li>Select TCP or SSL transport option</li> <li>Select IETF Format</li> <li>Ensure the format of the event is not customized</li> </ul>"},{"location":"sources/vendor/PaloaltoNetworks/panos/#options","title":"Options","text":"Variable default description SC4S_LISTEN_PULSE_PAN_PANOS_RFC6587_PORT empty string Enable a TCP using IETF Framing (RFC6587) port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_PAN_PANOS_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_DEST_PAN_PANOS_ARCHIVE no Enable archive to disk for this specific source SC4S_DEST_PAN_PANOS_HEC no When Splunk HEC is disabled globally set to yes to enable this specific source"},{"location":"sources/vendor/PaloaltoNetworks/panos/#verification","title":"Verification","text":"<p>An active firewall will generate frequent events. Use the following search to validate events are present per source device</p> <pre><code>index=&lt;asconfigured&gt; sourcetype=pan:*| stats count by host\n</code></pre>"},{"location":"sources/vendor/PaloaltoNetworks/prisma/","title":"Prisma SD-WAN ION","text":""},{"location":"sources/vendor/PaloaltoNetworks/prisma/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> </ul> Ref Link Splunk Add-on none Product Manual https://docs.paloaltonetworks.com/prisma/prisma-sd-wan/prisma-sd-wan-admin/prisma-sd-wan-sites-and-devices/use-external-services-for-monitoring/syslog-server-support-in-prisma-sd-wan Product Manual https://docs.paloaltonetworks.com/prisma/prisma-sd-wan/prisma-sd-wan-admin/prisma-sd-wan-sites-and-devices/use-external-services-for-monitoring/syslog-server-support-in-prisma-sd-wan/syslog-flow-export"},{"location":"sources/vendor/PaloaltoNetworks/prisma/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes prisma:sd-wan:flow prisma:sd-wan:authentication prisma:sd-wan:event"},{"location":"sources/vendor/PaloaltoNetworks/prisma/#index-configuration","title":"Index Configuration","text":"key index notes prisma_sd-wan_flow netwaf none prisma_sd-wan_authentication netwaf none prisma_sd-wan_event netwaf none"},{"location":"sources/vendor/PaloaltoNetworks/traps/","title":"Traps","text":""},{"location":"sources/vendor/PaloaltoNetworks/traps/#traps","title":"TRAPS","text":""},{"location":"sources/vendor/PaloaltoNetworks/traps/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/PaloaltoNetworks/traps/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/2757/"},{"location":"sources/vendor/PaloaltoNetworks/traps/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes pan:traps4 none"},{"location":"sources/vendor/PaloaltoNetworks/traps/#index-configuration","title":"Index Configuration","text":"key index notes Palo Alto Networks_Traps Agent epintel none"},{"location":"sources/vendor/Pfsense/firewall/","title":"Firewall","text":""},{"location":"sources/vendor/Pfsense/firewall/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Pfsense/firewall/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/1527/ Product Manual https://docs.netgate.com/pfsense/en/latest/monitoring/copying-logs-to-a-remote-host-with-syslog.html?highlight=syslog"},{"location":"sources/vendor/Pfsense/firewall/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes pfsense:filterlog None pfsense:* All programs other than filterlog"},{"location":"sources/vendor/Pfsense/firewall/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes pfsense pfsense netops none pfsense_filterlog pfsense:filterlog netfw none"},{"location":"sources/vendor/Pfsense/firewall/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-pfsense_firewall.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-pfsense_firewall[sc4s-vps] {\n filter { \n        \"${HOST}\" eq \"pfsense_firewall\"\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('pfsense')\n            product('firewall')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Polycom/rprm/","title":"RPRM","text":""},{"location":"sources/vendor/Polycom/rprm/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Polycom/rprm/#links","title":"Links","text":"Ref Link Splunk Add-on none Product Manual unknown"},{"location":"sources/vendor/Polycom/rprm/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes polycom:rprm:syslog"},{"location":"sources/vendor/Polycom/rprm/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes polycom_rprm polycom:rprm:syslog netops none"},{"location":"sources/vendor/Powertech/interact/","title":"PowerTech Interact","text":""},{"location":"sources/vendor/Powertech/interact/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>CEF Format default port 514</li> </ul>"},{"location":"sources/vendor/Powertech/interact/#links","title":"Links","text":"Ref Link Splunk Add-on None"},{"location":"sources/vendor/Powertech/interact/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes PowerTech:Interact:cef CEF"},{"location":"sources/vendor/Powertech/interact/#source","title":"Source","text":"source notes PowerTech:Interact:cef None"},{"location":"sources/vendor/Powertech/interact/#index-configuration","title":"Index Configuration","text":"key source index notes PowerTech_Interact PowerTech:Interact netops none"},{"location":"sources/vendor/Proofpoint/","title":"Proofpoint Protection Server","text":""},{"location":"sources/vendor/Proofpoint/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> <li>NOTE:  This filter will simply parse the syslog message itself, and will not perform the (required) re-assembly of related messages to create meaningful final output.  This will require follow-on processing in Splunk.</li> </ul>"},{"location":"sources/vendor/Proofpoint/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3080/ Product Manual https://proofpointcommunities.force.com/community/s/article/Remote-Syslog-Forwarding"},{"location":"sources/vendor/Proofpoint/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes pps_filter_log pps_mail_log This sourcetype will conflict with sendmail itself, so will require that the PPS send syslog on a dedicated port or be uniquely identifiable with a hostname glob or CIDR block if this sourcetype is desired for PPS."},{"location":"sources/vendor/Proofpoint/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes proofpoint_pps_filter pps_filter_log email none proofpoint_pps_sendmail pps_mail_log email none"},{"location":"sources/vendor/Proofpoint/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-proofpoint_pps.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-proofpoint_pps[sc4s-vps] {\n filter { \n        host(\"pps-*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('proofpoint')\n            product('pps')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Pulse/connectsecure/","title":"Pulse","text":""},{"location":"sources/vendor/Pulse/connectsecure/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>IETF Frames use port 601/tcp or 6587/TLS</li> </ul>"},{"location":"sources/vendor/Pulse/connectsecure/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3852/ JunOS TechLibrary https://docs.pulsesecure.net/WebHelp/Content/PCS/PCS_AdminGuide_8.2/Configuring%20Syslog.htm"},{"location":"sources/vendor/Pulse/connectsecure/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes pulse:connectsecure None pulse:connectsecure:web None"},{"location":"sources/vendor/Pulse/connectsecure/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes pulse_connect_secure pulse:connectsecure netfw none pulse_connect_secure_web pulse:connectsecure:web netproxy none"},{"location":"sources/vendor/PureStorage/array/","title":"Array","text":""},{"location":"sources/vendor/PureStorage/array/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/PureStorage/array/#links","title":"Links","text":"Ref Link Splunk Add-on None  note TA published on Splunk base does not include syslog extractions Product Manual"},{"location":"sources/vendor/PureStorage/array/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes purestorage:array purestorage:array:${class} This type is generated from the message"},{"location":"sources/vendor/PureStorage/array/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes purestorage_array purestorage:array infraops None purestorage_array_${class} purestorage:array:class infraops class is extracted as the string following \u201cpurity.\u201d"},{"location":"sources/vendor/Qumulo/storage/","title":"Storage","text":""},{"location":"sources/vendor/Qumulo/storage/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Qumulo/storage/#links","title":"Links","text":"Ref Link Splunk Add-on none"},{"location":"sources/vendor/Qumulo/storage/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes qumulo:storage None"},{"location":"sources/vendor/Qumulo/storage/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes qumulo_storage qumulo:storage infraops none"},{"location":"sources/vendor/Radware/defensepro/","title":"DefensePro","text":""},{"location":"sources/vendor/Radware/defensepro/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Radware/defensepro/#links","title":"Links","text":"Ref Link Splunk Add-on Note this add-on does not provide functional extractions https://splunkbase.splunk.com/app/4480/ Product Manual https://www.radware.com/products/defensepro/"},{"location":"sources/vendor/Radware/defensepro/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes radware:defensepro Note some events do not contain host"},{"location":"sources/vendor/Radware/defensepro/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes radware_defensepro radware:defensepro netops none"},{"location":"sources/vendor/Raritan/dsx/","title":"DSX","text":""},{"location":"sources/vendor/Raritan/dsx/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Raritan/dsx/#links","title":"Links","text":"Ref Link Splunk Add-on none Product Manual https://www.raritan.com/products/kvm-serial/serial-console-servers/serial-over-ip-console-server"},{"location":"sources/vendor/Raritan/dsx/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes raritan:dsx Note events do not contain host"},{"location":"sources/vendor/Raritan/dsx/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes raritan_dsx raritan:dsx infraops none"},{"location":"sources/vendor/Raritan/dsx/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-raritan_dsx.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-raritan_dsx[sc4s-vps] {\n filter { \n        host(\"raritan_dsx*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('raritan')\n            product('dsx')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Ricoh/mfp/","title":"MFP","text":""},{"location":"sources/vendor/Ricoh/mfp/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Ricoh/mfp/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Ricoh/mfp/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes ricoh:mfp None"},{"location":"sources/vendor/Ricoh/mfp/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes ricoh_syslog ricoh:mfp printer none"},{"location":"sources/vendor/Ricoh/mfp/#sc4s-options","title":"SC4S Options","text":"Variable default description SC4S_SOURCE_RICOH_SYSLOG_FIXHOST yes Current firmware incorrectly sends the value of HOST in the program field if this is ever corrected this value will need to be set back to no we suggest using yes"},{"location":"sources/vendor/Riverbed/","title":"Syslog","text":"<p>Used when more specific steelhead or steelconnect can not be identified</p>"},{"location":"sources/vendor/Riverbed/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC5424 or Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Riverbed/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Riverbed/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes riverbed:syslog None"},{"location":"sources/vendor/Riverbed/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes riverbed_syslog riverbed:syslog netops none riverbed_syslog_nix_syslog nix:syslog osnix none"},{"location":"sources/vendor/Riverbed/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-riverbed_syslog.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-riverbed_syslog[sc4s-vps] {\n filter {      \n        host(....)\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('riverbed')\n            product('syslog')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Riverbed/steelconnect/","title":"Steelconnect","text":""},{"location":"sources/vendor/Riverbed/steelconnect/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC5424 or Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Riverbed/steelconnect/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Riverbed/steelconnect/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes riverbed:steelconnect None"},{"location":"sources/vendor/Riverbed/steelconnect/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes riverbed_syslog_steelconnect riverbed:steelconnect netops none"},{"location":"sources/vendor/Riverbed/steelhead/","title":"SteelHead","text":""},{"location":"sources/vendor/Riverbed/steelhead/#key-facts","title":"Key facts","text":"<ul> <li>Partial MSG Format based filter</li> <li>RFC5424 or Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Riverbed/steelhead/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Riverbed/steelhead/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes riverbed:steelhead None"},{"location":"sources/vendor/Riverbed/steelhead/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes riverbed_syslog_steelhead riverbed:steelhead netops none"},{"location":"sources/vendor/Riverbed/steelhead/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-riverbed_syslog.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-riverbed_syslog[sc4s-vps] {\n filter {      \n        host(....)\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('riverbed')\n            product('syslog')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Ruckus/SmartZone/","title":"Smart Zone","text":"<p>Some events may not match the source format please report issues if found</p>"},{"location":"sources/vendor/Ruckus/SmartZone/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Ruckus/SmartZone/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Ruckus/SmartZone/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes ruckus:smartzone None"},{"location":"sources/vendor/Ruckus/SmartZone/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes ruckus_smartzone ruckus:smartzone netops none"},{"location":"sources/vendor/Schneider/apc/","title":"APC Power systems","text":""},{"location":"sources/vendor/Schneider/apc/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Schneider/apc/#links","title":"Links","text":"Ref Link Splunk Add-on none Product Manual multiple"},{"location":"sources/vendor/Schneider/apc/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes apc:syslog None"},{"location":"sources/vendor/Schneider/apc/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes schneider_apc apc:syslog main none"},{"location":"sources/vendor/Schneider/apc/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-schneider_apc.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-schneider_apc[sc4s-vps] {\n filter { \n        host(\"test_apc-*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('schneider')\n            product('apc')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/SecureAuthIdP/secureauth_idp/","title":"SecureAuth IdP","text":""},{"location":"sources/vendor/SecureAuthIdP/secureauth_idp/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC 5424 Framed</li> </ul>"},{"location":"sources/vendor/SecureAuthIdP/secureauth_idp/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3008"},{"location":"sources/vendor/SecureAuthIdP/secureauth_idp/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes secureauth:idp none"},{"location":"sources/vendor/SecureAuthIdP/secureauth_idp/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes secureauth_idp secureauth:idp netops None"},{"location":"sources/vendor/Semperis/DSP/","title":"Semperis DSP","text":""},{"location":"sources/vendor/Semperis/DSP/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> </ul>"},{"location":"sources/vendor/Semperis/DSP/#links","title":"Links","text":"Ref Link Splunk Add-on None"},{"location":"sources/vendor/Semperis/DSP/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes semperis:dsp none"},{"location":"sources/vendor/Semperis/DSP/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes semperis_dsp semperis:dsp netops None"},{"location":"sources/vendor/Solace/evenbroker/","title":"EventBroker","text":""},{"location":"sources/vendor/Solace/evenbroker/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Solace/evenbroker/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Solace/evenbroker/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes solace:eventbroker None"},{"location":"sources/vendor/Solace/evenbroker/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes solace_eventbroker solace:eventbroker main none"},{"location":"sources/vendor/Sophos/Firewall/","title":"Web Appliance","text":""},{"location":"sources/vendor/Sophos/Firewall/#key-facts","title":"Key facts","text":"<ul> <li>Community requested filter</li> <li>Default port 514</li> </ul>"},{"location":"sources/vendor/Sophos/Firewall/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/6187/ Product Manual unknown"},{"location":"sources/vendor/Sophos/Firewall/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes sophos:xg:atp None sophos:xg:anti_spam None sophos:xg:anti_virus None sophos:xg:content_filtering None sophos:xg:event None sophos:xg:firewall None sophos:xg:ssl None sophos:xg:sandbox None sophos:xg:system_health None sophos:xg:heartbeat None sophos:xg:waf None sophos:xg:wireless_protection None sophos:xg:idp None"},{"location":"sources/vendor/Sophos/Firewall/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes sophos_xg_atp sophos:xg:atp netdlp none sophos_xg_anti_spam sophos:xg:anti_spam netdlp none sophos_xg_anti_virus sophos:xg:anti_virus netdlp none sophos_xg_content_filtering sophos:xg:content_filtering netdlp none sophos_xg_event sophos:xg:event netdlp none sophos_xg_firewall sophos:xg:firewall netdlp none sophos_xg_ssl sophos:xg:ssl netdlp none sophos_xg_sandbox sophos:xg:sandbox netdlp none sophos_xg_system_health sophos:xg:system_health netdlp none sophos_xg_heartbeat sophos:xg:heartbeat netdlp none sophos_xg_waf sophos:xg:waf netdlp none sophos_xg_wireless_protection sophos:xg:wireless_protection netdlp none sophos_xg_idp sophos:xg:idp netdlp none"},{"location":"sources/vendor/Sophos/webappliance/","title":"Web Appliance","text":""},{"location":"sources/vendor/Sophos/webappliance/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Sophos/webappliance/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Sophos/webappliance/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes sophos:webappliance None"},{"location":"sources/vendor/Sophos/webappliance/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes sophos_webappliance sophos:webappliance netproxy none"},{"location":"sources/vendor/Sophos/webappliance/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-sophos_webappliance.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-sophos_webappliance[sc4s-vps] {\n filter { \n        host(\"test-sophos-webapp-\" type(string) flags(prefix))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('sophos')\n            product('webappliance')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Spectracom/","title":"NTP Appliance","text":""},{"location":"sources/vendor/Spectracom/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Spectracom/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Spectracom/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes spectracom:ntp None nix:syslog None"},{"location":"sources/vendor/Spectracom/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes spectracom_ntp spectracom:ntp netops none"},{"location":"sources/vendor/Spectracom/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-spectracom_ntp.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-spectracom_ntp[sc4s-vps] {\n filter { \n        netmask(169.254.100.1/24)\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('spectracom')\n            product('ntp')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Splunk/heavyforwarder/","title":"Splunk Heavy Forwarder","text":"<p>In certain network architectures such as those using data diodes or those networks requiring \u201cin the clear\u201d inspection at network egress  SC4S can be used to accept specially formatted output from Splunk as RFC5424 syslog.</p>"},{"location":"sources/vendor/Splunk/heavyforwarder/#key-facts","title":"Key facts","text":"<ul> <li>RFC 5424 using port 601 (Framed)</li> </ul>"},{"location":"sources/vendor/Splunk/heavyforwarder/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual unknown"},{"location":"sources/vendor/Splunk/heavyforwarder/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes spectracom:ntp None nix:syslog None"},{"location":"sources/vendor/Splunk/heavyforwarder/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"<p>Index Source and Sourcetype will be used as determined by the Source/HWF</p>"},{"location":"sources/vendor/Splunk/heavyforwarder/#splunk-configuration","title":"Splunk Configuration","text":"<ul> <li>Splunk MUST have props and transforms applied (Typically via add-ons)</li> <li>This configuration will consume all output presuming no S2S is allowed no Splunk destination will be used</li> </ul>"},{"location":"sources/vendor/Splunk/heavyforwarder/#outputsconf","title":"outputs.conf","text":"<pre><code>#Because audit trail is protected and we can't transform it we can not use default we must use tcp_routing\n[tcpout]\ndefaultGroup = NoForwarding\n\n[tcpout:nexthop]\nserver = localhost:9000\nsendCookedData = false\n</code></pre>"},{"location":"sources/vendor/Splunk/heavyforwarder/#propsconf","title":"props.conf","text":"<pre><code>[default]\nADD_EXTRA_TIME_FIELDS = none\nANNOTATE_PUNCT = false\nSHOULD_LINEMERGE = false\nTRANSFORMS-zza-syslog = syslog_canforward, metadata_meta,  metadata_source, metadata_sourcetype, metadata_index, metadata_host, metadata_subsecond, metadata_time, syslog_prefix, syslog_drop_zero\n# The following applies for TCP destinations where the IETF frame is required\nTRANSFORMS-zzz-syslog = syslog_octal, syslog_octal_append\n# Comment out the above and uncomment the following for udp\n#TRANSFORMS-zzz-syslog-udp = syslog_octal, syslog_octal_append, syslog_drop_zero\n\n[audittrail]\n# We can't transform this source type its protected\nTRANSFORMS-zza-syslog =\nTRANSFORMS-zzz-syslog =\n</code></pre>"},{"location":"sources/vendor/Splunk/heavyforwarder/#transformsconf","title":"transforms.conf","text":"<pre><code>syslog_canforward]\nREGEX = ^.(?!audit)\nDEST_KEY = _TCP_ROUTING\nFORMAT = nexthop\n\n[metadata_meta]\nSOURCE_KEY = _meta\nREGEX = (?ims)(.*)\nFORMAT = ~~~SM~~~$1~~~EM~~~$0 \nDEST_KEY = _raw\n\n[metadata_source]\nSOURCE_KEY = MetaData:Source\nREGEX = ^source::(.*)$\nFORMAT = s=\"$1\"] $0\nDEST_KEY = _raw\n\n[metadata_sourcetype]\nSOURCE_KEY = MetaData:Sourcetype\nREGEX = ^sourcetype::(.*)$\nFORMAT = st=\"$1\" $0\nDEST_KEY = _raw\n\n[metadata_index]\nSOURCE_KEY = _MetaData:Index\nREGEX = (.*)\nFORMAT = i=\"$1\" $0\nDEST_KEY = _raw\n\n[metadata_host]\nSOURCE_KEY = MetaData:Host\nREGEX = ^host::(.*)$\nFORMAT = \" h=\"$1\" $0\nDEST_KEY = _raw\n\n[syslog_prefix]\nSOURCE_KEY = _time\nREGEX = (.*)\nFORMAT = &lt;1&gt;1 - - SPLUNK - COOKED [fields@274489 $0\nDEST_KEY = _raw\n\n[metadata_time]\nSOURCE_KEY = _time\nREGEX = (.*)\nFORMAT =  t=\"$1$0\nDEST_KEY = _raw\n\n[metadata_subsecond]\nSOURCE_KEY = _meta\nREGEX = \\_subsecond\\:\\:(\\.\\d+)\nFORMAT = $1 $0\nDEST_KEY = _raw\n\n[syslog_octal]\nINGEST_EVAL= mlen=length(_raw)+1\n\n[syslog_octal_append]\nINGEST_EVAL = _raw=mlen + \" \" + _raw\n\n[syslog_drop_zero]\nINGEST_EVAL = queue=if(mlen&lt;10,\"nullQueue\",queue)\n</code></pre>"},{"location":"sources/vendor/Splunk/sc4s/","title":"Splunk Connect for Syslog (SC4S)","text":""},{"location":"sources/vendor/Splunk/sc4s/#key-facts","title":"Key facts","text":"<ul> <li>Internal events</li> </ul>"},{"location":"sources/vendor/Splunk/sc4s/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4740/ Product Manual https://splunk-connect-for-syslog.readthedocs.io/en/latest/"},{"location":"sources/vendor/Splunk/sc4s/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes sc4s:events Internal events from the SC4S container and underlying syslog-ng process sc4s:metrics syslog-ng operational metrics that will be delivered directly to a metrics index in Splunk"},{"location":"sources/vendor/Splunk/sc4s/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes splunk_sc4s_events all main none splunk_sc4s_metrics all _metrics none splunk_sc4s_fallback all main none"},{"location":"sources/vendor/Splunk/sc4s/#filter-type","title":"Filter type","text":"<p>SC4S events and metrics are generated automatically and no specific ports or filters need to be configured for the collection of this data.</p>"},{"location":"sources/vendor/Splunk/sc4s/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>The default index used for sc4s metrics will be \u201c_metrics\u201d</li> <li>Metrics data is collected by default as traditional events; use of Splunk Metrics is enabled by an opt-in set by the variable <code>SC4S_DEST_SPLUNK_SC4S_METRICS_HEC</code>. See the \u201cOptions\u201d section below for details.</li> </ul>"},{"location":"sources/vendor/Splunk/sc4s/#options","title":"Options","text":"Variable default description SC4S_DEST_SPLUNK_SC4S_METRICS_HEC multi2 <code>event</code> produce metrics as plain text events; <code>single</code> produce metrics using Splunk Enterprise 7.3 single metrics format; <code>multi</code> produce metrics using Splunk Enterprise &gt;8.1 multi metric format  <code>multi2</code> produces improved (reduced resource consumption) multi metric format SC4S_SOURCE_MARK_MESSAGE_NULLQUEUE yes (yes"},{"location":"sources/vendor/Splunk/sc4s/#verification","title":"Verification","text":"<p>SC4S will generate versioning events at startup. These startup events can be used to validate HEC is set up properly on the Splunk side.</p> <pre><code>index=&lt;asconfigured&gt; sourcetype=sc4s:events | stats count by host\n</code></pre> <p>Metrics can be observed via the \u201cAnalytics\u2013&gt;Metrics\u201d navigation in the Search and Reporting app in Splunk.</p> <ul> <li>NOTE:  The presentation of metrics is undergoing active development; the delivery of metrics is currently considered an experimental feature.</li> </ul>"},{"location":"sources/vendor/StealthWatch/StealthIntercept/","title":"Stealth Intercept","text":""},{"location":"sources/vendor/StealthWatch/StealthIntercept/#key-facts","title":"Key facts","text":"<ul> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4609/ Product Manual unknown"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes StealthINTERCEPT None StealthINTERCEPT:alerts SC4S Format Shifts to JSON override template to <code>t_msg_hdr</code> for original raw"},{"location":"sources/vendor/StealthWatch/StealthIntercept/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes stealthbits_stealthintercept StealthINTERCEPT netids none stealthbits_stealthintercept_alerts StealthINTERCEPT:alerts netids Note TA does not support this source type"},{"location":"sources/vendor/Tanium/platform/","title":"Platform","text":"<p>This source requires a TLS connection; in most cases enabling TLS and using the default port 6514 is adequate. The source is understood to require a valid certificate.</p>"},{"location":"sources/vendor/Tanium/platform/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Requires TLS and uses IETF Frames use port 6587 after TLS Configuration</li> </ul>"},{"location":"sources/vendor/Tanium/platform/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4439/"},{"location":"sources/vendor/Tanium/platform/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes tanium none"},{"location":"sources/vendor/Tanium/platform/#index-configuration","title":"Index Configuration","text":"key index notes tanium_syslog epintel none"},{"location":"sources/vendor/Tenable/ad/","title":"ad","text":""},{"location":"sources/vendor/Tenable/ad/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Tenable/ad/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual"},{"location":"sources/vendor/Tenable/ad/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes tenable:ad:alerts None"},{"location":"sources/vendor/Tenable/ad/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes tenable_ad tenable:ad:alerts oswinsec none"},{"location":"sources/vendor/Tenable/nnm/","title":"nnm","text":""},{"location":"sources/vendor/Tenable/nnm/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Tenable/nnm/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual https://docs.tenable.com/integrations/Splunk/Content/Splunk2/ProcessWorkflow.htm"},{"location":"sources/vendor/Tenable/nnm/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes tenable:nnm:vuln None"},{"location":"sources/vendor/Tenable/nnm/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes tenable_nnm tenable:nnm:vuln netfw none"},{"location":"sources/vendor/Thales/thales_vormetric/","title":"Thales Vormetric Data Security Platform","text":""},{"location":"sources/vendor/Thales/thales_vormetric/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>RFC5424 default port 514</li> </ul>"},{"location":"sources/vendor/Thales/thales_vormetric/#links","title":"Links","text":"Ref Link Splunk Add-on na Product Manual link"},{"location":"sources/vendor/Thales/thales_vormetric/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes thales:vormetric None"},{"location":"sources/vendor/Thales/thales_vormetric/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes thales_vormetric thales:vormetric netauth None"},{"location":"sources/vendor/Thycotic/secretserver/","title":"Secret Server","text":""},{"location":"sources/vendor/Thycotic/secretserver/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Thycotic/secretserver/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4060/ Product Manual"},{"location":"sources/vendor/Thycotic/secretserver/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes thycotic:syslog None"},{"location":"sources/vendor/Thycotic/secretserver/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes Thycotic Software_Secret Server thycotic:syslog netauth none"},{"location":"sources/vendor/Tintri/syslog/","title":"Syslog","text":""},{"location":"sources/vendor/Tintri/syslog/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Tintri/syslog/#links","title":"Links","text":"Ref Link Splunk Add-on None"},{"location":"sources/vendor/Tintri/syslog/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes tintri none"},{"location":"sources/vendor/Tintri/syslog/#index-configuration","title":"Index Configuration","text":"key index notes tintri_syslog infraops none"},{"location":"sources/vendor/Trellix/cms/","title":"Trellix CMS","text":""},{"location":"sources/vendor/Trellix/cms/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Trellix/cms/#links","title":"Links","text":"Ref Link Splunk Add-on None"},{"location":"sources/vendor/Trellix/cms/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes trellix:cms CEF"},{"location":"sources/vendor/Trellix/cms/#source","title":"Source","text":"source notes trellix:cms None"},{"location":"sources/vendor/Trellix/cms/#index-configuration","title":"Index Configuration","text":"key source index notes trellix_cms trellix:cms netops none"},{"location":"sources/vendor/Trellix/mps/","title":"Trellix MPS","text":""},{"location":"sources/vendor/Trellix/mps/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Trellix/mps/#links","title":"Links","text":"Ref Link Splunk Add-on None"},{"location":"sources/vendor/Trellix/mps/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes trellix:mps CEF"},{"location":"sources/vendor/Trellix/mps/#source","title":"Source","text":"source notes trellix:mps None"},{"location":"sources/vendor/Trellix/mps/#index-configuration","title":"Index Configuration","text":"key source index notes trellix_mps trellix:mps netops none"},{"location":"sources/vendor/Trend/deepsecurity/","title":"Deep Security","text":""},{"location":"sources/vendor/Trend/deepsecurity/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Trend/deepsecurity/#links","title":"Links","text":"Ref Link Splunk Add-on CEF https://splunkbase.splunk.com/app/1936/"},{"location":"sources/vendor/Trend/deepsecurity/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes deepsecurity-system_events deepsecurity-intrusion_prevention deepsecurity-integrity_monitoring deepsecurity-log_inspection deepsecurity-web_reputation deepsecurity-firewall deepsecurity-antimalware deepsecurity-app_control"},{"location":"sources/vendor/Trend/deepsecurity/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes Trend Micro_Deep Security Agent deepsecurity epintel Used only if a correct source type is not matched Trend Micro_Deep Security Agent_intrusion prevention deepsecurity-intrusion_prevention epintel Trend Micro_Deep Security Agent_integrity monitoring deepsecurity-integrity_monitoring epintel Trend Micro_Deep Security Agent_log inspection deepsecurity-log_inspection epintel Trend Micro_Deep Security Agent_web reputation deepsecurity-web_reputation epintel Trend Micro_Deep Security Agent_firewall deepsecurity-firewall epintel Trend Micro_Deep Security Agent_antimalware deepsecurity-antimalware epintel Trend Micro_Deep Security Agent_app control deepsecurity-app_control epintel Trend Micro_Deep Security Manager deepsecurity-system_events epintel"},{"location":"sources/vendor/Ubiquiti/unifi/","title":"Unifi","text":"<p>All Ubiquity Unfi firewalls, switches, and access points share a common syslog configuration via the NMS.</p> <ul> <li>Login to NMS</li> <li>Navigate to settings</li> <li>Navigate to Site</li> <li>Enable Remote syslog server</li> <li>Enter hostname and port</li> </ul>"},{"location":"sources/vendor/Ubiquiti/unifi/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Ubiquiti/unifi/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/4107/ Product Manual https://https://help.ubnt.com/"},{"location":"sources/vendor/Ubiquiti/unifi/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes ubnt Used when no sub source type is required by add on ubnt:fw USG events ubnt:threat USG IDS events ubnt:switch Unifi Switches ubnt:wireless Access Point logs"},{"location":"sources/vendor/Ubiquiti/unifi/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes ubiquiti_unifi ubnt netops none ubiquiti_unifi_fw ubnt:fw netfw none"},{"location":"sources/vendor/Ubiquiti/unifi/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-ubiquiti_unifi_fw.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-ubiquiti_unifi_fw[sc4s-vps] {\n filter { \n        host(\"usg-*\" type(glob))\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('ubiquiti')\n            product('unifi')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/VMWare/airwatch/","title":"Airwatch","text":"<p>AirWatch is a product used for enterprise mobility management (EMM) software and standalone management systems for content, applications and email.</p>"},{"location":"sources/vendor/VMWare/airwatch/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/VMWare/airwatch/#links","title":"Links","text":"Ref Link Product Manual https://docs.vmware.com/en/VMware-Workspace-ONE/index.html"},{"location":"sources/vendor/VMWare/airwatch/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes vmware:airwatch None"},{"location":"sources/vendor/VMWare/airwatch/#index-configuration","title":"Index Configuration","text":"key index notes vmware_airwatch epintel none"},{"location":"sources/vendor/VMWare/carbonblack/","title":"Carbon Black Protection","text":""},{"location":"sources/vendor/VMWare/carbonblack/#rfc-5424-format","title":"RFC 5424 Format","text":""},{"location":"sources/vendor/VMWare/carbonblack/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/VMWare/carbonblack/#links","title":"Links","text":"Ref Link Splunk Add-on none"},{"location":"sources/vendor/VMWare/carbonblack/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes vmware:cb:protect Common sourcetype"},{"location":"sources/vendor/VMWare/carbonblack/#source","title":"Source","text":"source notes carbonblack:protection:cef Note this method of onboarding is not recommended for a more complete experience utilize the json format supported by he product with hec or s3"},{"location":"sources/vendor/VMWare/carbonblack/#index-configuration","title":"Index Configuration","text":"key source index notes vmware_cb-protect carbonblack:protection:cef epintel none"},{"location":"sources/vendor/VMWare/carbonblack/#legacy-cef-format","title":"Legacy CEF Format","text":""},{"location":"sources/vendor/VMWare/carbonblack/#key-facts_1","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/VMWare/carbonblack/#links_1","title":"Links","text":"Ref Link Splunk Add-on none"},{"location":"sources/vendor/VMWare/carbonblack/#sourcetypes_1","title":"Sourcetypes","text":"sourcetype notes cef Common sourcetype"},{"location":"sources/vendor/VMWare/carbonblack/#source_1","title":"Source","text":"source notes carbonblack:protection:cef Note this method of onboarding is not recommended for a more complete experience utilize the json format supported by he product with hec or s3"},{"location":"sources/vendor/VMWare/carbonblack/#index-configuration_1","title":"Index Configuration","text":"key source index notes Carbon Black_Protection carbonblack:protection:cef epintel none"},{"location":"sources/vendor/VMWare/horizonview/","title":"Horizon View","text":""},{"location":"sources/vendor/VMWare/horizonview/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/VMWare/horizonview/#links","title":"Links","text":"Ref Link Splunk Add-on None Manual unknown"},{"location":"sources/vendor/VMWare/horizonview/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes vmware:horizon None nix:syslog When used with a default port this will follow the generic NIX configuration when using a dedicated port, IP or host rules events will follow the index configuration for vmware nsx"},{"location":"sources/vendor/VMWare/horizonview/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes vmware_horizon vmware:horizon main none"},{"location":"sources/vendor/VMWare/vsphere/","title":"Vsphere","text":""},{"location":"sources/vendor/VMWare/vsphere/#product-vsphere-esx-nsx-controller-manager-edge","title":"Product - vSphere - ESX NSX (Controller, Manager, Edge)","text":"<p>Vmware vsphere product line has multiple old and known issues in syslog output.</p> <ul> <li>GUID values sent in place of time stamp</li> <li>Improper time stamp in all RFC5424 events</li> <li>No PRI</li> <li>No syslog header for some split events</li> <li>mismatch syslog header for some split events (segment 1 contains header remaining segments contain no header)</li> </ul> <p>WARNING use of a load balancer with udp will cause \u201ccorrupt\u201d event behavior due to out of order message processing caused by the load balancer</p> Ref Link Splunk Add-on ESX https://splunkbase.splunk.com/app/5603/ Splunk Add-on Vcenter https://splunkbase.splunk.com/app/5601/ Splunk Add-on nxs none Splunk Add-on vsan none"},{"location":"sources/vendor/VMWare/vsphere/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes <code>vmware:esxlog:${PROGRAM}</code> None <code>vmware:nsxlog:${PROGRAM}</code> None <code>vmware:vclog:${PROGRAM}</code> None nix:syslog When used with a default port, this will follow the generic NIX configuration. When using a dedicated port, IP or host rules events will follow the index configuration for vmware nsx"},{"location":"sources/vendor/VMWare/vsphere/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes vmware_vsphere_esx <code>vmware:esxlog:${PROGRAM}</code> infraops none vmware_vsphere_nsx <code>vmware:nxlog:${PROGRAM}</code> infraops none vmware_vsphere_nsxfw <code>vmware:nxlog:dfwpktlogs</code> infraops none vmware_vsphere_vc <code>vmware:vclog:${PROGRAM}</code> infraops none"},{"location":"sources/vendor/VMWare/vsphere/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content when using the default configuration. SC4S will normalize the structure of vmware events from multiple incorrectly formed varients to rfc5424 format to improve parsing</p>"},{"location":"sources/vendor/VMWare/vsphere/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source.</li> <li>Refer to the Splunk TA documentation for the specific customer format required for proxy configuration</li> <li>Select TCP or SSL transport option</li> <li>Ensure the format of the event is customized per Splunk documentation</li> </ul>"},{"location":"sources/vendor/VMWare/vsphere/#options","title":"Options","text":"Variable default description SC4S_LISTEN_VMWARE_VSPHERE_TCP_PORT empty string Enable a TCP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_VMWARE_VSPHERE_UDP_PORT empty string Enable a UDP port for this specific vendor product using a comma-separated list of port numbers SC4S_LISTEN_VMWARE_VSPHERE_TLS_PORT empty string Enable a TLS port for this specific vendor product using a comma-separated list of port numbers SC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG empty string empty/yes groups known instances of improperly split events set \u201cyes\u201d to return to enable"},{"location":"sources/vendor/VMWare/vsphere/#verification","title":"Verification","text":"<p>An active proxy will generate frequent events. Use the following search to validate events are present per source device</p> <pre><code>index=&lt;asconfigured&gt; sourcetype=\"vmware:vsphere:*\" | stats count by host\n</code></pre>"},{"location":"sources/vendor/VMWare/vsphere/#automatic-parser-configuration","title":"Automatic Parser Configuration","text":"<p>Enable the following options in the env_file</p> <pre><code>#Do not enable with a SNAT load balancer\nSC4S_USE_NAME_CACHE=yes\n#Combine known split events into a single event for Splunk\nSC4S_SOURCE_VMWARE_VSPHERE_GROUPMSG=yes\n#Learn vendor product from recognized events and apply to generic events\n#for example after the first vpxd event sshd will utilize vps \"vmware_vsphere_nix_syslog\" rather than \"nix_syslog\"\nSC4S_USE_VPS_CACHE=yes\n</code></pre>"},{"location":"sources/vendor/VMWare/vsphere/#manual-parser-configuration","title":"Manual Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-vmware_vsphere.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-vmware_vsphere[sc4s-vps] {\n filter {      \n        #netmask(169.254.100.1/24)\n        #host(\"-esx-\")\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('vmware')\n            product('vsphere')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/Varonis/datadvantage/","title":"DatAdvantage","text":""},{"location":"sources/vendor/Varonis/datadvantage/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Varonis/datadvantage/#links","title":"Links","text":"Ref Link Technology Add-On for Varonis https://splunkbase.splunk.com/app/4256/"},{"location":"sources/vendor/Varonis/datadvantage/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes varonis:ta"},{"location":"sources/vendor/Varonis/datadvantage/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes Varonis Inc._DatAdvantage varonis:ta main"},{"location":"sources/vendor/Vectra/cognito/","title":"Cognito","text":""},{"location":"sources/vendor/Vectra/cognito/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Vectra/cognito/#links","title":"Links","text":"Ref Link Technology Add-On for Vectra Cognito https://splunkbase.splunk.com/app/4408/"},{"location":"sources/vendor/Vectra/cognito/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes vectra:cognito:detect vectra:cognito:accountdetect vectra:cognito:accountscoring vectra:cognito:audit vectra:cognito:campaigns vectra:cognito:health vectra:cognito:hostscoring vectra:cognito:accountlockdown"},{"location":"sources/vendor/Vectra/cognito/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes Vectra Networks_X Series vectra:cognito:detect main Vectra Networks_X Series_accountdetect vectra:cognito:accountdetect main Vectra Networks_X Series_asc vectra:cognito:accountscoring main Vectra Networks_X Series_audit vectra:cognito:audit main Vectra Networks_X Series_campaigns vectra:cognito:campaigns main Vectra Networks_X Series_health vectra:cognito:health main Vectra Networks_X Series_hsc vectra:cognito:hostscoring main Vectra Networks_X Series_lockdown vectra:cognito:accountlockdown main"},{"location":"sources/vendor/Wallix/bastion/","title":"Bastion","text":""},{"location":"sources/vendor/Wallix/bastion/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Wallix/bastion/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3661/"},{"location":"sources/vendor/Wallix/bastion/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes WB:syslog note this sourcetype includes program:rdproxy all other data will be treated as nix"},{"location":"sources/vendor/Wallix/bastion/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes wallix_bastion infraops main none"},{"location":"sources/vendor/Wallix/bastion/#parser-configuration","title":"Parser Configuration","text":"<pre><code>#/opt/sc4s/local/config/app-parsers/app-vps-wallix_bastion.conf\n#File name provided is a suggestion it must be globally unique\n\napplication app-vps-test-wallix_bastion[sc4s-vps] {\n filter { \n        host('^wasb')\n    }; \n    parser { \n        p_set_netsource_fields(\n            vendor('wallix')\n            product('bastion')\n        ); \n    };   \n};\n</code></pre>"},{"location":"sources/vendor/XYPro/mergedaudit/","title":"Merged Audit","text":"<p>XY Pro merged audit also called XYGate or XMA is the defacto solution for syslog from HP Nonstop Server (Tandem)</p>"},{"location":"sources/vendor/XYPro/mergedaudit/#key-facts","title":"Key facts","text":"<ul> <li>Legacy BSD Format default port 514 CEF Format</li> </ul>"},{"location":"sources/vendor/XYPro/mergedaudit/#links","title":"Links","text":"Ref Link Splunk Add-on None Product Manual https://xypro.com/products/hpe-software-from-xypro/"},{"location":"sources/vendor/XYPro/mergedaudit/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes cef None"},{"location":"sources/vendor/XYPro/mergedaudit/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes XYPRO_NONSTOP cef infraops none"},{"location":"sources/vendor/Zscaler/lss/","title":"LSS","text":"<p>The ZScaler product manual includes and extensive section of configuration for multiple Splunk TCP input ports around page 26. When using SC4S these ports are not required and should not be used. Simply configure all outputs from the LSS to utilize the IP or host name of the SC4S instance and port 514</p>"},{"location":"sources/vendor/Zscaler/lss/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Zscaler/lss/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3865/ Product Manual https://community.zscaler.com/t/zscaler-splunk-app-design-and-installation-documentation/4728"},{"location":"sources/vendor/Zscaler/lss/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes zscalerlss-zpa-app None zscalerlss-zpa-bba None zscalerlss-zpa-connector None zscalerlss-zpa-auth None zscalerlss-zpa-audit None"},{"location":"sources/vendor/Zscaler/lss/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes zscaler_lss zscalerlss-zpa-app, zscalerlss-zpa-bba, zscalerlss-zpa-connector, zscalerlss-zpa-auth, zscalerlss-zpa-audit netproxy none"},{"location":"sources/vendor/Zscaler/nss/","title":"NSS","text":"<p>The ZScaler product manual includes and extensive section of configuration for multiple Splunk TCP input ports around page 26. When using SC4S these ports are not required and should not be used. Simply configure all outputs from the NSS to utilize the IP or host name of the SC4S instance and port 514</p>"},{"location":"sources/vendor/Zscaler/nss/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/Zscaler/nss/#links","title":"Links","text":"Ref Link Splunk Add-on https://splunkbase.splunk.com/app/3865/ Product Manual https://community.zscaler.com/t/zscaler-splunk-app-design-and-installation-documentation/4728"},{"location":"sources/vendor/Zscaler/nss/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes zscaler_nss_alerts Requires format customization add <code>\\tvendor=Zscaler\\tproduct=alerts</code> immediately prior to the <code>\\n</code> in the NSS Alert Web format. See Zscaler manual for more info. zscaler_nss_dns Requires format customization  add <code>\\tvendor=Zscaler\\tproduct=dns</code> immediately prior to the <code>\\n</code> in the NSS DNS format. See Zscaler manual for more info. zscaler_nss_web None zscaler_nss_fw Requires format customization add <code>\\tvendor=Zscaler\\tproduct=fw</code> immediately prior to the <code>\\n</code> in the Firewall format. See Zscaler manual for more info."},{"location":"sources/vendor/Zscaler/nss/#sourcetype-and-index-configuration","title":"Sourcetype and Index Configuration","text":"key sourcetype index notes zscaler_nss_alerts zscalernss-alerts main none zscaler_nss_dns zscalernss-dns netdns none zscaler_nss_fw zscalernss-fw netfw none zscaler_nss_web zscalernss-web netproxy none zscaler_nss_tunnel zscalernss-tunnel netops none zscaler_zia_audit zscalernss-zia-audit netops none zscaler_zia_sandbox zscalernss-zia-sandbox main none"},{"location":"sources/vendor/Zscaler/nss/#filter-type","title":"Filter type","text":"<p>MSG Parse: This filter parses message content</p>"},{"location":"sources/vendor/Zscaler/nss/#setup-and-configuration","title":"Setup and Configuration","text":"<ul> <li>Install the Splunk Add-on on the search head(s) for the user communities interested in this data source. If SC4S is exclusively used the addon is not required on the indexer.</li> <li>Review and update the splunk_metadata.csv file and set the index and sourcetype as required for the data source.</li> <li>Refer to the Splunk TA documentation for the specific customer format required for proxy configuration</li> <li>Select TCP or SSL transport option</li> <li>Ensure the format of the event is customized per Splunk documentation</li> </ul>"},{"location":"sources/vendor/a10networks/vthunder/","title":"a10networks vthunder","text":""},{"location":"sources/vendor/a10networks/vthunder/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/a10networks/vthunder/#links","title":"Links","text":"Ref Link A10 Networks SSL Insight App https://splunkbase.splunk.com/app/3937 A10 Networks Application Firewall App https://splunkbase.splunk.com/app/3920 A10 Networks L4 Firewall App https://splunkbase.splunk.com/app/3910"},{"location":"sources/vendor/a10networks/vthunder/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes a10networks:vThunder:cef CEF a10networks:vThunder:syslog Syslog"},{"location":"sources/vendor/a10networks/vthunder/#source","title":"Source","text":"source notes a10networks:vThunder None"},{"location":"sources/vendor/a10networks/vthunder/#index-configuration","title":"Index Configuration","text":"key source index notes a10networks_vThunder a10networks:vThunder netwaf, netops none"},{"location":"sources/vendor/epic/epic_ehr/","title":"Epic EHR","text":""},{"location":"sources/vendor/epic/epic_ehr/#key-facts","title":"Key facts","text":"<ul> <li>Requires vendor product by source configuration</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/epic/epic_ehr/#links","title":"Links","text":"Ref Link Splunk Add-on na"},{"location":"sources/vendor/epic/epic_ehr/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes epic:epic-ehr:syslog None"},{"location":"sources/vendor/epic/epic_ehr/#index-configuration","title":"Index Configuration","text":"key sourcetype index notes epic_epic-ehr epic:epic-ehr:syslog main none"},{"location":"sources/vendor/syslog-ng/loggen/","title":"loggen","text":"<p>Loggen is a tool used to load test syslog implementations.</p>"},{"location":"sources/vendor/syslog-ng/loggen/#key-facts","title":"Key facts","text":"<ul> <li>MSG Format based filter</li> <li>Legacy BSD Format default port 514</li> </ul>"},{"location":"sources/vendor/syslog-ng/loggen/#links","title":"Links","text":"Ref Link Product Manual https://www.syslog-ng.com/technical-documents/doc/syslog-ng-open-source-edition/3.26/administration-guide/96#loggen.1"},{"location":"sources/vendor/syslog-ng/loggen/#sourcetypes","title":"Sourcetypes","text":"sourcetype notes syslogng:loggen By default, loggen uses the legacy BSD-syslog message format.BSD example:<code>loggen --inet --dgram --number 1 &lt;ip&gt; &lt;port&gt;</code>RFC5424 example:<code>loggen --inet --dgram -PF --number 1 &lt;ip&gt; &lt;port&gt;</code>Refer to above manual link for more examples."},{"location":"sources/vendor/syslog-ng/loggen/#index-configuration","title":"Index Configuration","text":"key index notes syslogng_loggen main none"},{"location":"troubleshooting/troubleshoot_SC4S_server/","title":"SC4S Server Startup and Operational Validation","text":"<p>The following sections will guide the administrator to the most commons solutions to startup and operational issues with SC4S.  In general, if you are just starting out with SC4S and wish to simply run with the \u201cstock\u201d configuration, startup out of systemd is recommended.  If, on the other hand, you are in the depths of a custom configuration of SC4S with significant modifications (such as multiple unique ports for sources, hostname/CIDR block configuration for sources, new log paths, etc.) then it is best to start SC4S with the container runtime command (<code>podman</code> or <code>docker</code>) directly from the command line (below).  When you are satisfied with the operation, a transition to systemd can then be made.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#systemd-errors-during-sc4s-startup","title":"systemd Errors During SC4S Startup","text":"<p>Most issues that occur with startup and operation of sc4s typically involve syntax errors or duplicate listening ports.  If you are running out of systemd, you may see this at startup:</p> <p><pre><code>[root@sc4s syslog-ng]# systemctl start sc4s\nJob for sc4s.service failed because the control process exited with error code. See \"systemctl status sc4s.service\" and \"journalctl -xe\" for details.\n</code></pre> Follow the checks below to resolve the issue:</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#is-the-sc4s-container-running","title":"Is the SC4S container running?","text":"<p>There may be nothing untoward after starting with systemd, but the container is not running at all after checking with <code>podman logs SC4S</code> or <code>podman ps</code>.  A more informative command than <code>journalctl -xe</code> is the following, <pre><code>journalctl -b -u sc4s | tail -100\n</code></pre> which will print the last 100 lines of the system journal in far more detail, which should be sufficient to see the specific failure (syntax or runtime) and guide you in troubleshooting why the container exited unexpectedly.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#does-the-sc4s-container-start-and-run-properly-outside-of-the-systemd-service-environment","title":"Does the SC4S container start (and run) properly outside of the systemd service environment?","text":"<p>As an alternative to launching via systemd during the initial installation phase, you may wish to test the container startup outside of the systemd startup environment. This alternative should be considered required when undergoing heavy troubleshooting or log path development (e.g. when <code>SC4S_DEBUG_CONTAINER</code> is set to \u201cyes\u201d).  The following command will launch the container directly from the CLI. This command assumes the local mounted directories are set up as shown in the \u201cgetting started\u201d examples; adjust for your local requirements:</p> <pre><code>/usr/bin/podman run \\\n    -v splunk-sc4s-var:/var/lib/syslog-ng \\\n    -v /opt/sc4s/local:/etc/syslog-ng/conf.d/local:z \\\n    -v /opt/sc4s/archive:/var/lib/syslog-ng/archive:z \\\n    -v /opt/sc4s/tls:/etc/syslog-ng/tls:z \\\n    --env-file=/opt/sc4s/env_file \\\n    --network host \\\n    --name SC4S \\\n    --rm splunk/scs:latest\n</code></pre> <p>If you are using docker, substitute \u201cdocker\u201d for \u201cpodman\u201d for the container runtime command above.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#is-the-container-still-running-when-systemd-thinks-its-not","title":"Is the container still running (when systemd thinks it\u2019s not)?","text":"<p>In some instances, (particularly when <code>SC4S_DEBUG_CONTAINER=yes</code>) an SC4S container might not shut down completely when starting/stopping out of systemd, and systemd will attempt to start a new container when one is already running with the <code>SC4S</code> name. You will see this type of output when viewing the journal after a failed start caused by this condition, or a similar message when the container is run directly from the CLI:</p> <pre><code>Jul 15 18:45:20 sra-sc4s-alln01-02 podman[11187]: Error: error creating container storage: the container name \"SC4S\" is already in use by \"894357502b2a7142d097ea3ca1468d1cb4fbc69959a9817a1bbe145a09d37fb9\". You have to remove that container...\nJul 15 18:45:20 sra-sc4s-alln01-02 systemd[1]: sc4s.service: Main process exited, code=exited, status=125/n/a\n</code></pre> <p>To rectify this, simply execute <pre><code>podman rm -f SC4S\n</code></pre></p> <p>SC4S should then start normally.</p> <ul> <li>NOTE:  This symptom will recur if <code>SC4S_DEBUG_CONTAINER</code> is set to \u201cyes\u201d.  Do not attempt to use systemd when this variable is set; use the CLI <code>podman</code> or <code>docker</code> commands directly to start/stop SC4S.</li> </ul>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#hectoken-connection-errors-aka-no-data-in-splunk","title":"HEC/token connection errors (AKA \u201cNo data in Splunk\u201d)","text":"<p>SC4S performs basic HEC connectivity and index checks at startup.  These indicate general connection issues and indexes that may not be accessible and/or configured on the Splunk side.  To check the container logs which contain the results of these tests, run:</p> <pre><code>/usr/bin/&lt;podman|docker&gt; logs SC4S\n</code></pre> <p>and note the output.  You will see entries similar to these:</p> <pre><code>SC4S_ENV_CHECK_HEC: Splunk HEC connection test successful; checking indexes...\n\nSC4S_ENV_CHECK_INDEX: Checking email {\"text\":\"Incorrect index\",\"code\":7,\"invalid-event-number\":1}\nSC4S_ENV_CHECK_INDEX: Checking epav {\"text\":\"Incorrect index\",\"code\":7,\"invalid-event-number\":1}\nSC4S_ENV_CHECK_INDEX: Checking main {\"text\":\"Success\",\"code\":0}\n</code></pre> <p>Note the specifics of the indexes that are not configured correctly, and rectify in the Splunk configuration.  If this is not addressed properly, you may see output similar to the below when data flows into sc4s:</p> <p><pre><code>Mar 16 19:00:06 b817af4e89da syslog-ng[1]: Server returned with a 4XX (client errors) status code, which means we are not authorized or the URL is not found.; url='https://splunk-instance.com:8088/services/collector/event', status_code='400', driver='d_hec#0', location='/opt/syslog-ng/etc/conf.d/destinations/splunk_hec.conf:2:5'\nMar 16 19:00:06 b817af4e89da syslog-ng[1]: Server disconnected while preparing messages for sending, trying again; driver='d_hec#0', location='/opt/syslog-ng/etc/conf.d/destinations/splunk_hec.conf:2:5', worker_index='4', time_reopen='10', batch_size='1000'\n</code></pre> This is an indication that the standard <code>d_hec</code> destination in syslog-ng (which is the route to Splunk) is being rejected by the HEC endpoint. A <code>400</code> error (not 404) is normally caused by an index that has not been created on the Splunk side.  This can present a serious problem, as just one bad index will \u201ctaint\u201d the entire batch (in this case, 1000 events) and prevent any of them from being sent to Splunk.  It is imperative that the container logs be free of these kinds of errors in production. You can use the alternate HEC debug destination (below) to help debug this condition by sending direct \u201ccurl\u201d commands to the HEC endpoint outside of the SC4S setting.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#invalid-sc4s-listen-ports","title":"Invalid SC4S listen ports","text":"<p>SC4S exclusively grant a port to a device when <code>SC4S_LISTEN_{vendor}_{product}_{TCP/UDP/TLS}_PORT={port}</code>.</p> <p>During startup, SC4S validates that listening ports are configured correctly, and in case of misconfiguration, you will be able see any issues in container logs.</p> <p>You will receive an error message similar to the following if listening ports for <code>MERAKI SWITCHES</code> are configured incorrectly: <pre><code>SC4S_LISTEN_MERAKI_SWITCHES_TCP_PORT: Wrong port number, don't use default port like (514,614,6514)\nSC4S_LISTEN_MERAKI_SWITCHES_UDP_PORT: 7000 is not unique and has already been used for another source\nSC4S_LISTEN_MERAKI_SWITCHES_TLS_PORT: 999999999999 must be integer within the range (0, 10000)\n</code></pre></p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-local-disk-resource-considerations","title":"SC4S Local Disk Resource Considerations","text":"<ul> <li> <p>Check the HEC connection to Splunk. If the connection is down for a long period of time, the local disk buffer used for backup will exhaust local disk resources.  The size of the local disk buffer is configured in the env_file: Disk buffer configuration</p> </li> <li> <p>Check the env_file to see if <code>SC4S_DEST_GLOBAL_ALTERNATES</code> is set to <code>d_hec_debug</code>,<code>d_archive</code> or other file-based destination; all of these will consume significant local disk space.</p> </li> </ul> <p><code>d_hec_debug</code> and <code>d_archive</code> are organized by sourcetype; the <code>du -sh *</code> command can be used in each subdirectory to find the culprit. </p> <ul> <li>Try rebuilding sc4s volume <pre><code>podman volume rm splunk-sc4s-var\npodman volume create splunk-sc4s-var\n</code></pre></li> <li>Try pruning containers <pre><code>podman system prune [--all]\n</code></pre></li> </ul>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4skernel-udp-input-buffer-settings","title":"SC4S/kernel UDP Input Buffer Settings","text":"<p>SC4S has a setting that requests a certain buffer size when configuring the UDP sockets.  The kernel must have its parameters set to at least the same size (or greater) than the syslog-ng config is requesting, or the following will occur in the SC4S logs:</p> <p><pre><code>/usr/bin/&lt;podman|docker&gt; logs SC4S\n</code></pre> Note the output. The following warning message is not a failure condition unless we are reaching the upper limit of hardware performance. <pre><code>The kernel refused to set the receive buffer (SO_RCVBUF) to the requested size, you probably need to adjust buffer related kernel parameters; so_rcvbuf='1703936', so_rcvbuf_set='425984'\n</code></pre> Make changes to /etc/sysctl.conf. Changing receive buffer values here to 16 MB:</p> <p><pre><code>net.core.rmem_default = 17039360\nnet.core.rmem_max = 17039360 \n</code></pre> Run following commands for changes to be affected. <pre><code>sysctl -p restart SC4S \n</code></pre></p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-tls-listener-validation","title":"SC4S TLS Listener Validation","text":"<p>To verify the correct configuration of the TLS server use the following command. Replace the IP, FQDN, and port as appropriate:</p> <pre><code>&lt;podman|docker&gt; run -ti drwetter/testssl.sh --severity MEDIUM --ip 127.0.0.1 selfsigned.example.com:6510\n</code></pre>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#timezone-mismatch-in-events","title":"Timezone mismatch in events","text":"<p>By default, SC4S resolves the timezone to GMT. If customer have a preference to use local TZ then set the user TZ preference in Splunk during search time rather than at index time.  Timezone config documentation</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#dealing-with-non-rfc-5424-compliant-sources","title":"Dealing with non RFC-5424 compliant sources","text":"<p>If a data source you are trying to ingest claims it is RFC-5424 compliant but you are getting an \u201cError processing log message:\u201d from SC4S, the message violates the standard in some way.  Unfortunately multiple vendors claim RFC-5424 compliance without fully testing that they are. In this case, the underlying syslog-ng process will send an error event, with the location of the error in the original event highlighted with <code>&gt;@&lt;</code> to indicate where the error occurred. Here is an example error message:</p> <pre><code>{ [-]\n   ISODATE: 2020-05-04T21:21:59.001+00:00\n   MESSAGE: Error processing log message: &lt;14&gt;1 2020-05-04T21:21:58.117351+00:00 arcata-pks-cluster-1 pod.log/cf-workloads/logspinner-testing-6446b8ef - - [kubernetes@47450 cloudfoundry.org/process_type=\"web\" cloudfoundry.org/rootfs-version=\"v75.0.0\" cloudfoundry.org/version=\"eae53cc3-148d-4395-985c-8fef0606b9e3\" controller-revision-hash=\"logspinner-testing-6446b8ef05-7db777754c\" cloudfoundry.org/app_guid=\"f71634fe-34a4-4f89-adac-3e523f61a401\" cloudfoundry.org/source_type=\"APP\" security.istio.io/tlsMode=\"istio\" statefulset.kubernetes.io/pod-n&gt;@&lt;ame=\"logspinner-testing-6446b8ef05-0\" cloudfoundry.org/guid=\"f71634fe-34a4-4f89-adac-3e523f61a401\" namespace_name=\"cf-workloads\" object_name=\"logspinner-testing-6446b8ef05-0\" container_name=\"opi\" vm_id=\"vm-e34452a3-771e-4994-666e-bfbc7eb77489\"] Duration 10.00299412s TotalSent 10 Rate 0.999701 \n   PID: 33\n   PRI: &lt;43&gt;\n   PROGRAM: syslog-ng\n}\n</code></pre> <p>In this example the error can be seen in the snippet <code>statefulset.kubernetes.io/pod-n&gt;@&lt;ame</code>. Looking at the spec for RFC5424, it states that the \u201cSD-NAME\u201d (the left-hand side of the name=value pairs) cannot be longer than 32 printable ASCII characters. In this message, the indicated name exceeds that. Unfortunately, this is a spec violation on the part of the vendor. Ideally the vendor would address this violation so their logs would be RFC-5424 compliant. Alternatively, an exception could be added to the SC4S filter log path (or an alternative (workaround) log path created) for the data source if the vendor can\u2019t/won\u2019t fix the defect.</p> <p>In this example, the reason <code>RAWMSG</code> is not shown in the fields above is because this error message is coming from syslog-ng itself \u2013 not the filter/log path. In messages of the type <code>Error processing log message:</code> where the PROGRAM is shown as <code>syslog-ng</code>, that is the clue your incoming message is not RFC-5424 compliant (though it\u2019s often close, as is the case here).</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#in-byoe-the-metricsinternal-processing-message-are-abusing-the-terminal-how-to-fix-this","title":"In BYOE the metrics/internal processing message are abusing the terminal , how to fix this?","text":"<p>In BYOE, when we try to start sc4s service , the terminal is getting abused from the internal and metrics log Example of the issue is Github Terminal abuse issue</p> <p>To rectify this, Please set following property in env_file <pre><code>SC4S_SEND_METRICS_TERMINAL=no\n</code></pre></p> <p>Restart SC4S and it will not send any more metrics data to Terminal.</p> <ul> <li>NOTE:  This symptom will recur if <code>SC4S_DEBUG_CONTAINER</code> is set to \u201cyes\u201d.  Do not attempt to use systemd when this variable is set; use the CLI <code>podman</code> or <code>docker</code> commands directly to start/stop SC4S.</li> </ul>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#sc4s-dropping-invalid-events","title":"SC4S dropping invalid events","text":""},{"location":"troubleshooting/troubleshoot_SC4S_server/#sometimes-you-notice-you-are-missing-some-cef-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"Sometimes you notice you are missing some CEF logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_CEF=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid CEF format.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-vmware-cb-protect-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"If you notice you are missing some VMWARE CB-PROTECT logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_VMWARE_CB_PROTECT=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid VMWARE CB-PROTECT format.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-cisco-ios-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"If you notice you are missing some CISCO IOS logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_CISCO=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid CISCO IOS format.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-vmware-vsphere-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"If you notice you are missing some VMWARE VSPHERE logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_VMWARE_VSPHERE=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid VMWARE VSPHERE format.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-raw-bsd-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"If you notice you are missing some RAW BSD logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_RAW_BSD=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid RAW BSD format.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-raw-xml-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"If you notice you are missing some RAW XML logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_XML=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid RAW XML format.</p>"},{"location":"troubleshooting/troubleshoot_SC4S_server/#if-you-notice-you-are-missing-some-hpe-jetdirect-logs-which-are-not-rfc-compliant-but-logs-are-important-how-to-fix-this","title":"If you notice you are missing some HPE JETDIRECT logs which are not RFC compliant but logs are important, how to fix this?","text":"<p>To rectify this, Please set following property in env_file <pre><code>SC4S_DISABLE_DROP_INVALID_HPE=yes\n</code></pre></p> <p>Restart SC4S and it will not drop any invalid HPE JETDIRECT format.</p> <p>NOTE: Please use only in this case of exception and this is splunk-unsupported feature. Also this setting might impact SC4S performance.</p>"},{"location":"troubleshooting/troubleshoot_resources/","title":"SC4S Logging and Troubleshooting Resources","text":""},{"location":"troubleshooting/troubleshoot_resources/#helpful-linux-and-container-commands","title":"Helpful Linux and Container Commands","text":""},{"location":"troubleshooting/troubleshoot_resources/#linux-service-systemd-commands","title":"Linux service (systemd) commands","text":"<ul> <li>Check service status <code>systemctl status sc4s</code></li> <li>Start service <code>systemctl start service</code></li> <li>Stop service <code>systemctl stop service</code></li> <li>Restart service <code>systemctl restart service</code></li> <li>Enabling service at boot <code>systemctl enable sc4s</code></li> <li>Query the system journal <code>journalctl -b -u sc4s</code></li> </ul>"},{"location":"troubleshooting/troubleshoot_resources/#container-commands","title":"Container Commands","text":"<ul> <li> <p>NOTE:  All container commands below can be run with either runtime (<code>podman</code> or <code>docker</code>).</p> </li> <li> <p>Container logs <code>sudo podman logs SC4S</code></p> </li> <li>Exec into SC4S container <code>podman exec -it SC4S bash</code></li> <li>Rebuilding SC4S volume <pre><code>podman volume rm splunk-sc4s-var\npodman volume create splunk-sc4s-var\n</code></pre></li> <li>Pull an image or a repository from a registry <code>podman pull ghcr.io/splunk/splunk-connect-for-syslog/container3</code></li> <li>Remove unused data <code>podman system prune</code></li> <li>Load an image from a tar archive or STDIN <code>podman load &lt;tar&gt;</code></li> </ul>"},{"location":"troubleshooting/troubleshoot_resources/#test-commands","title":"Test Commands","text":"<p>Checking SC4S port using \u201cnc\u201d. Run this command where SC4S is hosted and check for data in Splunk for success and failure <pre><code>echo '&lt;raw_sample&gt;' |nc &lt;host&gt; &lt;port&gt;\n</code></pre></p>"},{"location":"troubleshooting/troubleshoot_resources/#obtaining-on-the-wire-raw-events","title":"Obtaining \u201cOn-the-wire\u201d Raw Events","text":"<p>In almost all cases during development or troubleshooting, you will need to obtain samples of the messages exactly as they are received by SC4S. These \u201craw\u201d events contain the full syslog message (including the <code>&lt;PRI&gt;</code> preamble) and differs from those that appear in Splunk after processing by sc4s and/or Splunk. This is the only way to determine if SC4S parsers and filters are operating correctly, as raw messages are needed for \u201cplayback\u201d when testing. In addition, the community supporting SC4S will always first ask for raw samples (kind of like the way Splunk support always asks for \u201cdiags\u201d) before any development or troubleshooting exercise.</p> <p>Here are some options for obtaining raw logs for one or more sourcetypes:</p> <ul> <li> <p>Run <code>tcpdump</code> on the collection interface and display the results in ASCII.  You will see events of the form <pre><code>&lt;165&gt;1 2007-02-15T09:17:15.719Z router1 mgd 3046 UI_DBASE_LOGOUT_EVENT [junos@2636.1.1.1.2.18 username=\"user\"] User 'user' exiting configuration mode\n</code></pre> buried in the packet contents.</p> </li> <li> <p>Set the variable <code>SC4S_SOURCE_STORE_RAWMSG=yes</code> in <code>env_file</code> and restart sc4s.  This will store the raw message in a syslog-ng macro called <code>RAWMSG</code> and will be displayed in Splunk for all <code>fallback</code> messages.</p> </li> <li>For most other sourcetypes, the <code>RAWMSG</code> is not displayed, but can be surfaced by changing the output template to one of the JSON variants (t_JSON_3164 or t_JSON_5424 depending on RFC message type). See SC4S metadata configuration for more details.</li> <li>In order to send <code>RAWMSG</code> to Splunk regardless the sourcetype you can also temporarily place the following final filter in the local parsers\u2019 directory: <pre><code>block parser app-finalfilter-fetch-rawmsg() {\n    channel {\n        rewrite {\n            r_set_splunk_dest_default(\n                template('t_fallback_kv')\n            );\n        };\n    };\n};\n\napplication app-finalfilter-fetch-rawmsg[sc4s-finalfilter] {\n    parser { app-finalfilter-fetch-rawmsg(); };\n};\n</code></pre> With both <code>SC4S_SOURCE_STORE_RAWMSG=yes</code> in <code>/opt/sc4s/env_file</code> and this finalfilter placed in <code>/opt/sc4s/local/config/app_parsers</code> your restarted SC4S instance will add raw messages to all the messages sent to Splunk.</li> </ul> <p>** IMPORTANT!  Be sure to turn off the <code>RAWMSG</code> variable when you are finished, as it doubles the memory and disk requirements of sc4s.  Do not use <code>RAWMSG</code> in production!</p> <ul> <li>Lastly, you can enable the alternate destination <code>d_rawmsg</code> for one or more sourcetypes.  This destination will write the raw messages to the container directory <code>/var/syslog-ng/archive/rawmsg/&lt;sourcetype&gt;</code> (which is typically mapped locally to <code>/opt/sc4s/archive</code>). Within this directory, the logs are organized by host and time.</li> </ul>"},{"location":"troubleshooting/troubleshoot_resources/#exec-into-the-container-advanced","title":"\u201cexec\u201d into the container (advanced)","text":"<p>You can confirm how the templating process created the actual syslog-ng config files that are in use by \u201cexec\u2019ing in\u201d to the container and navigating the syslog-ng config filesystem directly.  To do this, run <pre><code>/usr/bin/podman exec -it SC4S /bin/bash\n</code></pre> and navigate to <code>/opt/syslog-ng/etc/</code> to see the actual config files in use.  If you are adept with container operations and syslog-ng itself, you can modify files directly and reload syslog-ng with the command <code>kill -1 1</code> in the container. You can also run the <code>/entrypoint.sh</code> script by hand (or a subset of it, such as everything but syslog-ng) and have complete control over the templating and underlying syslog-ng process. This is an advanced topic and further help can be obtained via the github issue tracker and Slack channels.</p>"},{"location":"troubleshooting/troubleshoot_resources/#keeping-a-failed-container-running-even-more-advanced","title":"Keeping a failed container running (even more advanced)","text":"<p>When debugging a configuration syntax issue at startup, it is often helpful to keep the container running after a syslog-ng startup failure. In order to facilitate troubleshooting and make \u201con the fly\u201d syslog-ng configuration changes from within a running container, the container can be forced to remain running when syslog-ng fails to start (which normally terminates the container). This can be enabled by adding <code>SC4S_DEBUG_CONTAINER=yes</code> to the <code>env_file</code>.  Use this capability in conjunction with \u201cexec-ing\u201d into the container described above.</p> <ul> <li>NOTE:  Do not attempt to enable the debug container mode while running out of systemd.  Run the container manually from the CLI, as <code>podman</code> or <code>docker</code> commands will be required to start, stop, and optionally clean up cruft left behind by the debug process. Only when <code>SC4S_DEBUG_CONTAINER</code> is set to \u201cno\u201d (or completely unset) should systemd startup processing resume.</li> </ul>"},{"location":"troubleshooting/troubleshoot_resources/#fix-timezone","title":"Fix timezone","text":"<p>Mismatch in TZ can occur if SC4S and logHost are not in same TZ. This is commonly occurring problem. To fix it one must  create a filter using <code>sc4s-lp-dest-format-d_hec_fmt</code>. Example:</p> <pre><code>#filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-fix_tz_something.conf\n\nblock parser app-dest-rewrite-checkpoint_drop-d_fmt_hec_default() {    \n    channel {\n            rewrite { fix-time-zone(\"EST5EDT\"); };\n    };\n};\napplication app-dest-rewrite-fix_tz_something-d_fmt_hec_default[sc4s-lp-dest-format-d_hec_fmt] {\n    filter {\n        match('checkpoint' value('fields.sc4s_vendor') type(string))                 &lt;- this has to be customized\n        and match('syslog' value('fields.sc4s_product') type(string))                &lt;- this has to be customized\n        and match('Drop' value('.SDATA.sc4s@2620.action') type(string))              &lt;- this has to be customized\n        and match('12.' value('.SDATA.sc4s@2620.src') type(string) flags(prefix) );  &lt;- this has to be customized\n\n    };    \n    parser { app-dest-rewrite-checkpoint_drop-d_fmt_hec_default(); };   \n};\n</code></pre> <p>Or create a post-filter if destport, container, proto are not available in indexed fields: </p> <p><pre><code>#filename: /opt/sc4s/local/config/app_parsers/rewriters/app-dest-rewrite-fix_tz_something.conf\n\nblock parser app-dest-rewrite-fortinet_fortios-d_fmt_hec_default() {\n    channel {\n            rewrite {\n                  fix-time-zone(\"EST5EDT\");\n            };\n    };\n};\n\napplication app-dest-rewrite-device-d_fmt_hec_default[sc4s-postfilter] {\n    filter {\n         match(\"xxxx\", value(\"fields.sc4s_destport\") type(glob));  &lt;- this has to be customized\n    };\n    parser { app-dest-rewrite-fortinet_fortios-d_fmt_hec_default(); };\n};\n</code></pre> Note that filter match statement should be aligned to your data! Parser accepts timezone in formats: \u201cAmerica/New York\u201d or \u201cEST5EDT\u201d (though not short form like \u201cEST\u201d only).</p>"},{"location":"troubleshooting/troubleshoot_resources/#cyberark-logs-known-issue","title":"Cyberark logs known issue","text":"<p>When the data is received on the indexers all the events are merged together into one. Please check the below link for configuration on cyberark side https://cyberark-customers.force.com/s/article/00004289</p>"},{"location":"troubleshooting/troubleshoot_resources/#sc4s-events-dropping-issue-when-another-interface-used-to-receive-logs","title":"SC4S events dropping issue when another interface used to receive logs","text":"<p>When second / another interface used to receive syslog traffic, RPF (Reverse Path Forwarding filtering) in RHEL (configured as default configuration) was dropping the events.</p> <p>Need to add static route for source device to point back dedicated syslog interface. Reference: https://access.redhat.com/solutions/53031</p>"},{"location":"troubleshooting/troubleshoot_resources/#sc4s-events-not-ingested-in-splunk-from-other-vm","title":"SC4S events not ingested in splunk from other VM","text":"<p>When data is transmitted through an echo message from the same instance, it is successfully sending data to splunk. However, when the echo is sent from a different instance, the data does not appear in splunk and no errors are reported in the logs. To resolve this issue, it is essential to check whether an internal firewall is enabled. If an  internal firewall is active, it\u2019s important to verify whether the default port 514 or the port which you have used is blocked or not. Here are some commands to check and enable, if not enabled: <pre><code>#To list all the firewall ports\nsudo firewall-cmd --list-all\n#to enable 514 if its not enabled\nsudo firewall-cmd --zone=public --permanent --add-port=514/udp\nsudo firewall-cmd  --reload\n</code></pre></p>"}]}